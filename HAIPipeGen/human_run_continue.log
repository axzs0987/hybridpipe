Traceback (most recent call last):
  File ".\hai.py", line 5, in <module>
    run_one_hi('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1794, in run_one_hi
    res = pro.profiling_code(notebook_id, need_remove_model=1)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 779, in profiling_code
    self.load_origin_code(notebook_id, need_remove_model)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 367, in load_origin_code
    os.mkdir(self.file_path)
FileNotFoundError: [WinError 3] 系统找不到指定的路径。: 'hi_res_data/datascientist25_gender-recognition-by-voice-using-machine-learning'
!!!!!!!!!!!!!!!!!!!!!!!
Traceback (most recent call last):
  File ".\hai.py", line 5, in <module>
    run_one_hi('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1794, in run_one_hi
    res = pro.profiling_code(notebook_id, need_remove_model=1)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 795, in profiling_code
    self.change_train_test_split(result_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 547, in change_train_test_split
    with open("prenotebook_varibles_index/"+str(self.notebook_id)+".json", 'w') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'prenotebook_varibles_index/datascientist25_gender-recognition-by-voice-using-machine-learning.json'
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
Traceback (most recent call last):
  File ".\hai.py", line 5, in <module>
    run_one_hi('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1794, in run_one_hi
    res = pro.profiling_code(notebook_id, need_remove_model=1)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 795, in profiling_code
    self.change_train_test_split(result_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 547, in change_train_test_split
    with open("prenotebook_varibles_index/"+str(self.notebook_id)+".json", 'w') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'prenotebook_varibles_index/datascientist25_gender-recognition-by-voice-using-machine-learning.json'
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
Traceback (most recent call last):
  File ".\hai.py", line 5, in <module>
    run_one_hi('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1794, in run_one_hi
    res = pro.profiling_code(notebook_id, need_remove_model=1)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 795, in profiling_code
    self.change_train_test_split(result_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 547, in change_train_test_split
    with open("new_data/prenotebook_varibles_index/"+str(self.notebook_id)+".json", 'w') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'new_data/prenotebook_varibles_index/datascientist25_gender-recognition-by-voice-using-machine-learning.json'
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
Traceback (most recent call last):
  File ".\hai.py", line 5, in <module>
    if not os.path.exists('new_data'):
NameError: name 'os' is not defined
Traceback (most recent call last):
  File ".\hai.py", line 8, in <module>
    run_one_hi('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1794, in run_one_hi
    res = pro.profiling_code(notebook_id, need_remove_model=1)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 795, in profiling_code
    self.change_train_test_split(result_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 547, in change_train_test_split
    with open("new_data/prenotebook_varibles_index/"+str(self.notebook_id)+".json", 'w') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'new_data/prenotebook_varibles_index/datascientist25_gender-recognition-by-voice-using-machine-learning.json'
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
Traceback (most recent call last):
  File ".\hai.py", line 8, in <module>
    run_one_hi('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1798, in run_one_hi
    res = pro.profiling_code(notebook_id, need_remove_model=1)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 796, in profiling_code
    res = self.add_model_code()
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 584, in add_model_code
    self.code += '    np.save("' + self.train_feature_file + '.npy' + '", ' + self.x_train_varible + ')\n'
AttributeError: 'Preprocessing' object has no attribute 'train_feature_file'
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
Traceback (most recent call last):
  File ".\hai.py", line 8, in <module>
    run_one_hi('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1798, in run_one_hi
    res = pro.profiling_code(notebook_id, need_remove_model=1)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 798, in profiling_code
    self.save_code('new_data/prenotebook_code/')
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 554, in save_code
    with open(root_path+ str(self.notebook_id) + '.py', 'w') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'new_data/prenotebook_code/datascientist25_gender-recognition-by-voice-using-machine-learning.py'
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

Traceback (most recent call last):
  File ".\hai.py", line 8, in <module>
    run_one_hi('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1798, in run_one_hi
    res = pro.profiling_code(notebook_id, need_remove_model=1)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 798, in profiling_code
    self.save_code('new_data/prenotebook_code/')
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 554, in save_code
    with open(root_path+ str(self.notebook_id) + '.py', 'w') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'new_data/prenotebook_code/datascientist25_gender-recognition-by-voice-using-machine-learning.py'
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

Traceback (most recent call last):
  File ".\hai.py", line 8, in <module>
    run_one_hi('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1803, in run_one_hi
    self.run_origin_test(notebook_id, need_try_again=2)
NameError: name 'self' is not defined
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

save_code True
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 2, in <module>
ModuleNotFoundError: No module named 'pandas'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 2, in <module>
ModuleNotFoundError: No module named 'pandas'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ".\hai.py", line 8, in <module>
    run_one_hi('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1803, in run_one_hi
    pro.run_origin_test(notebook_id, need_try_again=2)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 805, in run_origin_test
    self.code = self.run_one_code(notebook_id, self.code, dataset_root_path + self.info_triple[notebook_id]['dataset_name'], 0)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 691, in run_one_code
    os.system(command)
KeyboardInterrupt
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

save_code True
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

[0;33;40m try times:0[0m
[0;31;40merror_str[0m No module named 'pandas'
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 4, in <module>
ModuleNotFoundError: No module named 'seaborn'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 4, in <module>
ModuleNotFoundError: No module named 'seaborn'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ".\hai.py", line 8, in <module>
    run_one_hi('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1803, in run_one_hi
    pro.run_origin_test(notebook_id, need_try_again=2)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 805, in run_origin_test
    self.code = self.run_one_code(notebook_id, self.code, dataset_root_path + self.info_triple[notebook_id]['dataset_name'], 0)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 691, in run_one_code
    os.system(command)
KeyboardInterrupt
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

save_code True
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

[0;33;40m try times:0[0m
[0;31;40merror_str[0m No module named 'seaborn'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 3, in <module>
ModuleNotFoundError: No module named 'matplotlib'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 3, in <module>
ModuleNotFoundError: No module named 'matplotlib'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 4, in <module>
ModuleNotFoundError: No module named 'seaborn'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 3, in <module>
ModuleNotFoundError: No module named 'matplotlib'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 4, in <module>
ModuleNotFoundError: No module named 'seaborn'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 3, in <module>
ModuleNotFoundError: No module named 'matplotlib'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 4, in <module>
ModuleNotFoundError: No module named 'seaborn'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 121, in <module>
  File "<__array_function__ internals>", line 5, in save
  File "D:\Anaconda3\envs\firefly\lib\site-packages\numpy\lib\npyio.py", line 525, in save
    file_ctx = open(file, "wb")
FileNotFoundError: [Errno 2] No such file or directory: 'new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy'
Traceback (most recent call last):
  File ".\hai.py", line 8, in <module>
    run_one_hi('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1803, in run_one_hi
    pro.run_origin_test(notebook_id, need_try_again=2)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 808, in run_origin_test
    self.save_code('new_data/runned_notebook/')
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 554, in save_code
    with open(root_path+ str(self.notebook_id) + '.py', 'w') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'new_data/runned_notebook/datascientist25_gender-recognition-by-voice-using-machine-learning.py'
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

save_code True
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

[0;33;40m try times:0[0m
[0;31;40merror_str[0m No module named 'matplotlib'
package lack error
[0;33;40m try times:1[0m
[0;31;40merror_str[0m No module named 'seaborn'
package lack error
[0;33;40m try times:2[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:3[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;31;40merror_str[0m [Errno 2] No such file or directory: 'new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy'
path error
[0;33;40m try times:4[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 121, in <module>
  File "<__array_function__ internals>", line 5, in save
  File "D:\Anaconda3\envs\firefly\lib\site-packages\numpy\lib\npyio.py", line 525, in save
    file_ctx = open(file, "wb")
FileNotFoundError: [Errno 2] No such file or directory: 'new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy'
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

save_code True
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;31;40merror_str[0m [Errno 2] No such file or directory: 'new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy'
path error
[0;33;40m try times:2[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

save_code True
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
Traceback (most recent call last):
  File ".\hai.py", line 5, in <module>
    from NotebookGraph import build_one_graph
  File "E:\firefly\HAIPipeGen\NotebookGraph.py", line 6, in <module>
    from graphviz import Digraph
ModuleNotFoundError: No module named 'graphviz'
Traceback (most recent call last):
  File ".\hai.py", line 5, in <module>
    from NotebookGraph import build_one_graph
  File "E:\firefly\HAIPipeGen\NotebookGraph.py", line 6, in <module>
    from graphviz import Digraph
ModuleNotFoundError: No module named 'graphviz'
Traceback (most recent call last):
  File ".\hai.py", line 10, in <module>
    build_one_graph(datascientist25_gender-recognition-by-voice-using-machine-learning)
NameError: name 'datascientist25_gender' is not defined
ASSIGN******

df = pd.read_csv('../input/voicegender/voice.csv')

[1;33;44m{'pd'}[0m
    ###########
    create black edge
    black code: 
df = pd.read_csv('../input/voicegender/voice.csv')

self.get_varible_results ['df', 'pd']
    varibles ['df']
EXPR******

df.columns

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.columns

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.shape

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.shape

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.dtypes

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.dtypes

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.head(3)

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <NotebookGraph.Node object at 0x000001FDE05970D0>
should_create_black_edge False
EXPR******

df.isnull().values.any()

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.isnull().values.any()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <NotebookGraph.Node object at 0x000001FDE0597280>
should_create_black_edge False
ASSIGN******

colors = ['pink', 'Lightblue']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['pink', 'Lightblue']

self.get_varible_results ['colors']
    varibles ['colors']
ASSIGN******

data_y = df[df.columns[(- 1)]]

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
data_y = df[df.columns[(- 1)]]

self.get_varible_results ['data_y', 'df', 'df']
    varibles ['df']
EXPR******

print(df['label'].value_counts())

self.should_create_white_edge True
    ###########
    create white edge
    white code: print(df['label'].value_counts())

self.get_varible_results [('print', False), ('df', True)]
should_create_black_edge True
    ###########
    create black edge
    black code: 
print(df['label'].value_counts())

self.get_varible_results ['print', 'df']
    varibles ['df']
ASSIGN******

correlation = df.corr()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df.corr()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df', 'colors', 'data_y'])
    ###########
    create assign edge
    assign target: ['correlation']
    assign code: 
correlation = df.corr()

ASSIGN******

X = df[df.columns[:(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
X = df[df.columns[:(- 1)]].values

self.get_varible_results ['X', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

y = df[df.columns[(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
y = df[df.columns[(- 1)]].values

self.get_varible_results ['y', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(X, y, test_size=0.3)

self.get_varible_results [('train_test_split', False), ('X', False), ('y', False)]
    white varibles: ['X', 'y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

EXPR******

warnings.filterwarnings('ignore')

self.should_create_white_edge True
    ###########
    create white edge
    white code: warnings.filterwarnings('ignore')

self.get_varible_results [('warnings', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
warnings.filterwarnings('ignore')

self.get_varible_results ['warnings']
    varibles []
ASSIGN******

rand_forest = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
rand_forest = RandomForestClassifier()

self.get_varible_results ['rand_forest', 'RandomForestClassifier']
    varibles ['rand_forest']
ASSIGN******

CVFirst = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
CVFirst = GaussianNB()

self.get_varible_results ['CVFirst', 'GaussianNB']
    varibles ['CVFirst']
ASSIGN******

male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

self.get_varible_results ['male_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

self.get_varible_results ['female_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

[1;33;44m{'list'}[0m
    ###########
    create black edge
    black code: 
index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

self.get_varible_results ['index_to_remove', 'list', 'male_funFreq_outlier_index', 'list', 'female_funFreq_outlier_index']
    varibles ['male_funFreq_outlier_index', 'female_funFreq_outlier_index']
var male_funFreq_outlier_index
var female_funFreq_outlier_index
EXPR******

len(index_to_remove)

self.should_create_white_edge True
    ###########
    create white edge
    white code: len(index_to_remove)

self.get_varible_results [('len', False), ('index_to_remove', False)]
    white varibles: ['index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove'])
Expr white_edge_node <NotebookGraph.Node object at 0x000001FDE05972E0>
should_create_black_edge False
ASSIGN******

data_x = df[df.columns[0:20]].copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df[df.columns[0:20]].copy()

self.get_varible_results [('df', True), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = df[df.columns[0:20]].copy()

self.get_varible_results ['data_x', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

self.get_varible_results [('data_x', False)]
    white varibles: ['data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

EXPR******

data2.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: data2.head(3)

self.get_varible_results [('data2', False)]
    white varibles: ['data2']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
Expr white_edge_node <NotebookGraph.Node object at 0x000001FDE0597BB0>
should_create_black_edge False
ASSIGN******

data2 = data2.drop(index_to_remove, axis=0)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data2.drop(index_to_remove, axis=0)

self.get_varible_results [('data2', False), ('index_to_remove', False)]
    white varibles: ['data2', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data2.drop(index_to_remove, axis=0)

ASSIGN******

data_y = pd.Series(y).drop(index_to_remove, axis=0)

[1;33;44m{'pd'}[0m
    ###########
    create white edge
    white code: pd.Series(y).drop(index_to_remove, axis=0)

self.get_varible_results [('pd', False), ('y', False), ('index_to_remove', False)]
    white varibles: ['y', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data_y']
    assign code: 
data_y = pd.Series(y).drop(index_to_remove, axis=0)

ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

self.get_varible_results [('train_test_split', False), ('data2', False), ('data_y', False)]
    white varibles: ['data2', 'data_y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

ASSIGN******

clf1 = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
clf1 = RandomForestClassifier()

self.get_varible_results ['clf1', 'RandomForestClassifier']
    varibles ['clf1']
ASSIGN******

clf2 = DecisionTreeClassifier()

[1;33;44m{'DecisionTreeClassifier'}[0m
    ###########
    create black edge
    black code: 
clf2 = DecisionTreeClassifier()

self.get_varible_results ['clf2', 'DecisionTreeClassifier']
    varibles ['clf2']
ASSIGN******

clf3 = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
clf3 = GaussianNB()

self.get_varible_results ['clf3', 'GaussianNB']
    varibles ['clf3']
ASSIGN******

clf4 = LogisticRegression()

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
clf4 = LogisticRegression()

self.get_varible_results ['clf4', 'LogisticRegression']
    varibles ['clf4']
ASSIGN******

labels = ['female', 'male']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = ['female', 'male']

self.get_varible_results ['labels']
    varibles ['labels']
EXPR******

pl.title('Confusion matrix of the classifier')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.title('Confusion matrix of the classifier')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.title('Confusion matrix of the classifier')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.xlabel('Predicted')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.xlabel('Predicted')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.xlabel('Predicted')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.ylabel('True')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.ylabel('True')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.ylabel('True')

self.get_varible_results ['pl']
    varibles []
EXPR******

style.use('ggplot')

self.should_create_white_edge True
    ###########
    create white edge
    white code: style.use('ggplot')

self.get_varible_results [('style', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
style.use('ggplot')

self.get_varible_results ['style']
    varibles []
ASSIGN******

data_x = np.array(df[['meanfreq', 'meanfun']])

[1;33;44m{'np'}[0m
    ###########
    create white edge
    white code: np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results [('np', False), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results ['data_x', 'np', 'df']
    varibles ['data_x', 'df']
var data_x
var df
ASSIGN******

kmeans = KMeans(n_clusters=2)

[1;33;44m{'KMeans'}[0m
    ###########
    create black edge
    black code: 
kmeans = KMeans(n_clusters=2)

self.get_varible_results ['kmeans', 'KMeans']
    varibles ['kmeans']
EXPR******

kmeans.fit(data_x)

self.should_create_white_edge True
    ###########
    create white edge
    white code: kmeans.fit(data_x)

self.get_varible_results [('kmeans', False), ('data_x', False)]
    white varibles: ['kmeans', 'data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans'])
Expr white_edge_node <NotebookGraph.Node object at 0x000001FDE0597E50>
should_create_black_edge False
ASSIGN******

centroids = kmeans.cluster_centers_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
centroids = kmeans.cluster_centers_

self.get_varible_results ['centroids', 'kmeans']
    varibles ['kmeans']
var kmeans
ASSIGN******

labels = kmeans.labels_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = kmeans.labels_

self.get_varible_results ['labels', 'kmeans']
    varibles ['labels', 'kmeans']
var labels
var kmeans
ASSIGN******

colors = ['g.', 'b.']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['g.', 'b.']

self.get_varible_results ['colors']
    varibles ['colors']
EXPR******

print('start running model training........')

self.should_create_white_edge True
    ###########
    create white edge
    white code: print('start running model training........')

self.get_varible_results [('print', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
print('start running model training........')

self.get_varible_results ['print']
    varibles []
ASSIGN******

model = LogisticRegression(solver='liblinear', random_state=0)

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
model = LogisticRegression(solver='liblinear', random_state=0)

self.get_varible_results ['model', 'LogisticRegression']
    varibles ['model']
EXPR******

model.fit(Xtrain, ytrain)

self.should_create_white_edge True
    ###########
    create white edge
    white code: model.fit(Xtrain, ytrain)

self.get_varible_results [('model', False), ('Xtrain', False), ('ytrain', False)]
    white varibles: ['model', 'Xtrain', 'ytrain']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
Expr white_edge_node <NotebookGraph.Node object at 0x000001FDE05AF6D0>
should_create_black_edge False
ASSIGN******

y_pred = model.predict(Xtest)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: model.predict(Xtest)

self.get_varible_results [('model', False), ('Xtest', False)]
    white varibles: ['model', 'Xtest']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
    ###########
    create assign edge
    assign target: ['y_pred']
    assign code: 
y_pred = model.predict(Xtest)

ASSIGN******

score = accuracy_score(ytest, y_pred)

[1;33;44m{'accuracy_score'}[0m
    ###########
    create white edge
    white code: accuracy_score(ytest, y_pred)

self.get_varible_results [('accuracy_score', False), ('ytest', False), ('y_pred', False)]
    white varibles: ['ytest', 'y_pred']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred'])
    ###########
    create assign edge
    assign target: ['score']
    assign code: 
score = accuracy_score(ytest, y_pred)

EXPR******

np.save('new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.should_create_white_edge True
    ###########
    create white edge
    white code: np.save('new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.get_varible_results [('np', False), ('score', False)]
    white varibles: ['score']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred', '-RESULT-accuracy_score', 'score'])
Expr white_edge_node <NotebookGraph.Node object at 0x000001FDE05AFA30>
should_create_black_edge False
predict model.predict(Xtest)

predict model.predict(Xtest)

self.model_variable ['model', 'model']
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name correlation
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name X
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name male_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name female_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name index_to_remove
self.nodes[node_index].varible_name -RESULT-len
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
model
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name centroids
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
y_pred
self.nodes[node_index].varible_name y_pred
-RESULT-accuracy_score
self.nodes[node_index].varible_name -RESULT-accuracy_score
score
self.nodes[node_index].varible_name score
-RESULT-np.save
self.nodes[node_index].varible_name -RESULT-np.save
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import matplotlib.pyplot as plt
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import seaborn as sns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.columns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.shape
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.dtypes
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.isnull().values.any()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ['pink','Lightblue']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = df[df.columns[-1]]
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.axis('equal')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line print (df['label'].value_counts())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line correlation =df.corr()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.heatmap(correlation)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line X = df[df.columns[:-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y = df[df.columns[-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import warnings
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line warnings.filterwarnings("ignore")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.ensemble import RandomForestClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line rand_forest = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = rand_forest.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn import metrics, neighbors
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import confusion_matrix
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(confusion_matrix(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.naive_bayes import GaussianNB
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import cross_val_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line CVFirst = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line len(index_to_remove)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = df[df.columns[0:20]].copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
##############################
line_index 45
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy() edge data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

line_id [45]
[45]
[45]
xxxx
line data2.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf1 = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf1.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = clf1.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.tree import DecisionTreeClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf2 = DecisionTreeClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf2.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict = clf2.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf3 = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predd = clf3.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf4 = LogisticRegression()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf4.fit(Xtrain,ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict4 = clf4.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pylab as pl
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = ['female', 'male']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig = plt.figure()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax = fig.add_subplot(111)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cax =ax.matshow(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.title('Confusion matrix of the classifier')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig.colorbar(cax)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_xticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_yticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.xlabel('Predicted')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.ylabel('True')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #pl.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import classification_report
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(classification_report(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.cluster import KMeans
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from matplotlib import style
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line style.use("ggplot")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans = KMeans(n_clusters= 2)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans.fit(data_x)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line centroids = kmeans.cluster_centers_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = kmeans.labels_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ["g.","b."]  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #for i in range(len(data_x)):
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line     
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.ylabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.xlabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line print("start running model training........")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y_pred = model.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line score = accuracy_score(ytest, y_pred)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
ASSIGN******

df = pd.read_csv('../input/voicegender/voice.csv')

[1;33;44m{'pd'}[0m
    ###########
    create black edge
    black code: 
df = pd.read_csv('../input/voicegender/voice.csv')

self.get_varible_results ['df', 'pd']
    varibles ['df']
EXPR******

df.columns

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.columns

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.shape

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.shape

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.dtypes

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.dtypes

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.head(3)

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <NotebookGraph.Node object at 0x000002713A29A0D0>
should_create_black_edge False
EXPR******

df.isnull().values.any()

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.isnull().values.any()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <NotebookGraph.Node object at 0x000002713A29A250>
should_create_black_edge False
ASSIGN******

colors = ['pink', 'Lightblue']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['pink', 'Lightblue']

self.get_varible_results ['colors']
    varibles ['colors']
ASSIGN******

data_y = df[df.columns[(- 1)]]

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
data_y = df[df.columns[(- 1)]]

self.get_varible_results ['data_y', 'df', 'df']
    varibles ['df']
EXPR******

print(df['label'].value_counts())

self.should_create_white_edge True
    ###########
    create white edge
    white code: print(df['label'].value_counts())

self.get_varible_results [('print', False), ('df', True)]
should_create_black_edge True
    ###########
    create black edge
    black code: 
print(df['label'].value_counts())

self.get_varible_results ['print', 'df']
    varibles ['df']
ASSIGN******

correlation = df.corr()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df.corr()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df', 'colors', 'data_y'])
    ###########
    create assign edge
    assign target: ['correlation']
    assign code: 
correlation = df.corr()

ASSIGN******

X = df[df.columns[:(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
X = df[df.columns[:(- 1)]].values

self.get_varible_results ['X', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

y = df[df.columns[(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
y = df[df.columns[(- 1)]].values

self.get_varible_results ['y', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(X, y, test_size=0.3)

self.get_varible_results [('train_test_split', False), ('X', False), ('y', False)]
    white varibles: ['X', 'y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

EXPR******

warnings.filterwarnings('ignore')

self.should_create_white_edge True
    ###########
    create white edge
    white code: warnings.filterwarnings('ignore')

self.get_varible_results [('warnings', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
warnings.filterwarnings('ignore')

self.get_varible_results ['warnings']
    varibles []
ASSIGN******

rand_forest = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
rand_forest = RandomForestClassifier()

self.get_varible_results ['rand_forest', 'RandomForestClassifier']
    varibles ['rand_forest']
ASSIGN******

CVFirst = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
CVFirst = GaussianNB()

self.get_varible_results ['CVFirst', 'GaussianNB']
    varibles ['CVFirst']
ASSIGN******

male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

self.get_varible_results ['male_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

self.get_varible_results ['female_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

[1;33;44m{'list'}[0m
    ###########
    create black edge
    black code: 
index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

self.get_varible_results ['index_to_remove', 'list', 'male_funFreq_outlier_index', 'list', 'female_funFreq_outlier_index']
    varibles ['male_funFreq_outlier_index', 'female_funFreq_outlier_index']
var male_funFreq_outlier_index
var female_funFreq_outlier_index
EXPR******

len(index_to_remove)

self.should_create_white_edge True
    ###########
    create white edge
    white code: len(index_to_remove)

self.get_varible_results [('len', False), ('index_to_remove', False)]
    white varibles: ['index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove'])
Expr white_edge_node <NotebookGraph.Node object at 0x000002713A29A2B0>
should_create_black_edge False
ASSIGN******

data_x = df[df.columns[0:20]].copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df[df.columns[0:20]].copy()

self.get_varible_results [('df', True), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = df[df.columns[0:20]].copy()

self.get_varible_results ['data_x', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

self.get_varible_results [('data_x', False)]
    white varibles: ['data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

EXPR******

data2.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: data2.head(3)

self.get_varible_results [('data2', False)]
    white varibles: ['data2']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
Expr white_edge_node <NotebookGraph.Node object at 0x000002713A29ABB0>
should_create_black_edge False
ASSIGN******

data2 = data2.drop(index_to_remove, axis=0)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data2.drop(index_to_remove, axis=0)

self.get_varible_results [('data2', False), ('index_to_remove', False)]
    white varibles: ['data2', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data2.drop(index_to_remove, axis=0)

ASSIGN******

data_y = pd.Series(y).drop(index_to_remove, axis=0)

[1;33;44m{'pd'}[0m
    ###########
    create white edge
    white code: pd.Series(y).drop(index_to_remove, axis=0)

self.get_varible_results [('pd', False), ('y', False), ('index_to_remove', False)]
    white varibles: ['y', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data_y']
    assign code: 
data_y = pd.Series(y).drop(index_to_remove, axis=0)

ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

self.get_varible_results [('train_test_split', False), ('data2', False), ('data_y', False)]
    white varibles: ['data2', 'data_y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

ASSIGN******

clf1 = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
clf1 = RandomForestClassifier()

self.get_varible_results ['clf1', 'RandomForestClassifier']
    varibles ['clf1']
ASSIGN******

clf2 = DecisionTreeClassifier()

[1;33;44m{'DecisionTreeClassifier'}[0m
    ###########
    create black edge
    black code: 
clf2 = DecisionTreeClassifier()

self.get_varible_results ['clf2', 'DecisionTreeClassifier']
    varibles ['clf2']
ASSIGN******

clf3 = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
clf3 = GaussianNB()

self.get_varible_results ['clf3', 'GaussianNB']
    varibles ['clf3']
ASSIGN******

clf4 = LogisticRegression()

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
clf4 = LogisticRegression()

self.get_varible_results ['clf4', 'LogisticRegression']
    varibles ['clf4']
ASSIGN******

labels = ['female', 'male']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = ['female', 'male']

self.get_varible_results ['labels']
    varibles ['labels']
EXPR******

pl.title('Confusion matrix of the classifier')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.title('Confusion matrix of the classifier')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.title('Confusion matrix of the classifier')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.xlabel('Predicted')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.xlabel('Predicted')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.xlabel('Predicted')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.ylabel('True')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.ylabel('True')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.ylabel('True')

self.get_varible_results ['pl']
    varibles []
EXPR******

style.use('ggplot')

self.should_create_white_edge True
    ###########
    create white edge
    white code: style.use('ggplot')

self.get_varible_results [('style', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
style.use('ggplot')

self.get_varible_results ['style']
    varibles []
ASSIGN******

data_x = np.array(df[['meanfreq', 'meanfun']])

[1;33;44m{'np'}[0m
    ###########
    create white edge
    white code: np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results [('np', False), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results ['data_x', 'np', 'df']
    varibles ['data_x', 'df']
var data_x
var df
ASSIGN******

kmeans = KMeans(n_clusters=2)

[1;33;44m{'KMeans'}[0m
    ###########
    create black edge
    black code: 
kmeans = KMeans(n_clusters=2)

self.get_varible_results ['kmeans', 'KMeans']
    varibles ['kmeans']
EXPR******

kmeans.fit(data_x)

self.should_create_white_edge True
    ###########
    create white edge
    white code: kmeans.fit(data_x)

self.get_varible_results [('kmeans', False), ('data_x', False)]
    white varibles: ['kmeans', 'data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans'])
Expr white_edge_node <NotebookGraph.Node object at 0x000002713A29AE80>
should_create_black_edge False
ASSIGN******

centroids = kmeans.cluster_centers_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
centroids = kmeans.cluster_centers_

self.get_varible_results ['centroids', 'kmeans']
    varibles ['kmeans']
var kmeans
ASSIGN******

labels = kmeans.labels_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = kmeans.labels_

self.get_varible_results ['labels', 'kmeans']
    varibles ['labels', 'kmeans']
var labels
var kmeans
ASSIGN******

colors = ['g.', 'b.']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['g.', 'b.']

self.get_varible_results ['colors']
    varibles ['colors']
EXPR******

print('start running model training........')

self.should_create_white_edge True
    ###########
    create white edge
    white code: print('start running model training........')

self.get_varible_results [('print', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
print('start running model training........')

self.get_varible_results ['print']
    varibles []
ASSIGN******

model = LogisticRegression(solver='liblinear', random_state=0)

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
model = LogisticRegression(solver='liblinear', random_state=0)

self.get_varible_results ['model', 'LogisticRegression']
    varibles ['model']
EXPR******

model.fit(Xtrain, ytrain)

self.should_create_white_edge True
    ###########
    create white edge
    white code: model.fit(Xtrain, ytrain)

self.get_varible_results [('model', False), ('Xtrain', False), ('ytrain', False)]
    white varibles: ['model', 'Xtrain', 'ytrain']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
Expr white_edge_node <NotebookGraph.Node object at 0x000002713A2AE6A0>
should_create_black_edge False
ASSIGN******

y_pred = model.predict(Xtest)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: model.predict(Xtest)

self.get_varible_results [('model', False), ('Xtest', False)]
    white varibles: ['model', 'Xtest']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
    ###########
    create assign edge
    assign target: ['y_pred']
    assign code: 
y_pred = model.predict(Xtest)

ASSIGN******

score = accuracy_score(ytest, y_pred)

[1;33;44m{'accuracy_score'}[0m
    ###########
    create white edge
    white code: accuracy_score(ytest, y_pred)

self.get_varible_results [('accuracy_score', False), ('ytest', False), ('y_pred', False)]
    white varibles: ['ytest', 'y_pred']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred'])
    ###########
    create assign edge
    assign target: ['score']
    assign code: 
score = accuracy_score(ytest, y_pred)

EXPR******

np.save('new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.should_create_white_edge True
    ###########
    create white edge
    white code: np.save('new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.get_varible_results [('np', False), ('score', False)]
    white varibles: ['score']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred', '-RESULT-accuracy_score', 'score'])
Expr white_edge_node <NotebookGraph.Node object at 0x000002713A2AEA00>
should_create_black_edge False
predict model.predict(Xtest)

predict model.predict(Xtest)

self.model_variable ['model', 'model']
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name correlation
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name X
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name male_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name female_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name index_to_remove
self.nodes[node_index].varible_name -RESULT-len
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
model
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name centroids
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
y_pred
self.nodes[node_index].varible_name y_pred
-RESULT-accuracy_score
self.nodes[node_index].varible_name -RESULT-accuracy_score
score
self.nodes[node_index].varible_name score
-RESULT-np.save
self.nodes[node_index].varible_name -RESULT-np.save
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import matplotlib.pyplot as plt
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import seaborn as sns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.columns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.shape
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.dtypes
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.isnull().values.any()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ['pink','Lightblue']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = df[df.columns[-1]]
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.axis('equal')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line print (df['label'].value_counts())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line correlation =df.corr()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.heatmap(correlation)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line X = df[df.columns[:-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y = df[df.columns[-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import warnings
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line warnings.filterwarnings("ignore")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.ensemble import RandomForestClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line rand_forest = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = rand_forest.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn import metrics, neighbors
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import confusion_matrix
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(confusion_matrix(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.naive_bayes import GaussianNB
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import cross_val_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line CVFirst = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line len(index_to_remove)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = df[df.columns[0:20]].copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
##############################
line_index 45
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy() edge data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

line_id [45]
[45]
[45]
xxxx
line data2.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf1 = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf1.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = clf1.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.tree import DecisionTreeClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf2 = DecisionTreeClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf2.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict = clf2.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf3 = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predd = clf3.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf4 = LogisticRegression()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf4.fit(Xtrain,ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict4 = clf4.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pylab as pl
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = ['female', 'male']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig = plt.figure()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax = fig.add_subplot(111)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cax =ax.matshow(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.title('Confusion matrix of the classifier')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig.colorbar(cax)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_xticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_yticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.xlabel('Predicted')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.ylabel('True')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #pl.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import classification_report
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(classification_report(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.cluster import KMeans
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from matplotlib import style
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line style.use("ggplot")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans = KMeans(n_clusters= 2)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans.fit(data_x)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line centroids = kmeans.cluster_centers_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = kmeans.labels_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ["g.","b."]  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #for i in range(len(data_x)):
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line     
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.ylabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.xlabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line print("start running model training........")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y_pred = model.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line score = accuracy_score(ytest, y_pred)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
Traceback (most recent call last):
  File ".\hai.py", line 14, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 12, in generate_hai
    merge_one_code(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 2459, in merge_one_code
    merger.merging_one_notebook_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 790, in merging_one_notebook_rl
    res = self.enum_adding_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 271, in enum_adding_rl
    res = self.load_one_rl_operation(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 139, in load_one_rl_operation
    found_task = dataset_label[notebook_id]['dataset']+'_'+info_triple[noteook_id]['model']+'_'+dataset_label[notebook_id]['label']
NameError: name 'noteook_id' is not defined
Traceback (most recent call last):
  File ".\hai.py", line 14, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 12, in generate_hai
    merge_one_code(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 2459, in merge_one_code
    merger.merging_one_notebook_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 790, in merging_one_notebook_rl
    res = self.enum_adding_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 271, in enum_adding_rl
    res = self.load_one_rl_operation(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 139, in load_one_rl_operation
    found_task = dataset_label[notebook_id]['dataset']+'_'+info_triple[notebook_id]['model']+'_'+dataset_label[notebook_id]['label']
KeyError: 'model'
Traceback (most recent call last):
  File ".\hai.py", line 14, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 12, in generate_hai
    merge_one_code(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 2459, in merge_one_code
    merger.merging_one_notebook_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 790, in merging_one_notebook_rl
    res = self.enum_adding_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 271, in enum_adding_rl
    res = self.load_one_rl_operation(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 139, in load_one_rl_operation
    found_task = dataset_label[notebook_id]['dataset']+'_'+info_triple[notebook_id]['model_type']+'_'+dataset_label[notebook_id]['label']
KeyError: 'label'
Traceback (most recent call last):
  File ".\hai.py", line 14, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 12, in generate_hai
    merge_one_code(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 2459, in merge_one_code
    merger.merging_one_notebook_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 799, in merging_one_notebook_rl
    self.load_origin_code(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 78, in load_origin_code
    with open(output_path, 'r') as src_file:
FileNotFoundError: [Errno 2] No such file or directory: '/home/yxm/staticfg-master/new_data/prenotebook_code/datascientist25_gender-recognition-by-voice-using-machine-learning.py'
[[<merge.Step object at 0x000001B4D7F382B0>],
 [<merge.Step object at 0x000001B4D7F38280>],
 [<merge.Step object at 0x000001B4D7F381C0>],
 [<merge.Step object at 0x000001B4D7F382E0>],
 [<merge.Step object at 0x000001B4D7F38340>],
 [<merge.Step object at 0x000001B4D7F383A0>],
 [<merge.Step object at 0x000001B4D7F38460>],
 [<merge.Step object at 0x000001B4D7F38E50>],
 [<merge.Step object at 0x000001B4D7F38550>],
 [<merge.Step object at 0x000001B4D7F382B0>,
  <merge.Step object at 0x000001B4D7F38550>],
 [<merge.Step object at 0x000001B4D7F381C0>,
  <merge.Step object at 0x000001B4D7F38550>],
 [<merge.Step object at 0x000001B4D7F38340>,
  <merge.Step object at 0x000001B4D7F38550>],
 [<merge.Step object at 0x000001B4D7F38640>],
 [<merge.Step object at 0x000001B4D7F382B0>,
  <merge.Step object at 0x000001B4D7F38640>],
 [<merge.Step object at 0x000001B4D7F38280>,
  <merge.Step object at 0x000001B4D7F38640>],
 [<merge.Step object at 0x000001B4D7F381C0>,
  <merge.Step object at 0x000001B4D7F38640>],
 [<merge.Step object at 0x000001B4D7F382E0>,
  <merge.Step object at 0x000001B4D7F38640>],
 [<merge.Step object at 0x000001B4D7F38340>,
  <merge.Step object at 0x000001B4D7F38640>],
 [<merge.Step object at 0x000001B4D7F383A0>,
  <merge.Step object at 0x000001B4D7F38640>],
 [<merge.Step object at 0x000001B4D7F38790>],
 [<merge.Step object at 0x000001B4D7F381C0>,
  <merge.Step object at 0x000001B4D7F38790>],
 [<merge.Step object at 0x000001B4D7F38340>,
  <merge.Step object at 0x000001B4D7F38790>],
 [<merge.Step object at 0x000001B4D7F38610>],
 [<merge.Step object at 0x000001B4D7F381C0>,
  <merge.Step object at 0x000001B4D7F38610>],
 [<merge.Step object at 0x000001B4D7F382E0>,
  <merge.Step object at 0x000001B4D7F38610>],
 [<merge.Step object at 0x000001B4D7F38340>,
  <merge.Step object at 0x000001B4D7F38610>],
 [<merge.Step object at 0x000001B4D7F383A0>,
  <merge.Step object at 0x000001B4D7F38610>],
 [<merge.Step object at 0x000001B4D7F38940>],
 [<merge.Step object at 0x000001B4D7F38340>,
  <merge.Step object at 0x000001B4D7F38940>],
 [<merge.Step object at 0x000001B4D7F38A60>],
 [<merge.Step object at 0x000001B4D7F38340>,
  <merge.Step object at 0x000001B4D7F38A60>],
 [<merge.Step object at 0x000001B4D7F383A0>,
  <merge.Step object at 0x000001B4D7F38A60>],
 [<merge.Step object at 0x000001B4D7F38B50>],
 [<merge.Step object at 0x000001B4D7F382B0>,
  <merge.Step object at 0x000001B4D7F38B50>],
 [<merge.Step object at 0x000001B4D7F381C0>,
  <merge.Step object at 0x000001B4D7F38B50>],
 [<merge.Step object at 0x000001B4D7F38340>,
  <merge.Step object at 0x000001B4D7F38B50>],
 [<merge.Step object at 0x000001B4D7F38460>,
  <merge.Step object at 0x000001B4D7F38B50>],
 [<merge.Step object at 0x000001B4D7F38BE0>],
 [<merge.Step object at 0x000001B4D7F382B0>,
  <merge.Step object at 0x000001B4D7F38BE0>],
 [<merge.Step object at 0x000001B4D7F38280>,
  <merge.Step object at 0x000001B4D7F38BE0>],
 [<merge.Step object at 0x000001B4D7F381C0>,
  <merge.Step object at 0x000001B4D7F38BE0>],
 [<merge.Step object at 0x000001B4D7F382E0>,
  <merge.Step object at 0x000001B4D7F38BE0>],
 [<merge.Step object at 0x000001B4D7F38340>,
  <merge.Step object at 0x000001B4D7F38BE0>],
 [<merge.Step object at 0x000001B4D7F383A0>,
  <merge.Step object at 0x000001B4D7F38BE0>],
 [<merge.Step object at 0x000001B4D7F38460>,
  <merge.Step object at 0x000001B4D7F38BE0>],
 [<merge.Step object at 0x000001B4D7F38E50>,
  <merge.Step object at 0x000001B4D7F38BE0>],
 [<merge.Step object at 0x000001B4D7995FD0>],
 [<merge.Step object at 0x000001B4D7F38EB0>,
  <merge.Step object at 0x000001B4D7F38FA0>],
 [<merge.Step object at 0x000001B4D7F38EE0>]]
('InteractionFeatures', 'end'),
('InteractionFeatures', 'end'),('VarianceThreshold', 'end'),
('VarianceThreshold', 'end'),
Traceback (most recent call last):
  File ".\hai.py", line 14, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 12, in generate_hai
    merge_one_code(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 2459, in merge_one_code
    merger.merging_one_notebook_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 830, in merging_one_notebook_rl
    self.add_one_ope(notebook_id, edge_id, now_ope, position, varible)
  File "E:\firefly\HAIPipeGen\merge.py", line 624, in add_one_ope
    with open("prenotebook_varibles_index/"+str(notebook_id)+'.json', 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'prenotebook_varibles_index/datascientist25_gender-recognition-by-voice-using-machine-learning.json'
[[<merge.Step object at 0x00000168D37F92B0>],
 [<merge.Step object at 0x00000168D37F9280>],
 [<merge.Step object at 0x00000168D37F91C0>],
 [<merge.Step object at 0x00000168D37F92E0>],
 [<merge.Step object at 0x00000168D37F9340>],
 [<merge.Step object at 0x00000168D37F93A0>],
 [<merge.Step object at 0x00000168D37F9460>],
 [<merge.Step object at 0x00000168D37F9E50>],
 [<merge.Step object at 0x00000168D37F9550>],
 [<merge.Step object at 0x00000168D37F92B0>,
  <merge.Step object at 0x00000168D37F9550>],
 [<merge.Step object at 0x00000168D37F91C0>,
  <merge.Step object at 0x00000168D37F9550>],
 [<merge.Step object at 0x00000168D37F9340>,
  <merge.Step object at 0x00000168D37F9550>],
 [<merge.Step object at 0x00000168D37F9640>],
 [<merge.Step object at 0x00000168D37F92B0>,
  <merge.Step object at 0x00000168D37F9640>],
 [<merge.Step object at 0x00000168D37F9280>,
  <merge.Step object at 0x00000168D37F9640>],
 [<merge.Step object at 0x00000168D37F91C0>,
  <merge.Step object at 0x00000168D37F9640>],
 [<merge.Step object at 0x00000168D37F92E0>,
  <merge.Step object at 0x00000168D37F9640>],
 [<merge.Step object at 0x00000168D37F9340>,
  <merge.Step object at 0x00000168D37F9640>],
 [<merge.Step object at 0x00000168D37F93A0>,
  <merge.Step object at 0x00000168D37F9640>],
 [<merge.Step object at 0x00000168D37F9790>],
 [<merge.Step object at 0x00000168D37F91C0>,
  <merge.Step object at 0x00000168D37F9790>],
 [<merge.Step object at 0x00000168D37F9340>,
  <merge.Step object at 0x00000168D37F9790>],
 [<merge.Step object at 0x00000168D37F9610>],
 [<merge.Step object at 0x00000168D37F91C0>,
  <merge.Step object at 0x00000168D37F9610>],
 [<merge.Step object at 0x00000168D37F92E0>,
  <merge.Step object at 0x00000168D37F9610>],
 [<merge.Step object at 0x00000168D37F9340>,
  <merge.Step object at 0x00000168D37F9610>],
 [<merge.Step object at 0x00000168D37F93A0>,
  <merge.Step object at 0x00000168D37F9610>],
 [<merge.Step object at 0x00000168D37F9940>],
 [<merge.Step object at 0x00000168D37F9340>,
  <merge.Step object at 0x00000168D37F9940>],
 [<merge.Step object at 0x00000168D37F9A60>],
 [<merge.Step object at 0x00000168D37F9340>,
  <merge.Step object at 0x00000168D37F9A60>],
 [<merge.Step object at 0x00000168D37F93A0>,
  <merge.Step object at 0x00000168D37F9A60>],
 [<merge.Step object at 0x00000168D37F9B50>],
 [<merge.Step object at 0x00000168D37F92B0>,
  <merge.Step object at 0x00000168D37F9B50>],
 [<merge.Step object at 0x00000168D37F91C0>,
  <merge.Step object at 0x00000168D37F9B50>],
 [<merge.Step object at 0x00000168D37F9340>,
  <merge.Step object at 0x00000168D37F9B50>],
 [<merge.Step object at 0x00000168D37F9460>,
  <merge.Step object at 0x00000168D37F9B50>],
 [<merge.Step object at 0x00000168D37F9BE0>],
 [<merge.Step object at 0x00000168D37F92B0>,
  <merge.Step object at 0x00000168D37F9BE0>],
 [<merge.Step object at 0x00000168D37F9280>,
  <merge.Step object at 0x00000168D37F9BE0>],
 [<merge.Step object at 0x00000168D37F91C0>,
  <merge.Step object at 0x00000168D37F9BE0>],
 [<merge.Step object at 0x00000168D37F92E0>,
  <merge.Step object at 0x00000168D37F9BE0>],
 [<merge.Step object at 0x00000168D37F9340>,
  <merge.Step object at 0x00000168D37F9BE0>],
 [<merge.Step object at 0x00000168D37F93A0>,
  <merge.Step object at 0x00000168D37F9BE0>],
 [<merge.Step object at 0x00000168D37F9460>,
  <merge.Step object at 0x00000168D37F9BE0>],
 [<merge.Step object at 0x00000168D37F9E50>,
  <merge.Step object at 0x00000168D37F9BE0>],
 [<merge.Step object at 0x00000168D3255FD0>],
 [<merge.Step object at 0x00000168D37F9EB0>,
  <merge.Step object at 0x00000168D37F9FA0>],
 [<merge.Step object at 0x00000168D37F9EE0>]]
('InteractionFeatures', 'end'),
('InteractionFeatures', 'end'),('VarianceThreshold', 'end'),
('VarianceThreshold', 'end'),
[[<merge.Step object at 0x0000015D0C9192B0>],
 [<merge.Step object at 0x0000015D0C919280>],
 [<merge.Step object at 0x0000015D0C9191C0>],
 [<merge.Step object at 0x0000015D0C9192E0>],
 [<merge.Step object at 0x0000015D0C919340>],
 [<merge.Step object at 0x0000015D0C9193A0>],
 [<merge.Step object at 0x0000015D0C919460>],
 [<merge.Step object at 0x0000015D0C919E20>],
 [<merge.Step object at 0x0000015D0C919520>],
 [<merge.Step object at 0x0000015D0C9192B0>,
  <merge.Step object at 0x0000015D0C919520>],
 [<merge.Step object at 0x0000015D0C9191C0>,
  <merge.Step object at 0x0000015D0C919520>],
 [<merge.Step object at 0x0000015D0C919340>,
  <merge.Step object at 0x0000015D0C919520>],
 [<merge.Step object at 0x0000015D0C9195E0>],
 [<merge.Step object at 0x0000015D0C9192B0>,
  <merge.Step object at 0x0000015D0C9195E0>],
 [<merge.Step object at 0x0000015D0C919280>,
  <merge.Step object at 0x0000015D0C9195E0>],
 [<merge.Step object at 0x0000015D0C9191C0>,
  <merge.Step object at 0x0000015D0C9195E0>],
 [<merge.Step object at 0x0000015D0C9192E0>,
  <merge.Step object at 0x0000015D0C9195E0>],
 [<merge.Step object at 0x0000015D0C919340>,
  <merge.Step object at 0x0000015D0C9195E0>],
 [<merge.Step object at 0x0000015D0C9193A0>,
  <merge.Step object at 0x0000015D0C9195E0>],
 [<merge.Step object at 0x0000015D0C919730>],
 [<merge.Step object at 0x0000015D0C9191C0>,
  <merge.Step object at 0x0000015D0C919730>],
 [<merge.Step object at 0x0000015D0C919340>,
  <merge.Step object at 0x0000015D0C919730>],
 [<merge.Step object at 0x0000015D0C9195B0>],
 [<merge.Step object at 0x0000015D0C9191C0>,
  <merge.Step object at 0x0000015D0C9195B0>],
 [<merge.Step object at 0x0000015D0C9192E0>,
  <merge.Step object at 0x0000015D0C9195B0>],
 [<merge.Step object at 0x0000015D0C919340>,
  <merge.Step object at 0x0000015D0C9195B0>],
 [<merge.Step object at 0x0000015D0C9193A0>,
  <merge.Step object at 0x0000015D0C9195B0>],
 [<merge.Step object at 0x0000015D0C9198E0>],
 [<merge.Step object at 0x0000015D0C919340>,
  <merge.Step object at 0x0000015D0C9198E0>],
 [<merge.Step object at 0x0000015D0C919A00>],
 [<merge.Step object at 0x0000015D0C919340>,
  <merge.Step object at 0x0000015D0C919A00>],
 [<merge.Step object at 0x0000015D0C9193A0>,
  <merge.Step object at 0x0000015D0C919A00>],
 [<merge.Step object at 0x0000015D0C919AF0>],
 [<merge.Step object at 0x0000015D0C9192B0>,
  <merge.Step object at 0x0000015D0C919AF0>],
 [<merge.Step object at 0x0000015D0C9191C0>,
  <merge.Step object at 0x0000015D0C919AF0>],
 [<merge.Step object at 0x0000015D0C919340>,
  <merge.Step object at 0x0000015D0C919AF0>],
 [<merge.Step object at 0x0000015D0C919460>,
  <merge.Step object at 0x0000015D0C919AF0>],
 [<merge.Step object at 0x0000015D0C919BB0>],
 [<merge.Step object at 0x0000015D0C9192B0>,
  <merge.Step object at 0x0000015D0C919BB0>],
 [<merge.Step object at 0x0000015D0C919280>,
  <merge.Step object at 0x0000015D0C919BB0>],
 [<merge.Step object at 0x0000015D0C9191C0>,
  <merge.Step object at 0x0000015D0C919BB0>],
 [<merge.Step object at 0x0000015D0C9192E0>,
  <merge.Step object at 0x0000015D0C919BB0>],
 [<merge.Step object at 0x0000015D0C919340>,
  <merge.Step object at 0x0000015D0C919BB0>],
 [<merge.Step object at 0x0000015D0C9193A0>,
  <merge.Step object at 0x0000015D0C919BB0>],
 [<merge.Step object at 0x0000015D0C919460>,
  <merge.Step object at 0x0000015D0C919BB0>],
 [<merge.Step object at 0x0000015D0C919E20>,
  <merge.Step object at 0x0000015D0C919BB0>],
 [<merge.Step object at 0x0000015D0C375FD0>],
 [<merge.Step object at 0x0000015D0C919E80>,
  <merge.Step object at 0x0000015D0C919F40>],
 [<merge.Step object at 0x0000015D0C919FA0>]]
('InteractionFeatures', 'end'),
('InteractionFeatures', 'end'),('VarianceThreshold', 'end'),
('VarianceThreshold', 'end'),
Traceback (most recent call last):
  File ".\hai.py", line 14, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 12, in generate_hai
    merge_one_code(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 2566, in merge_one_code
    transform_one_validation_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 2551, in transform_one_validation_rl
    os.mkdir('new_data/rl_cross_validation_code/'+notebook_id)
FileNotFoundError: [WinError 3] 系统找不到指定的路径。: 'new_data/rl_cross_validation_code/datascientist25_gender-recognition-by-voice-using-machine-learning'
Traceback (most recent call last):
  File ".\hai.py", line 14, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 12, in generate_hai
    merge_one_code(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 2570, in merge_one_code
    transform_one_validation_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 2553, in transform_one_validation_rl
    os.mkdir('new_data/rl_cross_validation_code_py/'+notebook_id)
FileNotFoundError: [WinError 3] 系统找不到指定的路径。: 'new_data/rl_cross_validation_code_py/datascientist25_gender-recognition-by-voice-using-machine-learning'
Traceback (most recent call last):
  File ".\hai.py", line 14, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 12, in generate_hai
    merge_one_code(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 2662, in merge_one_code
    transform_one_origin_validation_code(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 2643, in transform_one_origin_validation_code
    os.mkdir('new_data/cross_validation_code'+'/'+notebook_id)
FileNotFoundError: [WinError 3] 系统找不到指定的路径。: 'new_data/cross_validation_code/datascientist25_gender-recognition-by-voice-using-machine-learning'
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
index Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
index Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
Traceback (most recent call last):
  File ".\hai.py", line 15, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 13, in generate_hai
    run_one_validation_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1815, in run_one_validation_rl
    print('index',index)
NameError: name 'index' is not defined
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
0
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
True
mkdir  new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/0.npy", { "accuracy_score": cross_score })




[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
其他错误
item, 1.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
1
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\_pylab_helpers.py", line 86, in destroy_all
    manager.destroy()
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\backends\_backend_tk.py", line 495, in destroy
    self.window.after_idle(self.window.after, 0, delayed_destroy)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 830, in after_idle
    return self.after('idle', func, *args)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 821, in after
    name = self._register(callit)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 1528, in _register
    self.tk.createcommand(name, f)
RuntimeError: main thread is not in main loop
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/1.npy", { "accuracy_score": cross_score })




[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
其他错误
item, 2.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
2
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/2.npy", { "accuracy_score": cross_score })




[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
其他错误
item, origin.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
origin
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/origin.npy", { "accuracy_score": cross_score })




[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
其他错误
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\_pylab_helpers.py", line 86, in destroy_all
    manager.destroy()
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\backends\_backend_tk.py", line 495, in destroy
    self.window.after_idle(self.window.after, 0, delayed_destroy)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 830, in after_idle
    return self.after('idle', func, *args)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 821, in after
    name = self._register(callit)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 1528, in _register
    self.tk.createcommand(name, f)
RuntimeError: main thread is not in main loop
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
0
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/0.npy", { "accuracy_score": cross_score })




[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, 1.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
1
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/1.npy", { "accuracy_score": cross_score })




[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, 2.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
2
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/2.npy", { "accuracy_score": cross_score })




[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, origin.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
origin
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/origin.npy", { "accuracy_score": cross_score })




[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
Traceback (most recent call last):
  File ".\hai.py", line 32, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 30, in generate_hai
    run_one_max_hai(notebook_id)
NameError: name 'run_one_max_hai' is not defined
Traceback (most recent call last):
  File ".\hai.py", line 32, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 30, in generate_hai
    run_one_max_hai(notebook_id)
NameError: name 'run_one_max_hai' is not defined
Traceback (most recent call last):
  File ".\hai.py", line 32, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 30, in generate_hai
    run_one_max_hai(notebook_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1844, in run_one_max_hai
    run_path_1('rl_test_merge_code/'+notebook_id+'/'+item, replace_code = 'merge_max_result_rl/',test=False)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1253, in run_path_1
    code = load_code_1(path, test)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1216, in load_code_1
    with open(path, 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'rl_test_merge_code/datascientist25_gender-recognition-by-voice-using-machine-learning/0.json'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File ".\hai.py", line 32, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 30, in generate_hai
    run_one_max_hai(notebook_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1844, in run_one_max_hai
    run_path_1('new_data/rl_test_merge_code/'+notebook_id+'/'+item, replace_code = 'merge_max_result_rl/',test=False)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1253, in run_path_1
    code = load_code_1(path, test)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1216, in load_code_1
    with open(path, 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'new_data/rl_test_merge_code/datascientist25_gender-recognition-by-voice-using-machine-learning/origin.json'
0
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning
True
mkdir  new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
import pylab as pl

[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
[0;32;40msucceed[0m
1
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
import pylab as pl

[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
[0;32;40msucceed[0m
2
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
import pylab as pl

[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
[0;32;40msucceed[0m
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })



0
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
import pylab as pl

[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
[0;32;40msucceed[0m
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File ".\hai.py", line 32, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 30, in generate_hai
    run_one_max_hai(notebook_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1844, in run_one_max_hai
    run_path_1('new_data/rl_test_merge_code/'+notebook_id+'/'+item, replace_code = 'merge_max_result_rl/',test=False)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1253, in run_path_1
    code = load_code_1(path, test)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1216, in load_code_1
    with open(path, 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'new_data/rl_test_merge_code/datascientist25_gender-recognition-by-voice-using-machine-learning/origin.json'
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })



1
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
import pylab as pl

[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
[0;32;40msucceed[0m
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })



2
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
import pylab as pl

[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
[0;32;40msucceed[0m
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
0
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning/0.npy", { "accuracy_score": score })




[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
0
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning/0.npy", { "accuracy_score": score })




[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
Traceback (most recent call last):
  File ".\hai.py", line 45, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 43, in generate_hai
    output(notebook_id,hai_name)
  File ".\hai.py", line 23, in output
    hi_score = np.load("new_data/prenotebook_res/"+notebook_id+'.', allow_pickle=True).item()
  File "D:\Anaconda3\envs\firefly\lib\site-packages\numpy\lib\npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.'
Traceback (most recent call last):
  File ".\hai.py", line 45, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 43, in generate_hai
    output(notebook_id,hai_name)
  File ".\hai.py", line 23, in output
    hi_score = np.load("new_data/prenotebook_res/"+notebook_id+'.', allow_pickle=True).item()
  File "D:\Anaconda3\envs\firefly\lib\site-packages\numpy\lib\npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.'
Traceback (most recent call last):
  File ".\hai.py", line 45, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 43, in generate_hai
    output(notebook_id,hai_name)
  File ".\hai.py", line 23, in output
    hi_score = np.load("new_data/prenotebook_res/"+notebook_id+'.', allow_pickle=True).item()
  File "D:\Anaconda3\envs\firefly\lib\site-packages\numpy\lib\npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.'
Traceback (most recent call last):
  File ".\hai.py", line 45, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 43, in generate_hai
    output(notebook_id,hai_name)
  File ".\hai.py", line 23, in output
    hi_score = np.load("new_data/prenotebook_res/"+notebook_id+'.', allow_pickle=True).item()
  File "D:\Anaconda3\envs\firefly\lib\site-packages\numpy\lib\npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.'
notebook_id datascientist25_gender-recognition-by-voice-using-machine-learning
hi_score: {'accuracy_score': 0.9186991869918699}
hai_score: {'accuracy_score': 0.9857723577235772}
Traceback (most recent call last):
  File ".\hai.py", line 45, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 43, in generate_hai
    output(notebook_id,hai_name)
  File ".\hai.py", line 28, in output
    os.system('cp new_data/rl_test_merge_code_py/'+notebook_id +'/'+hai_index+'.py','../firefly/'+hai_name)
TypeError: system() takes at most 1 argument (2 given)
Traceback (most recent call last):
  File ".\hai.py", line 46, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 44, in generate_hai
    output(notebook_id,hai_name)
  File ".\hai.py", line 29, in output
    shutil.copyfile('new_data/rl_test_merge_code_py/'+notebook_id +'/'+hai_index+'.py', '../firefly/'+hai_name)
  File "D:\Anaconda3\envs\firefly\lib\shutil.py", line 264, in copyfile
    with open(src, 'rb') as fsrc, open(dst, 'wb') as fdst:
FileNotFoundError: [Errno 2] No such file or directory: '../firefly/hai.py'
notebook_id datascientist25_gender-recognition-by-voice-using-machine-learning
hi_score: {'accuracy_score': 0.9186991869918699}
hai_score: {'accuracy_score': 0.9857723577235772}
notebook_id datascientist25_gender-recognition-by-voice-using-machine-learning
hi_score: {'accuracy_score': 0.9186991869918699}
hai_score: {'accuracy_score': 0.9857723577235772}
notebook_id datascientist25_gender-recognition-by-voice-using-machine-learning
hi_score: {'accuracy_score': 0.9186991869918699}
hai_score: {'accuracy_score': 0.9857723577235772}
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

save_code True
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
ASSIGN******

df = pd.read_csv('../input/voicegender/voice.csv')

[1;33;44m{'pd'}[0m
    ###########
    create black edge
    black code: 
df = pd.read_csv('../input/voicegender/voice.csv')

self.get_varible_results ['df', 'pd']
    varibles ['df']
EXPR******

df.columns

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.columns

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.shape

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.shape

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.dtypes

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.dtypes

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.head(3)

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <NotebookGraph.Node object at 0x000002296600B760>
should_create_black_edge False
EXPR******

df.isnull().values.any()

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.isnull().values.any()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <NotebookGraph.Node object at 0x000002296600B910>
should_create_black_edge False
ASSIGN******

colors = ['pink', 'Lightblue']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['pink', 'Lightblue']

self.get_varible_results ['colors']
    varibles ['colors']
ASSIGN******

data_y = df[df.columns[(- 1)]]

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
data_y = df[df.columns[(- 1)]]

self.get_varible_results ['data_y', 'df', 'df']
    varibles ['df']
EXPR******

print(df['label'].value_counts())

self.should_create_white_edge True
    ###########
    create white edge
    white code: print(df['label'].value_counts())

self.get_varible_results [('print', False), ('df', True)]
should_create_black_edge True
    ###########
    create black edge
    black code: 
print(df['label'].value_counts())

self.get_varible_results ['print', 'df']
    varibles ['df']
ASSIGN******

correlation = df.corr()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df.corr()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df', 'colors', 'data_y'])
    ###########
    create assign edge
    assign target: ['correlation']
    assign code: 
correlation = df.corr()

ASSIGN******

X = df[df.columns[:(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
X = df[df.columns[:(- 1)]].values

self.get_varible_results ['X', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

y = df[df.columns[(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
y = df[df.columns[(- 1)]].values

self.get_varible_results ['y', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(X, y, test_size=0.3)

self.get_varible_results [('train_test_split', False), ('X', False), ('y', False)]
    white varibles: ['X', 'y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

EXPR******

warnings.filterwarnings('ignore')

self.should_create_white_edge True
    ###########
    create white edge
    white code: warnings.filterwarnings('ignore')

self.get_varible_results [('warnings', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
warnings.filterwarnings('ignore')

self.get_varible_results ['warnings']
    varibles []
ASSIGN******

rand_forest = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
rand_forest = RandomForestClassifier()

self.get_varible_results ['rand_forest', 'RandomForestClassifier']
    varibles ['rand_forest']
ASSIGN******

CVFirst = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
CVFirst = GaussianNB()

self.get_varible_results ['CVFirst', 'GaussianNB']
    varibles ['CVFirst']
ASSIGN******

male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

self.get_varible_results ['male_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

self.get_varible_results ['female_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

[1;33;44m{'list'}[0m
    ###########
    create black edge
    black code: 
index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

self.get_varible_results ['index_to_remove', 'list', 'male_funFreq_outlier_index', 'list', 'female_funFreq_outlier_index']
    varibles ['male_funFreq_outlier_index', 'female_funFreq_outlier_index']
var male_funFreq_outlier_index
var female_funFreq_outlier_index
EXPR******

len(index_to_remove)

self.should_create_white_edge True
    ###########
    create white edge
    white code: len(index_to_remove)

self.get_varible_results [('len', False), ('index_to_remove', False)]
    white varibles: ['index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove'])
Expr white_edge_node <NotebookGraph.Node object at 0x000002296600B970>
should_create_black_edge False
ASSIGN******

data_x = df[df.columns[0:20]].copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df[df.columns[0:20]].copy()

self.get_varible_results [('df', True), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = df[df.columns[0:20]].copy()

self.get_varible_results ['data_x', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

self.get_varible_results [('data_x', False)]
    white varibles: ['data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

EXPR******

data2.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: data2.head(3)

self.get_varible_results [('data2', False)]
    white varibles: ['data2']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
Expr white_edge_node <NotebookGraph.Node object at 0x00000229508BB220>
should_create_black_edge False
ASSIGN******

data2 = data2.drop(index_to_remove, axis=0)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data2.drop(index_to_remove, axis=0)

self.get_varible_results [('data2', False), ('index_to_remove', False)]
    white varibles: ['data2', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data2.drop(index_to_remove, axis=0)

ASSIGN******

data_y = pd.Series(y).drop(index_to_remove, axis=0)

[1;33;44m{'pd'}[0m
    ###########
    create white edge
    white code: pd.Series(y).drop(index_to_remove, axis=0)

self.get_varible_results [('pd', False), ('y', False), ('index_to_remove', False)]
    white varibles: ['y', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data_y']
    assign code: 
data_y = pd.Series(y).drop(index_to_remove, axis=0)

ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

self.get_varible_results [('train_test_split', False), ('data2', False), ('data_y', False)]
    white varibles: ['data2', 'data_y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

ASSIGN******

clf1 = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
clf1 = RandomForestClassifier()

self.get_varible_results ['clf1', 'RandomForestClassifier']
    varibles ['clf1']
ASSIGN******

clf2 = DecisionTreeClassifier()

[1;33;44m{'DecisionTreeClassifier'}[0m
    ###########
    create black edge
    black code: 
clf2 = DecisionTreeClassifier()

self.get_varible_results ['clf2', 'DecisionTreeClassifier']
    varibles ['clf2']
ASSIGN******

clf3 = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
clf3 = GaussianNB()

self.get_varible_results ['clf3', 'GaussianNB']
    varibles ['clf3']
ASSIGN******

clf4 = LogisticRegression()

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
clf4 = LogisticRegression()

self.get_varible_results ['clf4', 'LogisticRegression']
    varibles ['clf4']
ASSIGN******

labels = ['female', 'male']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = ['female', 'male']

self.get_varible_results ['labels']
    varibles ['labels']
EXPR******

pl.title('Confusion matrix of the classifier')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.title('Confusion matrix of the classifier')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.title('Confusion matrix of the classifier')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.xlabel('Predicted')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.xlabel('Predicted')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.xlabel('Predicted')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.ylabel('True')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.ylabel('True')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.ylabel('True')

self.get_varible_results ['pl']
    varibles []
EXPR******

style.use('ggplot')

self.should_create_white_edge True
    ###########
    create white edge
    white code: style.use('ggplot')

self.get_varible_results [('style', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
style.use('ggplot')

self.get_varible_results ['style']
    varibles []
ASSIGN******

data_x = np.array(df[['meanfreq', 'meanfun']])

[1;33;44m{'np'}[0m
    ###########
    create white edge
    white code: np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results [('np', False), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results ['data_x', 'np', 'df']
    varibles ['data_x', 'df']
var data_x
var df
ASSIGN******

kmeans = KMeans(n_clusters=2)

[1;33;44m{'KMeans'}[0m
    ###########
    create black edge
    black code: 
kmeans = KMeans(n_clusters=2)

self.get_varible_results ['kmeans', 'KMeans']
    varibles ['kmeans']
EXPR******

kmeans.fit(data_x)

self.should_create_white_edge True
    ###########
    create white edge
    white code: kmeans.fit(data_x)

self.get_varible_results [('kmeans', False), ('data_x', False)]
    white varibles: ['kmeans', 'data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans'])
Expr white_edge_node <NotebookGraph.Node object at 0x00000229508BB910>
should_create_black_edge False
ASSIGN******

centroids = kmeans.cluster_centers_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
centroids = kmeans.cluster_centers_

self.get_varible_results ['centroids', 'kmeans']
    varibles ['kmeans']
var kmeans
ASSIGN******

labels = kmeans.labels_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = kmeans.labels_

self.get_varible_results ['labels', 'kmeans']
    varibles ['labels', 'kmeans']
var labels
var kmeans
ASSIGN******

colors = ['g.', 'b.']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['g.', 'b.']

self.get_varible_results ['colors']
    varibles ['colors']
EXPR******

print('start running model training........')

self.should_create_white_edge True
    ###########
    create white edge
    white code: print('start running model training........')

self.get_varible_results [('print', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
print('start running model training........')

self.get_varible_results ['print']
    varibles []
ASSIGN******

model = LogisticRegression(solver='liblinear', random_state=0)

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
model = LogisticRegression(solver='liblinear', random_state=0)

self.get_varible_results ['model', 'LogisticRegression']
    varibles ['model']
EXPR******

model.fit(Xtrain, ytrain)

self.should_create_white_edge True
    ###########
    create white edge
    white code: model.fit(Xtrain, ytrain)

self.get_varible_results [('model', False), ('Xtrain', False), ('ytrain', False)]
    white varibles: ['model', 'Xtrain', 'ytrain']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
Expr white_edge_node <NotebookGraph.Node object at 0x00000229508BBD00>
should_create_black_edge False
ASSIGN******

y_pred = model.predict(Xtest)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: model.predict(Xtest)

self.get_varible_results [('model', False), ('Xtest', False)]
    white varibles: ['model', 'Xtest']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
    ###########
    create assign edge
    assign target: ['y_pred']
    assign code: 
y_pred = model.predict(Xtest)

ASSIGN******

score = accuracy_score(ytest, y_pred)

[1;33;44m{'accuracy_score'}[0m
    ###########
    create white edge
    white code: accuracy_score(ytest, y_pred)

self.get_varible_results [('accuracy_score', False), ('ytest', False), ('y_pred', False)]
    white varibles: ['ytest', 'y_pred']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred'])
    ###########
    create assign edge
    assign target: ['score']
    assign code: 
score = accuracy_score(ytest, y_pred)

EXPR******

np.save('new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.should_create_white_edge True
    ###########
    create white edge
    white code: np.save('new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.get_varible_results [('np', False), ('score', False)]
    white varibles: ['score']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred', '-RESULT-accuracy_score', 'score'])
Expr white_edge_node <NotebookGraph.Node object at 0x00000229508A60A0>
should_create_black_edge False
predict model.predict(Xtest)

predict model.predict(Xtest)

self.model_variable ['model', 'model']
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name correlation
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name X
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name male_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name female_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name index_to_remove
self.nodes[node_index].varible_name -RESULT-len
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
model
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name centroids
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
y_pred
self.nodes[node_index].varible_name y_pred
-RESULT-accuracy_score
self.nodes[node_index].varible_name -RESULT-accuracy_score
score
self.nodes[node_index].varible_name score
-RESULT-np.save
self.nodes[node_index].varible_name -RESULT-np.save
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import matplotlib.pyplot as plt
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import seaborn as sns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.columns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.shape
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.dtypes
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.isnull().values.any()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ['pink','Lightblue']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = df[df.columns[-1]]
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.axis('equal')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line print (df['label'].value_counts())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line correlation =df.corr()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.heatmap(correlation)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line X = df[df.columns[:-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y = df[df.columns[-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import warnings
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line warnings.filterwarnings("ignore")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.ensemble import RandomForestClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line rand_forest = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = rand_forest.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn import metrics, neighbors
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import confusion_matrix
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(confusion_matrix(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.naive_bayes import GaussianNB
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import cross_val_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line CVFirst = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line len(index_to_remove)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = df[df.columns[0:20]].copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
##############################
line_index 45
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy() edge data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

line_id [45]
[45]
[45]
xxxx
line data2.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf1 = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf1.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = clf1.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.tree import DecisionTreeClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf2 = DecisionTreeClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf2.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict = clf2.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf3 = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predd = clf3.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf4 = LogisticRegression()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf4.fit(Xtrain,ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict4 = clf4.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pylab as pl
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = ['female', 'male']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig = plt.figure()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax = fig.add_subplot(111)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cax =ax.matshow(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.title('Confusion matrix of the classifier')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig.colorbar(cax)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_xticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_yticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.xlabel('Predicted')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.ylabel('True')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #pl.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import classification_report
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(classification_report(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.cluster import KMeans
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from matplotlib import style
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line style.use("ggplot")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans = KMeans(n_clusters= 2)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans.fit(data_x)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line centroids = kmeans.cluster_centers_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = kmeans.labels_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ["g.","b."]  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #for i in range(len(data_x)):
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line     
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.ylabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.xlabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line print("start running model training........")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y_pred = model.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line score = accuracy_score(ytest, y_pred)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File ".\hai.py", line 47, in <module>
    generate_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File ".\hai.py", line 41, in generate_hai
    merge_one_code(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 2661, in merge_one_code
    transform_one_validation_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 2455, in transform_one_validation_rl
    seq_files = os.listdir('new_data/rl_test_merge_code/'+notebook_id)
FileNotFoundError: [WinError 3] 系统找不到指定的路径。: 'new_data/rl_test_merge_code/datascientist25_gender-recognition-by-voice-using-machine-learning'
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

save_code True
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
ASSIGN******

df = pd.read_csv('../input/voicegender/voice.csv')

[1;33;44m{'pd'}[0m
    ###########
    create black edge
    black code: 
df = pd.read_csv('../input/voicegender/voice.csv')

self.get_varible_results ['df', 'pd']
    varibles ['df']
EXPR******

df.columns

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.columns

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.shape

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.shape

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.dtypes

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.dtypes

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.head(3)

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <NotebookGraph.Node object at 0x00000226F40EB6A0>
should_create_black_edge False
EXPR******

df.isnull().values.any()

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.isnull().values.any()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <NotebookGraph.Node object at 0x00000226F40EB8E0>
should_create_black_edge False
ASSIGN******

colors = ['pink', 'Lightblue']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['pink', 'Lightblue']

self.get_varible_results ['colors']
    varibles ['colors']
ASSIGN******

data_y = df[df.columns[(- 1)]]

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
data_y = df[df.columns[(- 1)]]

self.get_varible_results ['data_y', 'df', 'df']
    varibles ['df']
EXPR******

print(df['label'].value_counts())

self.should_create_white_edge True
    ###########
    create white edge
    white code: print(df['label'].value_counts())

self.get_varible_results [('print', False), ('df', True)]
should_create_black_edge True
    ###########
    create black edge
    black code: 
print(df['label'].value_counts())

self.get_varible_results ['print', 'df']
    varibles ['df']
ASSIGN******

correlation = df.corr()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df.corr()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df', 'colors', 'data_y'])
    ###########
    create assign edge
    assign target: ['correlation']
    assign code: 
correlation = df.corr()

ASSIGN******

X = df[df.columns[:(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
X = df[df.columns[:(- 1)]].values

self.get_varible_results ['X', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

y = df[df.columns[(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
y = df[df.columns[(- 1)]].values

self.get_varible_results ['y', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(X, y, test_size=0.3)

self.get_varible_results [('train_test_split', False), ('X', False), ('y', False)]
    white varibles: ['X', 'y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

EXPR******

warnings.filterwarnings('ignore')

self.should_create_white_edge True
    ###########
    create white edge
    white code: warnings.filterwarnings('ignore')

self.get_varible_results [('warnings', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
warnings.filterwarnings('ignore')

self.get_varible_results ['warnings']
    varibles []
ASSIGN******

rand_forest = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
rand_forest = RandomForestClassifier()

self.get_varible_results ['rand_forest', 'RandomForestClassifier']
    varibles ['rand_forest']
ASSIGN******

CVFirst = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
CVFirst = GaussianNB()

self.get_varible_results ['CVFirst', 'GaussianNB']
    varibles ['CVFirst']
ASSIGN******

male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

self.get_varible_results ['male_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

self.get_varible_results ['female_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

[1;33;44m{'list'}[0m
    ###########
    create black edge
    black code: 
index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

self.get_varible_results ['index_to_remove', 'list', 'male_funFreq_outlier_index', 'list', 'female_funFreq_outlier_index']
    varibles ['male_funFreq_outlier_index', 'female_funFreq_outlier_index']
var male_funFreq_outlier_index
var female_funFreq_outlier_index
EXPR******

len(index_to_remove)

self.should_create_white_edge True
    ###########
    create white edge
    white code: len(index_to_remove)

self.get_varible_results [('len', False), ('index_to_remove', False)]
    white varibles: ['index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove'])
Expr white_edge_node <NotebookGraph.Node object at 0x00000226F40EB940>
should_create_black_edge False
ASSIGN******

data_x = df[df.columns[0:20]].copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df[df.columns[0:20]].copy()

self.get_varible_results [('df', True), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = df[df.columns[0:20]].copy()

self.get_varible_results ['data_x', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

self.get_varible_results [('data_x', False)]
    white varibles: ['data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

EXPR******

data2.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: data2.head(3)

self.get_varible_results [('data2', False)]
    white varibles: ['data2']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
Expr white_edge_node <NotebookGraph.Node object at 0x00000226DE8D41F0>
should_create_black_edge False
ASSIGN******

data2 = data2.drop(index_to_remove, axis=0)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data2.drop(index_to_remove, axis=0)

self.get_varible_results [('data2', False), ('index_to_remove', False)]
    white varibles: ['data2', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data2.drop(index_to_remove, axis=0)

ASSIGN******

data_y = pd.Series(y).drop(index_to_remove, axis=0)

[1;33;44m{'pd'}[0m
    ###########
    create white edge
    white code: pd.Series(y).drop(index_to_remove, axis=0)

self.get_varible_results [('pd', False), ('y', False), ('index_to_remove', False)]
    white varibles: ['y', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data_y']
    assign code: 
data_y = pd.Series(y).drop(index_to_remove, axis=0)

ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

self.get_varible_results [('train_test_split', False), ('data2', False), ('data_y', False)]
    white varibles: ['data2', 'data_y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

ASSIGN******

clf1 = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
clf1 = RandomForestClassifier()

self.get_varible_results ['clf1', 'RandomForestClassifier']
    varibles ['clf1']
ASSIGN******

clf2 = DecisionTreeClassifier()

[1;33;44m{'DecisionTreeClassifier'}[0m
    ###########
    create black edge
    black code: 
clf2 = DecisionTreeClassifier()

self.get_varible_results ['clf2', 'DecisionTreeClassifier']
    varibles ['clf2']
ASSIGN******

clf3 = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
clf3 = GaussianNB()

self.get_varible_results ['clf3', 'GaussianNB']
    varibles ['clf3']
ASSIGN******

clf4 = LogisticRegression()

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
clf4 = LogisticRegression()

self.get_varible_results ['clf4', 'LogisticRegression']
    varibles ['clf4']
ASSIGN******

labels = ['female', 'male']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = ['female', 'male']

self.get_varible_results ['labels']
    varibles ['labels']
EXPR******

pl.title('Confusion matrix of the classifier')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.title('Confusion matrix of the classifier')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.title('Confusion matrix of the classifier')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.xlabel('Predicted')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.xlabel('Predicted')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.xlabel('Predicted')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.ylabel('True')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.ylabel('True')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.ylabel('True')

self.get_varible_results ['pl']
    varibles []
EXPR******

style.use('ggplot')

self.should_create_white_edge True
    ###########
    create white edge
    white code: style.use('ggplot')

self.get_varible_results [('style', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
style.use('ggplot')

self.get_varible_results ['style']
    varibles []
ASSIGN******

data_x = np.array(df[['meanfreq', 'meanfun']])

[1;33;44m{'np'}[0m
    ###########
    create white edge
    white code: np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results [('np', False), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results ['data_x', 'np', 'df']
    varibles ['data_x', 'df']
var data_x
var df
ASSIGN******

kmeans = KMeans(n_clusters=2)

[1;33;44m{'KMeans'}[0m
    ###########
    create black edge
    black code: 
kmeans = KMeans(n_clusters=2)

self.get_varible_results ['kmeans', 'KMeans']
    varibles ['kmeans']
EXPR******

kmeans.fit(data_x)

self.should_create_white_edge True
    ###########
    create white edge
    white code: kmeans.fit(data_x)

self.get_varible_results [('kmeans', False), ('data_x', False)]
    white varibles: ['kmeans', 'data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans'])
Expr white_edge_node <NotebookGraph.Node object at 0x00000226DE8D48E0>
should_create_black_edge False
ASSIGN******

centroids = kmeans.cluster_centers_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
centroids = kmeans.cluster_centers_

self.get_varible_results ['centroids', 'kmeans']
    varibles ['kmeans']
var kmeans
ASSIGN******

labels = kmeans.labels_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = kmeans.labels_

self.get_varible_results ['labels', 'kmeans']
    varibles ['labels', 'kmeans']
var labels
var kmeans
ASSIGN******

colors = ['g.', 'b.']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['g.', 'b.']

self.get_varible_results ['colors']
    varibles ['colors']
EXPR******

print('start running model training........')

self.should_create_white_edge True
    ###########
    create white edge
    white code: print('start running model training........')

self.get_varible_results [('print', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
print('start running model training........')

self.get_varible_results ['print']
    varibles []
ASSIGN******

model = LogisticRegression(solver='liblinear', random_state=0)

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
model = LogisticRegression(solver='liblinear', random_state=0)

self.get_varible_results ['model', 'LogisticRegression']
    varibles ['model']
EXPR******

model.fit(Xtrain, ytrain)

self.should_create_white_edge True
    ###########
    create white edge
    white code: model.fit(Xtrain, ytrain)

self.get_varible_results [('model', False), ('Xtrain', False), ('ytrain', False)]
    white varibles: ['model', 'Xtrain', 'ytrain']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
Expr white_edge_node <NotebookGraph.Node object at 0x00000226DE8D4CD0>
should_create_black_edge False
ASSIGN******

y_pred = model.predict(Xtest)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: model.predict(Xtest)

self.get_varible_results [('model', False), ('Xtest', False)]
    white varibles: ['model', 'Xtest']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
    ###########
    create assign edge
    assign target: ['y_pred']
    assign code: 
y_pred = model.predict(Xtest)

ASSIGN******

score = accuracy_score(ytest, y_pred)

[1;33;44m{'accuracy_score'}[0m
    ###########
    create white edge
    white code: accuracy_score(ytest, y_pred)

self.get_varible_results [('accuracy_score', False), ('ytest', False), ('y_pred', False)]
    white varibles: ['ytest', 'y_pred']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred'])
    ###########
    create assign edge
    assign target: ['score']
    assign code: 
score = accuracy_score(ytest, y_pred)

EXPR******

np.save('new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.should_create_white_edge True
    ###########
    create white edge
    white code: np.save('new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.get_varible_results [('np', False), ('score', False)]
    white varibles: ['score']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred', '-RESULT-accuracy_score', 'score'])
Expr white_edge_node <NotebookGraph.Node object at 0x00000226DE8C6070>
should_create_black_edge False
predict model.predict(Xtest)

predict model.predict(Xtest)

self.model_variable ['model', 'model']
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name correlation
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name X
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name male_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name female_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name index_to_remove
self.nodes[node_index].varible_name -RESULT-len
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
model
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name centroids
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
y_pred
self.nodes[node_index].varible_name y_pred
-RESULT-accuracy_score
self.nodes[node_index].varible_name -RESULT-accuracy_score
score
self.nodes[node_index].varible_name score
-RESULT-np.save
self.nodes[node_index].varible_name -RESULT-np.save
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import matplotlib.pyplot as plt
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import seaborn as sns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.columns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.shape
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.dtypes
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.isnull().values.any()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ['pink','Lightblue']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = df[df.columns[-1]]
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.axis('equal')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line print (df['label'].value_counts())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line correlation =df.corr()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.heatmap(correlation)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line X = df[df.columns[:-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y = df[df.columns[-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import warnings
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line warnings.filterwarnings("ignore")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.ensemble import RandomForestClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line rand_forest = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = rand_forest.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn import metrics, neighbors
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import confusion_matrix
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(confusion_matrix(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.naive_bayes import GaussianNB
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import cross_val_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line CVFirst = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line len(index_to_remove)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = df[df.columns[0:20]].copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
##############################
line_index 45
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy() edge data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

line_id [45]
[45]
[45]
xxxx
line data2.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf1 = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf1.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = clf1.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.tree import DecisionTreeClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf2 = DecisionTreeClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf2.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict = clf2.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf3 = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predd = clf3.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf4 = LogisticRegression()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf4.fit(Xtrain,ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict4 = clf4.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pylab as pl
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = ['female', 'male']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig = plt.figure()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax = fig.add_subplot(111)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cax =ax.matshow(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.title('Confusion matrix of the classifier')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig.colorbar(cax)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_xticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_yticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.xlabel('Predicted')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.ylabel('True')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #pl.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import classification_report
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(classification_report(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.cluster import KMeans
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from matplotlib import style
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line style.use("ggplot")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans = KMeans(n_clusters= 2)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans.fit(data_x)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line centroids = kmeans.cluster_centers_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = kmeans.labels_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ["g.","b."]  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #for i in range(len(data_x)):
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line     
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.ylabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.xlabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line print("start running model training........")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y_pred = model.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line score = accuracy_score(ytest, y_pred)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
[[<merge.Step object at 0x00000226DEC9BF40>],
 [<merge.Step object at 0x00000226DEC9BE20>],
 [<merge.Step object at 0x00000226DEC9BFD0>],
 [<merge.Step object at 0x00000226F0345070>],
 [<merge.Step object at 0x00000226F03450D0>],
 [<merge.Step object at 0x00000226DEC9BF70>],
 [<merge.Step object at 0x00000226F0345160>],
 [<merge.Step object at 0x00000226F03451C0>],
 [<merge.Step object at 0x00000226F0345220>],
 [<merge.Step object at 0x00000226DEC9BF40>,
  <merge.Step object at 0x00000226F0345220>],
 [<merge.Step object at 0x00000226DEC9BFD0>,
  <merge.Step object at 0x00000226F0345220>],
 [<merge.Step object at 0x00000226F03450D0>,
  <merge.Step object at 0x00000226F0345220>],
 [<merge.Step object at 0x00000226F0345280>],
 [<merge.Step object at 0x00000226DEC9BF40>,
  <merge.Step object at 0x00000226F0345280>],
 [<merge.Step object at 0x00000226DEC9BE20>,
  <merge.Step object at 0x00000226F0345280>],
 [<merge.Step object at 0x00000226DEC9BFD0>,
  <merge.Step object at 0x00000226F0345280>],
 [<merge.Step object at 0x00000226F0345070>,
  <merge.Step object at 0x00000226F0345280>],
 [<merge.Step object at 0x00000226F03450D0>,
  <merge.Step object at 0x00000226F0345280>],
 [<merge.Step object at 0x00000226DEC9BF70>,
  <merge.Step object at 0x00000226F0345280>],
 [<merge.Step object at 0x00000226F0345370>],
 [<merge.Step object at 0x00000226DEC9BFD0>,
  <merge.Step object at 0x00000226F0345370>],
 [<merge.Step object at 0x00000226F03450D0>,
  <merge.Step object at 0x00000226F0345370>],
 [<merge.Step object at 0x00000226F03454F0>],
 [<merge.Step object at 0x00000226DEC9BFD0>,
  <merge.Step object at 0x00000226F03454F0>],
 [<merge.Step object at 0x00000226F0345070>,
  <merge.Step object at 0x00000226F03454F0>],
 [<merge.Step object at 0x00000226F03450D0>,
  <merge.Step object at 0x00000226F03454F0>],
 [<merge.Step object at 0x00000226DEC9BF70>,
  <merge.Step object at 0x00000226F03454F0>],
 [<merge.Step object at 0x00000226F03455B0>],
 [<merge.Step object at 0x00000226F03450D0>,
  <merge.Step object at 0x00000226F03455B0>],
 [<merge.Step object at 0x00000226F03456D0>],
 [<merge.Step object at 0x00000226F03450D0>,
  <merge.Step object at 0x00000226F03456D0>],
 [<merge.Step object at 0x00000226DEC9BF70>,
  <merge.Step object at 0x00000226F03456D0>],
 [<merge.Step object at 0x00000226F0345760>],
 [<merge.Step object at 0x00000226DEC9BF40>,
  <merge.Step object at 0x00000226F0345760>],
 [<merge.Step object at 0x00000226DEC9BFD0>,
  <merge.Step object at 0x00000226F0345760>],
 [<merge.Step object at 0x00000226F03450D0>,
  <merge.Step object at 0x00000226F0345760>],
 [<merge.Step object at 0x00000226F0345160>,
  <merge.Step object at 0x00000226F0345760>],
 [<merge.Step object at 0x00000226F0345820>],
 [<merge.Step object at 0x00000226DEC9BF40>,
  <merge.Step object at 0x00000226F0345820>],
 [<merge.Step object at 0x00000226DEC9BE20>,
  <merge.Step object at 0x00000226F0345820>],
 [<merge.Step object at 0x00000226DEC9BFD0>,
  <merge.Step object at 0x00000226F0345820>],
 [<merge.Step object at 0x00000226F0345070>,
  <merge.Step object at 0x00000226F0345820>],
 [<merge.Step object at 0x00000226F03450D0>,
  <merge.Step object at 0x00000226F0345820>],
 [<merge.Step object at 0x00000226DEC9BF70>,
  <merge.Step object at 0x00000226F0345820>],
 [<merge.Step object at 0x00000226F0345160>,
  <merge.Step object at 0x00000226F0345820>],
 [<merge.Step object at 0x00000226F03451C0>,
  <merge.Step object at 0x00000226F0345820>],
 [<merge.Step object at 0x00000226DEC9BEE0>],
 [<merge.Step object at 0x00000226F0345AF0>,
  <merge.Step object at 0x00000226F0345B50>],
 [<merge.Step object at 0x00000226F0345BB0>]]
('InteractionFeatures', 'end'),
('InteractionFeatures', 'end'),('VarianceThreshold', 'end'),
('VarianceThreshold', 'end'),
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
index Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
0
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
True
mkdir  new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/0.npy", { "accuracy_score": cross_score })




[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, 1.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
1
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/1.npy", { "accuracy_score": cross_score })




[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 660, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
2.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
2
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/2.npy", { "accuracy_score": cross_score })




[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, origin.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
origin
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/origin.npy", { "accuracy_score": cross_score })




[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
0
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning
True
mkdir  new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning/0.npy", { "accuracy_score": score })




[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
notebook_id datascientist25_gender-recognition-by-voice-using-machine-learning
hi_score: {'accuracy_score': 0.9186991869918699}
hai_score: {'accuracy_score': 0.9857723577235772}
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
True
mkdir  HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
其他错误
item, 1.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
其他错误
item, 2.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
其他错误
item, origin.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
其他错误
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




其他错误
item, 1.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




其他错误
item, 2.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




其他错误
item, origin.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




其他错误
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




其他错误
item, 1.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




其他错误
item, 2.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




其他错误
item, origin.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




其他错误
[[<HAIPipeGen.merge.Step object at 0x000002990DE19190>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C2520>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C24C0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C2490>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C25B0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C25E0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C26A0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C2790>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C2850>],
 [<HAIPipeGen.merge.Step object at 0x000002990DE19190>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2850>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C24C0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2850>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C25B0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2850>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C28B0>],
 [<HAIPipeGen.merge.Step object at 0x000002990DE19190>,
  <HAIPipeGen.merge.Step object at 0x00000299200C28B0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C2520>,
  <HAIPipeGen.merge.Step object at 0x00000299200C28B0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C24C0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C28B0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C2490>,
  <HAIPipeGen.merge.Step object at 0x00000299200C28B0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C25B0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C28B0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C25E0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C28B0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C2940>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C24C0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2940>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C25B0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2940>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C2B20>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C24C0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2B20>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C2490>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2B20>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C25B0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2B20>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C25E0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2B20>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C2BE0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C25B0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2BE0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C2CD0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C25B0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2CD0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C25E0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2CD0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C2D00>],
 [<HAIPipeGen.merge.Step object at 0x000002990DE19190>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2D00>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C24C0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2D00>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C25B0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2D00>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C26A0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2D00>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C2DC0>],
 [<HAIPipeGen.merge.Step object at 0x000002990DE19190>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2DC0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C2520>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2DC0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C24C0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2DC0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C2490>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2DC0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C25B0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2DC0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C25E0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2DC0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C26A0>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2DC0>],
 [<HAIPipeGen.merge.Step object at 0x00000299200C2790>,
  <HAIPipeGen.merge.Step object at 0x00000299200C2DC0>],
 [<HAIPipeGen.merge.Step object at 0x000002990DE19100>],
 [<HAIPipeGen.merge.Step object at 0x000002991FCE8BE0>,
  <HAIPipeGen.merge.Step object at 0x000002991FCE8EE0>],
 [<HAIPipeGen.merge.Step object at 0x000002991FD0F430>]]
('InteractionFeatures', 'end'),
('InteractionFeatures', 'end'),('VarianceThreshold', 'end'),
('VarianceThreshold', 'end'),
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
index Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




其他错误
item, 1.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




其他错误
item, 2.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




其他错误
item, origin.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




其他错误
[[<HAIPipeGen.merge.Step object at 0x000001DA5D05AF10>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D24C0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2460>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2430>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2550>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2580>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D26D0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D27C0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2880>],
 [<HAIPipeGen.merge.Step object at 0x000001DA5D05AF10>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2880>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2460>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2880>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2550>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2880>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D28E0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA5D05AF10>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D28E0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D24C0>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D28E0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2460>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D28E0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2430>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D28E0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2550>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D28E0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2580>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D28E0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2970>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2460>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2970>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2550>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2970>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2AF0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2460>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2AF0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2430>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2AF0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2550>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2AF0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2580>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2AF0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2BB0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2550>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2BB0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2CD0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2550>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2CD0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2580>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2CD0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2D00>],
 [<HAIPipeGen.merge.Step object at 0x000001DA5D05AF10>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2D00>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2460>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2D00>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2550>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2D00>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D26D0>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2D00>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2DC0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA5D05AF10>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2DC0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D24C0>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2DC0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2460>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2DC0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2430>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2DC0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2550>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2DC0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D2580>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2DC0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D26D0>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2DC0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F4D27C0>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F4D2DC0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA5D05A8E0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F077BE0>,
  <HAIPipeGen.merge.Step object at 0x000001DA6F077EE0>],
 [<HAIPipeGen.merge.Step object at 0x000001DA6F09D430>]]
('InteractionFeatures', 'end'),
('InteractionFeatures', 'end'),('VarianceThreshold', 'end'),
('VarianceThreshold', 'end'),
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
index Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
其他错误
item, 1.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
其他错误
item, 2.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
其他错误
item, origin.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
其他错误
[[<HAIPipeGen.merge.Step object at 0x00000227E7188EE0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A1C0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A250>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A280>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A490>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A370>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A3A0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A4F0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A580>],
 [<HAIPipeGen.merge.Step object at 0x00000227E7188EE0>,
  <HAIPipeGen.merge.Step object at 0x00000227F924A580>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A250>,
  <HAIPipeGen.merge.Step object at 0x00000227F924A580>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A490>,
  <HAIPipeGen.merge.Step object at 0x00000227F924A580>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A610>],
 [<HAIPipeGen.merge.Step object at 0x00000227E7188EE0>,
  <HAIPipeGen.merge.Step object at 0x00000227F924A610>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A1C0>,
  <HAIPipeGen.merge.Step object at 0x00000227F924A610>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A250>,
  <HAIPipeGen.merge.Step object at 0x00000227F924A610>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A280>,
  <HAIPipeGen.merge.Step object at 0x00000227F924A610>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A490>,
  <HAIPipeGen.merge.Step object at 0x00000227F924A610>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A370>,
  <HAIPipeGen.merge.Step object at 0x00000227F924A610>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A760>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A250>,
  <HAIPipeGen.merge.Step object at 0x00000227F924A760>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A490>,
  <HAIPipeGen.merge.Step object at 0x00000227F924A760>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A8E0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A250>,
  <HAIPipeGen.merge.Step object at 0x00000227F924A8E0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A280>,
  <HAIPipeGen.merge.Step object at 0x00000227F924A8E0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A490>,
  <HAIPipeGen.merge.Step object at 0x00000227F924A8E0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A370>,
  <HAIPipeGen.merge.Step object at 0x00000227F924A8E0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A910>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A490>,
  <HAIPipeGen.merge.Step object at 0x00000227F924A910>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924AA30>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A490>,
  <HAIPipeGen.merge.Step object at 0x00000227F924AA30>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A370>,
  <HAIPipeGen.merge.Step object at 0x00000227F924AA30>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924AB20>],
 [<HAIPipeGen.merge.Step object at 0x00000227E7188EE0>,
  <HAIPipeGen.merge.Step object at 0x00000227F924AB20>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A250>,
  <HAIPipeGen.merge.Step object at 0x00000227F924AB20>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A490>,
  <HAIPipeGen.merge.Step object at 0x00000227F924AB20>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A3A0>,
  <HAIPipeGen.merge.Step object at 0x00000227F924AB20>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924ABE0>],
 [<HAIPipeGen.merge.Step object at 0x00000227E7188EE0>,
  <HAIPipeGen.merge.Step object at 0x00000227F924ABE0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A1C0>,
  <HAIPipeGen.merge.Step object at 0x00000227F924ABE0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A250>,
  <HAIPipeGen.merge.Step object at 0x00000227F924ABE0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A280>,
  <HAIPipeGen.merge.Step object at 0x00000227F924ABE0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A490>,
  <HAIPipeGen.merge.Step object at 0x00000227F924ABE0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A370>,
  <HAIPipeGen.merge.Step object at 0x00000227F924ABE0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A3A0>,
  <HAIPipeGen.merge.Step object at 0x00000227F924ABE0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924A4F0>,
  <HAIPipeGen.merge.Step object at 0x00000227F924ABE0>],
 [<HAIPipeGen.merge.Step object at 0x00000227E71AA0A0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924AE50>,
  <HAIPipeGen.merge.Step object at 0x00000227F924AEB0>],
 [<HAIPipeGen.merge.Step object at 0x00000227F924AF10>]]
('InteractionFeatures', 'end'),
('InteractionFeatures', 'end'),('VarianceThreshold', 'end'),
('VarianceThreshold', 'end'),
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
index Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




其他错误
item, 1.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




其他错误
item, 2.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




其他错误
item, origin.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




其他错误
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




Traceback (most recent call last):
  File ".\example.py", line 4, in <module>
    generate_one_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 42, in generate_one_hai
    run_one_validation_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1818, in run_one_validation_rl
    run_path('HAIPipeGen/new_data/rl_cross_validation_code/'+notebook_id+'/'+item, replace_code = 'rl_cross_val_res/',test=False)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1098, in run_path
    code = pro.run_one_code(notebook_id, new_c, dataset_root_path + pro.info_triple[notebook_id]['dataset_name'], 0)
KeyError: 'rl_cross_validation_code'
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




Traceback (most recent call last):
  File ".\example.py", line 4, in <module>
    generate_one_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 42, in generate_one_hai
    run_one_validation_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1820, in run_one_validation_rl
    run_path('HAIPipeGen/new_data/rl_cross_validation_code/'+notebook_id+'/'+item, replace_code = 'rl_cross_val_res/',test=False)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1100, in run_path
    code = pro.run_one_code(notebook_id, new_c, dataset_root_path + pro.info_triple[notebook_id]['dataset_name'], 0)
KeyError: 'rl_cross_validation_code'
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




--------------------4-------------------------
Traceback (most recent call last):
  File ".\example.py", line 4, in <module>
    generate_one_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 42, in generate_one_hai
    run_one_validation_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1820, in run_one_validation_rl
    run_path('HAIPipeGen/new_data/rl_cross_validation_code/'+notebook_id+'/'+item, replace_code = 'rl_cross_val_res/',test=False)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1100, in run_path
    code = pro.run_one_code(notebook_id, new_c, dataset_root_path + pro.info_triple[notebook_id]['dataset_name'], 0)
KeyError: 'rl_cross_validation_code'
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




--------------------4-------------------------
Traceback (most recent call last):
  File ".\example.py", line 4, in <module>
    generate_one_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 42, in generate_one_hai
    run_one_validation_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1820, in run_one_validation_rl
    run_path('HAIPipeGen/new_data/rl_cross_validation_code/'+notebook_id+'/'+item, replace_code = 'rl_cross_val_res/',test=False)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1100, in run_path
    code = HAIPipeGen.pro.run_one_code(notebook_id, new_c, dataset_root_path + pro.info_triple[notebook_id]['dataset_name'], 0)
NameError: name 'HAIPipeGen' is not defined
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




--------------------4-------------------------
Traceback (most recent call last):
  File ".\example.py", line 4, in <module>
    generate_one_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 42, in generate_one_hai
    run_one_validation_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1821, in run_one_validation_rl
    run_path('HAIPipeGen/new_data/rl_cross_validation_code/'+notebook_id+'/'+item, replace_code = 'rl_cross_val_res/',test=False)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1100, in run_path
    print(dataset_root_path + pro.info_triple[notebook_id]['dataset_name'])
KeyError: 'rl_cross_validation_code'
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




--------------------4-------------------------
data/dataset/
Traceback (most recent call last):
  File ".\example.py", line 4, in <module>
    generate_one_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 42, in generate_one_hai
    run_one_validation_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1823, in run_one_validation_rl
    run_path('HAIPipeGen/new_data/rl_cross_validation_code/'+notebook_id+'/'+item, replace_code = 'rl_cross_val_res/',test=False)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1101, in run_path
    print( pro.info_triple[notebook_id]['dataset_name'])
KeyError: 'rl_cross_validation_code'
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




--------------------4-------------------------
data/dataset/
Traceback (most recent call last):
  File ".\example.py", line 4, in <module>
    generate_one_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 42, in generate_one_hai
    run_one_validation_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1824, in run_one_validation_rl
    run_path('HAIPipeGen/new_data/rl_cross_validation_code/'+notebook_id+'/'+item, replace_code = 'rl_cross_val_res/',test=False)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1101, in run_path
    print( pro.info_triple[notebook_id]['dataset_name'])
KeyError: 'rl_cross_validation_code'
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
Traceback (most recent call last):
  File ".\example.py", line 4, in <module>
    generate_one_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 42, in generate_one_hai
    run_one_validation_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1821, in run_one_validation_rl
    run_path('HAIPipeGen/new_data/rl_cross_validation_code/'+notebook_id+'/'+item, replace_code = 'rl_cross_val_res/',test=False)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1101, in run_path
    code = pro.run_one_code(notebook_id, new_c, dataset_root_path + info_triple[notebook_id]['dataset_name'], 0)
KeyError: 'rl_cross_validation_code'
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




--------------------4-------------------------
Traceback (most recent call last):
  File ".\example.py", line 4, in <module>
    generate_one_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 42, in generate_one_hai
    run_one_validation_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1822, in run_one_validation_rl
    run_path('HAIPipeGen/new_data/rl_cross_validation_code/'+notebook_id+'/'+item, replace_code = 'rl_cross_val_res/',test=False)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1100, in run_path
    print(info_triple[notebook_id]['dataset_name'])
UnboundLocalError: local variable 'info_triple' referenced before assignment
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




--------------------4-------------------------
Traceback (most recent call last):
  File ".\example.py", line 4, in <module>
    generate_one_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 42, in generate_one_hai
    run_one_validation_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1823, in run_one_validation_rl
    run_path('HAIPipeGen/new_data/rl_cross_validation_code/'+notebook_id+'/'+item, replace_code = 'rl_cross_val_res/',test=False)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1102, in run_path
    print(info_triple[notebook_id]['dataset_name'])
KeyError: 'rl_cross_validation_code'
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
datascientist25_gender-recognition-by-voice-using-machine-learning
rl_cross_validation_code
??????
HAIPipeGen/new_data/rl_cross_val_res/rl_cross_validation_code
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": cross_score })




--------------------4-------------------------
rl_cross_validation_code
Traceback (most recent call last):
  File ".\example.py", line 4, in <module>
    generate_one_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 42, in generate_one_hai
    run_one_validation_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1823, in run_one_validation_rl
    run_path('HAIPipeGen/new_data/rl_cross_validation_code/'+notebook_id+'/'+item, replace_code = 'rl_cross_val_res/',test=False)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\func_timeout\py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1102, in run_path
    print(info_triple[notebook_id]['dataset_name'])
KeyError: 'rl_cross_validation_code'
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
0
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
True
mkdir  HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/0.npy", { "accuracy_score": cross_score })




--------------------4-------------------------
datascientist25_gender-recognition-by-voice-using-machine-learning
primaryobjects_voicegender
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;31;40merror_str[0m [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/0.npy'
path error
[0;33;40m try times:2[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, 1.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
1
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
import numpy as np
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_engine = PolynomialFeatures(interaction_only=True, include_bias=False)
add_engine.fit(data2)
train_data_x = add_engine.transform(data2)
train_data_x = pd.DataFrame(train_data_x)
data2 = train_data_x.loc[:, ~train_data_x.columns.duplicated()]
        
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/1.npy", { "accuracy_score": cross_score })




--------------------4-------------------------
datascientist25_gender-recognition-by-voice-using-machine-learning
primaryobjects_voicegender
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;31;40merror_str[0m [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/1.npy'
path error
[0;33;40m try times:2[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, 2.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
2
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 82, in <module>
  File "<__array_function__ internals>", line 5, in save
  File "D:\Anaconda3\envs\firefly\lib\site-packages\numpy\lib\npyio.py", line 525, in save
    file_ctx = open(file, "wb")
FileNotFoundError: [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/0.npy'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 95, in <module>
  File "<__array_function__ internals>", line 5, in save
  File "D:\Anaconda3\envs\firefly\lib\site-packages\numpy\lib\npyio.py", line 525, in save
    file_ctx = open(file, "wb")
FileNotFoundError: [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/1.npy'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 84, in <module>
  File "<__array_function__ internals>", line 5, in save
  File "D:\Anaconda3\envs\firefly\lib\site-packages\numpy\lib\npyio.py", line 525, in save
    file_ctx = open(file, "wb")
FileNotFoundError: [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/2.npy'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 70, in <module>
  File "<__array_function__ internals>", line 5, in save
  File "D:\Anaconda3\envs\firefly\lib\site-packages\numpy\lib\npyio.py", line 525, in save
    file_ctx = open(file, "wb")
FileNotFoundError: [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/origin.npy'
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\_pylab_helpers.py", line 86, in destroy_all
    manager.destroy()
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\backends\_backend_tk.py", line 495, in destroy
    self.window.after_idle(self.window.after, 0, delayed_destroy)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 830, in after_idle
    return self.after('idle', func, *args)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 821, in after
    name = self._register(callit)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 1528, in _register
    self.tk.createcommand(name, f)
RuntimeError: main thread is not in main loop
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np
from itertools import compress
        
data2 = pd.DataFrame(data2).reset_index(drop=True).infer_objects()
add_seletcion = VarianceThreshold()
add_seletcion.fit(data2)
cols = list(data2.columns)
mask = add_seletcion.get_support(indices=False)
final_cols = list(compress(cols, mask))
data2 = pd.DataFrame(add_seletcion.transform(data2), columns=final_cols)
        
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/2.npy", { "accuracy_score": cross_score })




--------------------4-------------------------
datascientist25_gender-recognition-by-voice-using-machine-learning
primaryobjects_voicegender
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;31;40merror_str[0m [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/2.npy'
path error
[0;33;40m try times:2[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, origin.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
origin
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
print (df['label'].value_counts())
correlation =df.corr()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
clf3 = GaussianNB()
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression(solver='liblinear')
import pylab as pl
labels = ['female', 'male']
pl.title('Confusion matrix of the classifier')
pl.xlabel('Predicted')
pl.ylabel('True')
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
    
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
from sklearn.model_selection import cross_val_score
cross_score = cross_val_score(model, Xtrain, ytrain,cv=4)
import numpy as np
np.save("HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/origin.npy", { "accuracy_score": cross_score })




--------------------4-------------------------
datascientist25_gender-recognition-by-voice-using-machine-learning
primaryobjects_voicegender
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;31;40merror_str[0m [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/origin.npy'
path error
[0;33;40m try times:2[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 82, in <module>
  File "<__array_function__ internals>", line 5, in save
  File "D:\Anaconda3\envs\firefly\lib\site-packages\numpy\lib\npyio.py", line 525, in save
    file_ctx = open(file, "wb")
FileNotFoundError: [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/0.npy'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 95, in <module>
  File "<__array_function__ internals>", line 5, in save
  File "D:\Anaconda3\envs\firefly\lib\site-packages\numpy\lib\npyio.py", line 525, in save
    file_ctx = open(file, "wb")
FileNotFoundError: [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/1.npy'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 84, in <module>
  File "<__array_function__ internals>", line 5, in save
  File "D:\Anaconda3\envs\firefly\lib\site-packages\numpy\lib\npyio.py", line 525, in save
    file_ctx = open(file, "wb")
FileNotFoundError: [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/2.npy'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 70, in <module>
  File "<__array_function__ internals>", line 5, in save
  File "D:\Anaconda3\envs\firefly\lib\site-packages\numpy\lib\npyio.py", line 525, in save
    file_ctx = open(file, "wb")
FileNotFoundError: [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/origin.npy'
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\_pylab_helpers.py", line 86, in destroy_all
    manager.destroy()
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\backends\_backend_tk.py", line 495, in destroy
    self.window.after_idle(self.window.after, 0, delayed_destroy)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 830, in after_idle
    return self.after('idle', func, *args)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 821, in after
    name = self._register(callit)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 1528, in _register
    self.tk.createcommand(name, f)
RuntimeError: main thread is not in main loop
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
0
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;31;40merror_str[0m [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/0.npy'
path error
[0;33;40m try times:2[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, 1.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
1
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;31;40merror_str[0m [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/1.npy'
path error
[0;33;40m try times:2[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, 2.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
2
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;31;40merror_str[0m [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/2.npy'
path error
[0;33;40m try times:2[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, origin.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
origin
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;31;40merror_str[0m [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/origin.npy'
path error
[0;33;40m try times:2[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 70, in <module>
  File "<__array_function__ internals>", line 5, in save
  File "D:\Anaconda3\envs\firefly\lib\site-packages\numpy\lib\npyio.py", line 525, in save
    file_ctx = open(file, "wb")
FileNotFoundError: [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/origin.npy'
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\_pylab_helpers.py", line 86, in destroy_all
    manager.destroy()
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\backends\_backend_tk.py", line 495, in destroy
    self.window.after_idle(self.window.after, 0, delayed_destroy)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 830, in after_idle
    return self.after('idle', func, *args)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 821, in after
    name = self._register(callit)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 1528, in _register
    self.tk.createcommand(name, f)
RuntimeError: main thread is not in main loop
[[<HAIPipeGen.merge.Step object at 0x00000242E0B49190>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C824C0>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82460>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82100>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82550>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82580>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82670>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82760>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82820>],
 [<HAIPipeGen.merge.Step object at 0x00000242E0B49190>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82820>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82460>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82820>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82550>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82820>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82880>],
 [<HAIPipeGen.merge.Step object at 0x00000242E0B49190>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82880>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C824C0>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82880>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82460>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82880>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82100>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82880>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82550>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82880>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82580>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82880>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82910>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82460>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82910>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82550>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82910>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82AC0>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82460>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82AC0>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82100>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82AC0>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82550>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82AC0>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82580>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82AC0>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82B80>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82550>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82B80>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82CA0>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82550>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82CA0>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82580>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82CA0>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82CD0>],
 [<HAIPipeGen.merge.Step object at 0x00000242E0B49190>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82CD0>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82460>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82CD0>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82550>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82CD0>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82670>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82CD0>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82D90>],
 [<HAIPipeGen.merge.Step object at 0x00000242E0B49190>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82D90>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C824C0>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82D90>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82460>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82D90>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82100>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82D90>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82550>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82D90>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82580>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82D90>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82670>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82D90>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2C82760>,
  <HAIPipeGen.merge.Step object at 0x00000242F2C82D90>],
 [<HAIPipeGen.merge.Step object at 0x00000242E0B49100>],
 [<HAIPipeGen.merge.Step object at 0x00000242F29E7BE0>,
  <HAIPipeGen.merge.Step object at 0x00000242F29E7EE0>],
 [<HAIPipeGen.merge.Step object at 0x00000242F2A10430>]]
('InteractionFeatures', 'end'),
('InteractionFeatures', 'end'),('VarianceThreshold', 'end'),
('VarianceThreshold', 'end'),
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
index Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
0
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, 1.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
1
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, 2.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
2
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, origin.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
origin
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;31;40merror_str[0m [Errno 2] No such file or directory: 'HAIPipeGen/new_data/rl_rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning/origin.npy'
path error
[0;33;40m try times:2[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
0
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning
True
mkdir  HAIPipeGen/new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
Traceback (most recent call last):
  File ".\example.py", line 4, in <module>
    generate_one_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 45, in generate_one_hai
    output(notebook_id,hai_name)
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 35, in output
    shutil.rmtree('new_data')
  File "D:\Anaconda3\envs\firefly\lib\shutil.py", line 740, in rmtree
    return _rmtree_unsafe(path, onerror)
  File "D:\Anaconda3\envs\firefly\lib\shutil.py", line 599, in _rmtree_unsafe
    onerror(os.scandir, path, sys.exc_info())
  File "D:\Anaconda3\envs\firefly\lib\shutil.py", line 596, in _rmtree_unsafe
    with os.scandir(path) as scandir_it:
FileNotFoundError: [WinError 3] 系统找不到指定的路径。: 'new_data'
notebook_id datascientist25_gender-recognition-by-voice-using-machine-learning
hi_score: {'accuracy_score': 0.9186991869918699}
hai_score: {'accuracy_score': 0.9857723577235772}
notebook_id datascientist25_gender-recognition-by-voice-using-machine-learning
hi_score: {'accuracy_score': 0.9186991869918699}
hai_score: {'accuracy_score': 0.9857723577235772}
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

save_code True
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
ASSIGN******

df = pd.read_csv('../input/voicegender/voice.csv')

[1;33;44m{'pd'}[0m
    ###########
    create black edge
    black code: 
df = pd.read_csv('../input/voicegender/voice.csv')

self.get_varible_results ['df', 'pd']
    varibles ['df']
EXPR******

df.columns

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.columns

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.shape

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.shape

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.dtypes

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.dtypes

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.head(3)

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x0000028BC367D790>
should_create_black_edge False
EXPR******

df.isnull().values.any()

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.isnull().values.any()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x0000028BC367D970>
should_create_black_edge False
ASSIGN******

colors = ['pink', 'Lightblue']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['pink', 'Lightblue']

self.get_varible_results ['colors']
    varibles ['colors']
ASSIGN******

data_y = df[df.columns[(- 1)]]

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
data_y = df[df.columns[(- 1)]]

self.get_varible_results ['data_y', 'df', 'df']
    varibles ['df']
EXPR******

print(df['label'].value_counts())

self.should_create_white_edge True
    ###########
    create white edge
    white code: print(df['label'].value_counts())

self.get_varible_results [('print', False), ('df', True)]
should_create_black_edge True
    ###########
    create black edge
    black code: 
print(df['label'].value_counts())

self.get_varible_results ['print', 'df']
    varibles ['df']
ASSIGN******

correlation = df.corr()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df.corr()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df', 'colors', 'data_y'])
    ###########
    create assign edge
    assign target: ['correlation']
    assign code: 
correlation = df.corr()

ASSIGN******

X = df[df.columns[:(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
X = df[df.columns[:(- 1)]].values

self.get_varible_results ['X', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

y = df[df.columns[(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
y = df[df.columns[(- 1)]].values

self.get_varible_results ['y', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(X, y, test_size=0.3)

self.get_varible_results [('train_test_split', False), ('X', False), ('y', False)]
    white varibles: ['X', 'y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

EXPR******

warnings.filterwarnings('ignore')

self.should_create_white_edge True
    ###########
    create white edge
    white code: warnings.filterwarnings('ignore')

self.get_varible_results [('warnings', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
warnings.filterwarnings('ignore')

self.get_varible_results ['warnings']
    varibles []
ASSIGN******

rand_forest = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
rand_forest = RandomForestClassifier()

self.get_varible_results ['rand_forest', 'RandomForestClassifier']
    varibles ['rand_forest']
ASSIGN******

CVFirst = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
CVFirst = GaussianNB()

self.get_varible_results ['CVFirst', 'GaussianNB']
    varibles ['CVFirst']
ASSIGN******

male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

self.get_varible_results ['male_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

self.get_varible_results ['female_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

[1;33;44m{'list'}[0m
    ###########
    create black edge
    black code: 
index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

self.get_varible_results ['index_to_remove', 'list', 'male_funFreq_outlier_index', 'list', 'female_funFreq_outlier_index']
    varibles ['male_funFreq_outlier_index', 'female_funFreq_outlier_index']
var male_funFreq_outlier_index
var female_funFreq_outlier_index
EXPR******

len(index_to_remove)

self.should_create_white_edge True
    ###########
    create white edge
    white code: len(index_to_remove)

self.get_varible_results [('len', False), ('index_to_remove', False)]
    white varibles: ['index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x0000028BC367D9D0>
should_create_black_edge False
ASSIGN******

data_x = df[df.columns[0:20]].copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df[df.columns[0:20]].copy()

self.get_varible_results [('df', True), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = df[df.columns[0:20]].copy()

self.get_varible_results ['data_x', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

self.get_varible_results [('data_x', False)]
    white varibles: ['data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

EXPR******

data2.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: data2.head(3)

self.get_varible_results [('data2', False)]
    white varibles: ['data2']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x0000028BADF1F2E0>
should_create_black_edge False
ASSIGN******

data2 = data2.drop(index_to_remove, axis=0)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data2.drop(index_to_remove, axis=0)

self.get_varible_results [('data2', False), ('index_to_remove', False)]
    white varibles: ['data2', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data2.drop(index_to_remove, axis=0)

ASSIGN******

data_y = pd.Series(y).drop(index_to_remove, axis=0)

[1;33;44m{'pd'}[0m
    ###########
    create white edge
    white code: pd.Series(y).drop(index_to_remove, axis=0)

self.get_varible_results [('pd', False), ('y', False), ('index_to_remove', False)]
    white varibles: ['y', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data_y']
    assign code: 
data_y = pd.Series(y).drop(index_to_remove, axis=0)

ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

self.get_varible_results [('train_test_split', False), ('data2', False), ('data_y', False)]
    white varibles: ['data2', 'data_y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

ASSIGN******

clf1 = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
clf1 = RandomForestClassifier()

self.get_varible_results ['clf1', 'RandomForestClassifier']
    varibles ['clf1']
ASSIGN******

clf2 = DecisionTreeClassifier()

[1;33;44m{'DecisionTreeClassifier'}[0m
    ###########
    create black edge
    black code: 
clf2 = DecisionTreeClassifier()

self.get_varible_results ['clf2', 'DecisionTreeClassifier']
    varibles ['clf2']
ASSIGN******

clf3 = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
clf3 = GaussianNB()

self.get_varible_results ['clf3', 'GaussianNB']
    varibles ['clf3']
ASSIGN******

clf4 = LogisticRegression()

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
clf4 = LogisticRegression()

self.get_varible_results ['clf4', 'LogisticRegression']
    varibles ['clf4']
ASSIGN******

labels = ['female', 'male']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = ['female', 'male']

self.get_varible_results ['labels']
    varibles ['labels']
EXPR******

pl.title('Confusion matrix of the classifier')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.title('Confusion matrix of the classifier')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.title('Confusion matrix of the classifier')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.xlabel('Predicted')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.xlabel('Predicted')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.xlabel('Predicted')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.ylabel('True')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.ylabel('True')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.ylabel('True')

self.get_varible_results ['pl']
    varibles []
EXPR******

style.use('ggplot')

self.should_create_white_edge True
    ###########
    create white edge
    white code: style.use('ggplot')

self.get_varible_results [('style', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
style.use('ggplot')

self.get_varible_results ['style']
    varibles []
ASSIGN******

data_x = np.array(df[['meanfreq', 'meanfun']])

[1;33;44m{'np'}[0m
    ###########
    create white edge
    white code: np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results [('np', False), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results ['data_x', 'np', 'df']
    varibles ['data_x', 'df']
var data_x
var df
ASSIGN******

kmeans = KMeans(n_clusters=2)

[1;33;44m{'KMeans'}[0m
    ###########
    create black edge
    black code: 
kmeans = KMeans(n_clusters=2)

self.get_varible_results ['kmeans', 'KMeans']
    varibles ['kmeans']
EXPR******

kmeans.fit(data_x)

self.should_create_white_edge True
    ###########
    create white edge
    white code: kmeans.fit(data_x)

self.get_varible_results [('kmeans', False), ('data_x', False)]
    white varibles: ['kmeans', 'data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x0000028BADF1F580>
should_create_black_edge False
ASSIGN******

centroids = kmeans.cluster_centers_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
centroids = kmeans.cluster_centers_

self.get_varible_results ['centroids', 'kmeans']
    varibles ['kmeans']
var kmeans
ASSIGN******

labels = kmeans.labels_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = kmeans.labels_

self.get_varible_results ['labels', 'kmeans']
    varibles ['labels', 'kmeans']
var labels
var kmeans
ASSIGN******

colors = ['g.', 'b.']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['g.', 'b.']

self.get_varible_results ['colors']
    varibles ['colors']
EXPR******

print('start running model training........')

self.should_create_white_edge True
    ###########
    create white edge
    white code: print('start running model training........')

self.get_varible_results [('print', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
print('start running model training........')

self.get_varible_results ['print']
    varibles []
ASSIGN******

model = LogisticRegression(solver='liblinear', random_state=0)

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
model = LogisticRegression(solver='liblinear', random_state=0)

self.get_varible_results ['model', 'LogisticRegression']
    varibles ['model']
EXPR******

model.fit(Xtrain, ytrain)

self.should_create_white_edge True
    ###########
    create white edge
    white code: model.fit(Xtrain, ytrain)

self.get_varible_results [('model', False), ('Xtrain', False), ('ytrain', False)]
    white varibles: ['model', 'Xtrain', 'ytrain']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x0000028BADF1FDC0>
should_create_black_edge False
ASSIGN******

y_pred = model.predict(Xtest)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: model.predict(Xtest)

self.get_varible_results [('model', False), ('Xtest', False)]
    white varibles: ['model', 'Xtest']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
    ###########
    create assign edge
    assign target: ['y_pred']
    assign code: 
y_pred = model.predict(Xtest)

ASSIGN******

score = accuracy_score(ytest, y_pred)

[1;33;44m{'accuracy_score'}[0m
    ###########
    create white edge
    white code: accuracy_score(ytest, y_pred)

self.get_varible_results [('accuracy_score', False), ('ytest', False), ('y_pred', False)]
    white varibles: ['ytest', 'y_pred']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred'])
    ###########
    create assign edge
    assign target: ['score']
    assign code: 
score = accuracy_score(ytest, y_pred)

EXPR******

np.save('HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.should_create_white_edge True
    ###########
    create white edge
    white code: np.save('HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.get_varible_results [('np', False), ('score', False)]
    white varibles: ['score']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred', '-RESULT-accuracy_score', 'score'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x0000028BADF16190>
should_create_black_edge False
predict model.predict(Xtest)

predict model.predict(Xtest)

self.model_variable ['model', 'model']
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name correlation
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name X
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name male_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name female_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name index_to_remove
self.nodes[node_index].varible_name -RESULT-len
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
model
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name centroids
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
y_pred
self.nodes[node_index].varible_name y_pred
-RESULT-accuracy_score
self.nodes[node_index].varible_name -RESULT-accuracy_score
score
self.nodes[node_index].varible_name score
-RESULT-np.save
self.nodes[node_index].varible_name -RESULT-np.save
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import matplotlib.pyplot as plt
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import seaborn as sns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.columns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.shape
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.dtypes
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.isnull().values.any()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ['pink','Lightblue']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = df[df.columns[-1]]
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.axis('equal')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line print (df['label'].value_counts())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line correlation =df.corr()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.heatmap(correlation)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line X = df[df.columns[:-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y = df[df.columns[-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import warnings
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line warnings.filterwarnings("ignore")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.ensemble import RandomForestClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line rand_forest = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = rand_forest.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn import metrics, neighbors
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import confusion_matrix
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(confusion_matrix(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.naive_bayes import GaussianNB
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import cross_val_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line CVFirst = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line len(index_to_remove)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = df[df.columns[0:20]].copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
##############################
line_index 45
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy() edge data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

line_id [45]
[45]
[45]
xxxx
line data2.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf1 = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf1.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = clf1.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.tree import DecisionTreeClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf2 = DecisionTreeClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf2.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict = clf2.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf3 = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predd = clf3.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf4 = LogisticRegression()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf4.fit(Xtrain,ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict4 = clf4.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pylab as pl
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = ['female', 'male']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig = plt.figure()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax = fig.add_subplot(111)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cax =ax.matshow(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.title('Confusion matrix of the classifier')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig.colorbar(cax)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_xticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_yticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.xlabel('Predicted')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.ylabel('True')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #pl.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import classification_report
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(classification_report(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.cluster import KMeans
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from matplotlib import style
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line style.use("ggplot")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans = KMeans(n_clusters= 2)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans.fit(data_x)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line centroids = kmeans.cluster_centers_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = kmeans.labels_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ["g.","b."]  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #for i in range(len(data_x)):
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line     
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.ylabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.xlabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line print("start running model training........")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y_pred = model.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line score = accuracy_score(ytest, y_pred)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
#plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
[[<HAIPipeGen.merge.Step object at 0x0000028BADD67F10>],
 [<HAIPipeGen.merge.Step object at 0x0000028BADD67EE0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C0A0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C100>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C160>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C1C0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C1F0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C250>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C2B0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BADD67F10>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C2B0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C0A0>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C2B0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C160>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C2B0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C310>],
 [<HAIPipeGen.merge.Step object at 0x0000028BADD67F10>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C310>],
 [<HAIPipeGen.merge.Step object at 0x0000028BADD67EE0>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C310>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C0A0>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C310>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C100>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C310>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C160>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C310>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C1C0>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C310>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C400>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C0A0>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C400>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C160>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C400>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C580>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C0A0>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C580>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C100>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C580>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C160>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C580>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C1C0>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C580>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C640>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C160>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C640>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C760>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C160>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C760>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C1C0>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C760>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C7F0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BADD67F10>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C7F0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C0A0>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C7F0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C160>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C7F0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C1F0>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C7F0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C8B0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BADD67F10>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C8B0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BADD67EE0>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C8B0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C0A0>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C8B0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C100>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C8B0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C160>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C8B0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C1C0>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C8B0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C1F0>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C8B0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367C250>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367C8B0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BADD67FA0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367CB80>,
  <HAIPipeGen.merge.Step object at 0x0000028BC367CBE0>],
 [<HAIPipeGen.merge.Step object at 0x0000028BC367CC40>]]
('InteractionFeatures', 'end'),
('InteractionFeatures', 'end'),('VarianceThreshold', 'end'),
('VarianceThreshold', 'end'),
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
index Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
0
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
True
mkdir  HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, 1.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
1
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, 2.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
2
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, origin.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
origin
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
0
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning
True
mkdir  HAIPipeGen/new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
female    1584
male      1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
notebook_id datascientist25_gender-recognition-by-voice-using-machine-learning
hi_score: {'accuracy_score': 0.9186991869918699}
hai_score: {'accuracy_score': 0.9857723577235772}
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

save_code True
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
ASSIGN******

df = pd.read_csv('../input/voicegender/voice.csv')

[1;33;44m{'pd'}[0m
    ###########
    create black edge
    black code: 
df = pd.read_csv('../input/voicegender/voice.csv')

self.get_varible_results ['df', 'pd']
    varibles ['df']
EXPR******

df.columns

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.columns

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.shape

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.shape

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.dtypes

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.dtypes

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.head(3)

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x000001EF7F34E790>
should_create_black_edge False
EXPR******

df.isnull().values.any()

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.isnull().values.any()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x000001EF7F34E970>
should_create_black_edge False
ASSIGN******

colors = ['pink', 'Lightblue']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['pink', 'Lightblue']

self.get_varible_results ['colors']
    varibles ['colors']
ASSIGN******

data_y = df[df.columns[(- 1)]]

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
data_y = df[df.columns[(- 1)]]

self.get_varible_results ['data_y', 'df', 'df']
    varibles ['df']
EXPR******

print(df['label'].value_counts())

self.should_create_white_edge True
    ###########
    create white edge
    white code: print(df['label'].value_counts())

self.get_varible_results [('print', False), ('df', True)]
should_create_black_edge True
    ###########
    create black edge
    black code: 
print(df['label'].value_counts())

self.get_varible_results ['print', 'df']
    varibles ['df']
ASSIGN******

correlation = df.corr()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df.corr()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df', 'colors', 'data_y'])
    ###########
    create assign edge
    assign target: ['correlation']
    assign code: 
correlation = df.corr()

ASSIGN******

X = df[df.columns[:(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
X = df[df.columns[:(- 1)]].values

self.get_varible_results ['X', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

y = df[df.columns[(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
y = df[df.columns[(- 1)]].values

self.get_varible_results ['y', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(X, y, test_size=0.3)

self.get_varible_results [('train_test_split', False), ('X', False), ('y', False)]
    white varibles: ['X', 'y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

EXPR******

warnings.filterwarnings('ignore')

self.should_create_white_edge True
    ###########
    create white edge
    white code: warnings.filterwarnings('ignore')

self.get_varible_results [('warnings', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
warnings.filterwarnings('ignore')

self.get_varible_results ['warnings']
    varibles []
ASSIGN******

rand_forest = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
rand_forest = RandomForestClassifier()

self.get_varible_results ['rand_forest', 'RandomForestClassifier']
    varibles ['rand_forest']
ASSIGN******

CVFirst = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
CVFirst = GaussianNB()

self.get_varible_results ['CVFirst', 'GaussianNB']
    varibles ['CVFirst']
ASSIGN******

male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

self.get_varible_results ['male_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

self.get_varible_results ['female_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

[1;33;44m{'list'}[0m
    ###########
    create black edge
    black code: 
index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

self.get_varible_results ['index_to_remove', 'list', 'male_funFreq_outlier_index', 'list', 'female_funFreq_outlier_index']
    varibles ['male_funFreq_outlier_index', 'female_funFreq_outlier_index']
var male_funFreq_outlier_index
var female_funFreq_outlier_index
EXPR******

len(index_to_remove)

self.should_create_white_edge True
    ###########
    create white edge
    white code: len(index_to_remove)

self.get_varible_results [('len', False), ('index_to_remove', False)]
    white varibles: ['index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x000001EF7F34E9D0>
should_create_black_edge False
ASSIGN******

data_x = df[df.columns[0:20]].copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df[df.columns[0:20]].copy()

self.get_varible_results [('df', True), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = df[df.columns[0:20]].copy()

self.get_varible_results ['data_x', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

self.get_varible_results [('data_x', False)]
    white varibles: ['data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

EXPR******

data2.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: data2.head(3)

self.get_varible_results [('data2', False)]
    white varibles: ['data2']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x000001EF6BC032E0>
should_create_black_edge False
ASSIGN******

data2 = data2.drop(index_to_remove, axis=0)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data2.drop(index_to_remove, axis=0)

self.get_varible_results [('data2', False), ('index_to_remove', False)]
    white varibles: ['data2', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data2.drop(index_to_remove, axis=0)

ASSIGN******

data_y = pd.Series(y).drop(index_to_remove, axis=0)

[1;33;44m{'pd'}[0m
    ###########
    create white edge
    white code: pd.Series(y).drop(index_to_remove, axis=0)

self.get_varible_results [('pd', False), ('y', False), ('index_to_remove', False)]
    white varibles: ['y', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data_y']
    assign code: 
data_y = pd.Series(y).drop(index_to_remove, axis=0)

ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

self.get_varible_results [('train_test_split', False), ('data2', False), ('data_y', False)]
    white varibles: ['data2', 'data_y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

ASSIGN******

clf1 = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
clf1 = RandomForestClassifier()

self.get_varible_results ['clf1', 'RandomForestClassifier']
    varibles ['clf1']
ASSIGN******

clf2 = DecisionTreeClassifier()

[1;33;44m{'DecisionTreeClassifier'}[0m
    ###########
    create black edge
    black code: 
clf2 = DecisionTreeClassifier()

self.get_varible_results ['clf2', 'DecisionTreeClassifier']
    varibles ['clf2']
ASSIGN******

clf3 = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
clf3 = GaussianNB()

self.get_varible_results ['clf3', 'GaussianNB']
    varibles ['clf3']
ASSIGN******

clf4 = LogisticRegression()

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
clf4 = LogisticRegression()

self.get_varible_results ['clf4', 'LogisticRegression']
    varibles ['clf4']
ASSIGN******

labels = ['female', 'male']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = ['female', 'male']

self.get_varible_results ['labels']
    varibles ['labels']
EXPR******

pl.title('Confusion matrix of the classifier')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.title('Confusion matrix of the classifier')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.title('Confusion matrix of the classifier')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.xlabel('Predicted')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.xlabel('Predicted')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.xlabel('Predicted')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.ylabel('True')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.ylabel('True')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.ylabel('True')

self.get_varible_results ['pl']
    varibles []
EXPR******

style.use('ggplot')

self.should_create_white_edge True
    ###########
    create white edge
    white code: style.use('ggplot')

self.get_varible_results [('style', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
style.use('ggplot')

self.get_varible_results ['style']
    varibles []
ASSIGN******

data_x = np.array(df[['meanfreq', 'meanfun']])

[1;33;44m{'np'}[0m
    ###########
    create white edge
    white code: np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results [('np', False), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results ['data_x', 'np', 'df']
    varibles ['data_x', 'df']
var data_x
var df
ASSIGN******

kmeans = KMeans(n_clusters=2)

[1;33;44m{'KMeans'}[0m
    ###########
    create black edge
    black code: 
kmeans = KMeans(n_clusters=2)

self.get_varible_results ['kmeans', 'KMeans']
    varibles ['kmeans']
EXPR******

kmeans.fit(data_x)

self.should_create_white_edge True
    ###########
    create white edge
    white code: kmeans.fit(data_x)

self.get_varible_results [('kmeans', False), ('data_x', False)]
    white varibles: ['kmeans', 'data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x000001EF6BC03580>
should_create_black_edge False
ASSIGN******

centroids = kmeans.cluster_centers_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
centroids = kmeans.cluster_centers_

self.get_varible_results ['centroids', 'kmeans']
    varibles ['kmeans']
var kmeans
ASSIGN******

labels = kmeans.labels_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = kmeans.labels_

self.get_varible_results ['labels', 'kmeans']
    varibles ['labels', 'kmeans']
var labels
var kmeans
ASSIGN******

colors = ['g.', 'b.']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['g.', 'b.']

self.get_varible_results ['colors']
    varibles ['colors']
EXPR******

print('start running model training........')

self.should_create_white_edge True
    ###########
    create white edge
    white code: print('start running model training........')

self.get_varible_results [('print', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
print('start running model training........')

self.get_varible_results ['print']
    varibles []
ASSIGN******

model = LogisticRegression(solver='liblinear', random_state=0)

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
model = LogisticRegression(solver='liblinear', random_state=0)

self.get_varible_results ['model', 'LogisticRegression']
    varibles ['model']
EXPR******

model.fit(Xtrain, ytrain)

self.should_create_white_edge True
    ###########
    create white edge
    white code: model.fit(Xtrain, ytrain)

self.get_varible_results [('model', False), ('Xtrain', False), ('ytrain', False)]
    white varibles: ['model', 'Xtrain', 'ytrain']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x000001EF6BC03DC0>
should_create_black_edge False
ASSIGN******

y_pred = model.predict(Xtest)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: model.predict(Xtest)

self.get_varible_results [('model', False), ('Xtest', False)]
    white varibles: ['model', 'Xtest']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
    ###########
    create assign edge
    assign target: ['y_pred']
    assign code: 
y_pred = model.predict(Xtest)

ASSIGN******

score = accuracy_score(ytest, y_pred)

[1;33;44m{'accuracy_score'}[0m
    ###########
    create white edge
    white code: accuracy_score(ytest, y_pred)

self.get_varible_results [('accuracy_score', False), ('ytest', False), ('y_pred', False)]
    white varibles: ['ytest', 'y_pred']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred'])
    ###########
    create assign edge
    assign target: ['score']
    assign code: 
score = accuracy_score(ytest, y_pred)

EXPR******

np.save('HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.should_create_white_edge True
    ###########
    create white edge
    white code: np.save('HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.get_varible_results [('np', False), ('score', False)]
    white varibles: ['score']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred', '-RESULT-accuracy_score', 'score'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x000001EF6BBF7160>
should_create_black_edge False
predict model.predict(Xtest)

predict model.predict(Xtest)

self.model_variable ['model', 'model']
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name correlation
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name X
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name male_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name female_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name index_to_remove
self.nodes[node_index].varible_name -RESULT-len
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
model
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name centroids
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
y_pred
self.nodes[node_index].varible_name y_pred
-RESULT-accuracy_score
self.nodes[node_index].varible_name -RESULT-accuracy_score
score
self.nodes[node_index].varible_name score
-RESULT-np.save
self.nodes[node_index].varible_name -RESULT-np.save
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import matplotlib.pyplot as plt
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import seaborn as sns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.columns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.shape
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.dtypes
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.isnull().values.any()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ['pink','Lightblue']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = df[df.columns[-1]]
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.axis('equal')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line print (df['label'].value_counts())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line correlation =df.corr()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.heatmap(correlation)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line X = df[df.columns[:-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y = df[df.columns[-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import warnings
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line warnings.filterwarnings("ignore")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.ensemble import RandomForestClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line rand_forest = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = rand_forest.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn import metrics, neighbors
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import confusion_matrix
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(confusion_matrix(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.naive_bayes import GaussianNB
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import cross_val_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line CVFirst = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line len(index_to_remove)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = df[df.columns[0:20]].copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
##############################
line_index 45
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy() edge data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

line_id [45]
[45]
[45]
xxxx
line data2.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf1 = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf1.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = clf1.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.tree import DecisionTreeClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf2 = DecisionTreeClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf2.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict = clf2.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf3 = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predd = clf3.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf4 = LogisticRegression()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf4.fit(Xtrain,ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict4 = clf4.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pylab as pl
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = ['female', 'male']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig = plt.figure()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax = fig.add_subplot(111)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cax =ax.matshow(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.title('Confusion matrix of the classifier')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig.colorbar(cax)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_xticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_yticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.xlabel('Predicted')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.ylabel('True')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #pl.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import classification_report
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(classification_report(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.cluster import KMeans
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from matplotlib import style
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line style.use("ggplot")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans = KMeans(n_clusters= 2)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans.fit(data_x)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line centroids = kmeans.cluster_centers_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = kmeans.labels_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ["g.","b."]  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #for i in range(len(data_x)):
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line     
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.ylabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.xlabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line print("start running model training........")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y_pred = model.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line score = accuracy_score(ytest, y_pred)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
#plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
[[<HAIPipeGen.merge.Step object at 0x000001EF6BA47F10>],
 [<HAIPipeGen.merge.Step object at 0x000001EF6BA47EE0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D0A0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D100>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D160>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D1C0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D1F0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D250>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D2B0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF6BA47F10>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D2B0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D0A0>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D2B0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D160>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D2B0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D310>],
 [<HAIPipeGen.merge.Step object at 0x000001EF6BA47F10>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D310>],
 [<HAIPipeGen.merge.Step object at 0x000001EF6BA47EE0>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D310>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D0A0>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D310>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D100>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D310>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D160>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D310>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D1C0>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D310>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D400>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D0A0>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D400>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D160>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D400>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D580>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D0A0>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D580>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D100>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D580>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D160>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D580>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D1C0>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D580>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D640>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D160>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D640>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D760>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D160>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D760>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D1C0>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D760>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D7F0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF6BA47F10>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D7F0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D0A0>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D7F0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D160>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D7F0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D1F0>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D7F0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D8B0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF6BA47F10>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D8B0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF6BA47EE0>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D8B0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D0A0>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D8B0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D100>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D8B0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D160>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D8B0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D1C0>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D8B0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D1F0>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D8B0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34D250>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34D8B0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF6BA47FA0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34DB80>,
  <HAIPipeGen.merge.Step object at 0x000001EF7F34DBE0>],
 [<HAIPipeGen.merge.Step object at 0x000001EF7F34DC40>]]
('InteractionFeatures', 'end'),
('InteractionFeatures', 'end'),('VarianceThreshold', 'end'),
('VarianceThreshold', 'end'),
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
index Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
item, 0.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
0
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
True
mkdir  HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, 1.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
1
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, 2.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
2
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
item, origin.json
notebook, datascientist25_gender-recognition-by-voice-using-machine-learning
origin
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/rl_cross_val_res/datascientist25_gender-recognition-by-voice-using-machine-learning
False
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
0
datascientist25_gender-recognition-by-voice-using-machine-learning
??????
HAIPipeGen/new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning
True
mkdir  HAIPipeGen/new_data/merge_max_result_rl/datascientist25_gender-recognition-by-voice-using-machine-learning
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
start running model training........
[0;32;40msucceed[0m
notebook_id datascientist25_gender-recognition-by-voice-using-machine-learning
hi_score: {'accuracy_score': 0.9186991869918699}
hai_score: {'accuracy_score': 0.9857723577235772}
Traceback (most recent call last):
  File ".\example.py", line 1, in <module>
    from HAIPipeGen.generate_hai import *
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 5, in <module>
    from HAIPipeGen.NotebookGraph import build_one_graph
  File "E:\firefly\HAIPipeGen\NotebookGraph.py", line 97
    class NotebookGraph(ast.NodeVisitor):
    ^
IndentationError: expected an indented block
Traceback (most recent call last):
  File ".\example.py", line 1, in <module>
    from HAIPipeGen.generate_hai import *
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 6, in <module>
    from HAIPipeGen.merge import merge_one_code
  File "E:\firefly\HAIPipeGen\merge.py", line 158
    def print_one_seq(self,seq):
    ^
IndentationError: expected an indented block
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ".\example.py", line 4, in <module>
    generate_one_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 39, in generate_one_hai
    run_one_hi(notebook_id)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 1808, in run_one_hi
    pro.run_origin_test(notebook_id, need_try_again=2)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 807, in run_origin_test
    self.code = self.run_one_code(notebook_id, self.code, dataset_root_path + self.info_triple[notebook_id]['dataset_name'], 0)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 734, in run_one_code
    res = self.run_one_code(notebook_id, new_code, new_path, try_time + 1,found)
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 81, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\pyplot.py", line 3026, in title
    return gca().set_title(
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\pyplot.py", line 2262, in gca
    return gcf().gca(**kwargs)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\pyplot.py", line 846, in gcf
    return figure()
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\pyplot.py", line 787, in figure
    manager = new_figure_manager(
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\pyplot.py", line 306, in new_figure_manager
    return _backend_mod.new_figure_manager(*args, **kwargs)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\backend_bases.py", line 3494, in new_figure_manager
    return cls.new_figure_manager_given_figure(num, fig)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\backends\_backend_tk.py", line 936, in new_figure_manager_given_figure
    window.iconphoto(False, icon_img)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 2127, in wm_iconphoto
    self.tk.call('wm', 'iconphoto', self._w, *args)
KeyboardInterrupt
!!!!!!!!!!!!!!!!!!!!!!!
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




self.line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )
self.train_test_split_index 49
result_id 1
self.train_test_split_index 49
self.code from sklearn.model_selection import train_test_split
data_y = pd.Series(y).drop(index_to_remove,axis=0)
self.end_index 48
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('../input/voicegender/voice.csv')
df.columns
df.shape
df.dtypes
df.head(3)
df.isnull().values.any()
colors = ['pink','Lightblue']
data_y = df[df.columns[-1]]
#plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
#plt.axis('equal')
print (df['label'].value_counts())
#df.boxplot(column = 'meanfreq',by='label',grid=False)
correlation =df.corr()
#sns.heatmap(correlation)
#plt.show()
from sklearn.model_selection import train_test_split
X = df[df.columns[:-1]].values
y = df[df.columns[-1]].values
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestClassifier
rand_forest = RandomForestClassifier()
#rand_forest.fit(Xtrain, ytrain)
#y_pred = rand_forest.predict(Xtest)
from sklearn import metrics, neighbors
from sklearn.metrics import accuracy_score
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.metrics import confusion_matrix
#print(confusion_matrix(ytest, y_pred))
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
CVFirst = GaussianNB()
#CVFirst = CVFirst.fit(Xtrain, ytrain)
#test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
len(index_to_remove)
data_x = df[df.columns[0:20]].copy()
data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.head(3)
data2 = data2.drop(index_to_remove,axis=0)
data_y = pd.Series(y).drop(index_to_remove,axis=0)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
clf1 = RandomForestClassifier()
#clf1.fit(Xtrain, ytrain)
#y_pred = clf1.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_pred))
from sklearn.tree import DecisionTreeClassifier
clf2 = DecisionTreeClassifier()
#clf2.fit(Xtrain, ytrain)
#y_predict = clf2.predict(Xtest)
#print(metrics.accuracy_score(ytest, y_predict))
clf3 = GaussianNB()
#clf3 = clf3.fit(Xtrain, ytrain)
#y_predd = clf3.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predd))
from sklearn.linear_model import LogisticRegression
clf4 = LogisticRegression()
#clf4.fit(Xtrain,ytrain)
#y_predict4 = clf4.predict(Xtest)
#print(metrics.accuracy_score(ytest,y_predict4))
#test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
#print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
#test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
#print('Accuracy obtained from 10-fold validation is:',test_result.mean())
import pylab as pl
labels = ['female', 'male']
#cm = confusion_matrix(ytest,y_pred,labels)  
#print(cm)
#fig = plt.figure()
#ax = fig.add_subplot(111)
#cax =ax.matshow(cm)
pl.title('Confusion matrix of the classifier')
#fig.colorbar(cax)
#ax.set_xticklabels([''] + labels)
#ax.set_yticklabels([''] + labels)
pl.xlabel('Predicted')
pl.ylabel('True')
#pl.show()
from sklearn.metrics import classification_report
#print(classification_report(ytest, y_pred))
#sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
#plt.show()
from sklearn.cluster import KMeans
from matplotlib import style
style.use("ggplot")
data_x = np.array(df[['meanfreq','meanfun']])
kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
centroids = kmeans.cluster_centers_
labels = kmeans.labels_
colors = ["g.","b."]  
#for i in range(len(data_x)):
#    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
    
#plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
#plt.ylabel('meanfun')
#plt.xlabel('meanfun')
#plt.show()




import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model.logistic import LogisticRegression
print("start running model training........")
model = LogisticRegression(solver='liblinear', random_state=0)
model.fit(Xtrain, ytrain)
y_pred = model.predict(Xtest)
score = accuracy_score(ytest, y_pred)
import numpy as np
np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })

save_code True
[0;33;40m try times:0[0m
[0;31;40merror_str[0m [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
path error
[0;33;40m try times:1[0m
male      1584
female    1584
Name: label, dtype: int64
female    1584
male      1584
Name: label, dtype: int64
ASSIGN******

df = pd.read_csv('../input/voicegender/voice.csv')

[1;33;44m{'pd'}[0m
    ###########
    create black edge
    black code: 
df = pd.read_csv('../input/voicegender/voice.csv')

self.get_varible_results ['df', 'pd']
    varibles ['df']
EXPR******

df.columns

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.columns

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.shape

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.shape

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.dtypes

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.dtypes

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.head(3)

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x000001946BA2B670>
should_create_black_edge False
EXPR******

df.isnull().values.any()

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.isnull().values.any()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x000001946BA2B850>
should_create_black_edge False
ASSIGN******

colors = ['pink', 'Lightblue']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['pink', 'Lightblue']

self.get_varible_results ['colors']
    varibles ['colors']
ASSIGN******

data_y = df[df.columns[(- 1)]]

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
data_y = df[df.columns[(- 1)]]

self.get_varible_results ['data_y', 'df', 'df']
    varibles ['df']
EXPR******

print(df['label'].value_counts())

self.should_create_white_edge True
    ###########
    create white edge
    white code: print(df['label'].value_counts())

self.get_varible_results [('print', False), ('df', True)]
should_create_black_edge True
    ###########
    create black edge
    black code: 
print(df['label'].value_counts())

self.get_varible_results ['print', 'df']
    varibles ['df']
ASSIGN******

correlation = df.corr()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df.corr()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df', 'colors', 'data_y'])
    ###########
    create assign edge
    assign target: ['correlation']
    assign code: 
correlation = df.corr()

ASSIGN******

X = df[df.columns[:(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
X = df[df.columns[:(- 1)]].values

self.get_varible_results ['X', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

y = df[df.columns[(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
y = df[df.columns[(- 1)]].values

self.get_varible_results ['y', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(X, y, test_size=0.3)

self.get_varible_results [('train_test_split', False), ('X', False), ('y', False)]
    white varibles: ['X', 'y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

EXPR******

warnings.filterwarnings('ignore')

self.should_create_white_edge True
    ###########
    create white edge
    white code: warnings.filterwarnings('ignore')

self.get_varible_results [('warnings', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
warnings.filterwarnings('ignore')

self.get_varible_results ['warnings']
    varibles []
ASSIGN******

rand_forest = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
rand_forest = RandomForestClassifier()

self.get_varible_results ['rand_forest', 'RandomForestClassifier']
    varibles ['rand_forest']
ASSIGN******

CVFirst = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
CVFirst = GaussianNB()

self.get_varible_results ['CVFirst', 'GaussianNB']
    varibles ['CVFirst']
ASSIGN******

male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

self.get_varible_results ['male_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

self.get_varible_results ['female_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

[1;33;44m{'list'}[0m
    ###########
    create black edge
    black code: 
index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

self.get_varible_results ['index_to_remove', 'list', 'male_funFreq_outlier_index', 'list', 'female_funFreq_outlier_index']
    varibles ['male_funFreq_outlier_index', 'female_funFreq_outlier_index']
var male_funFreq_outlier_index
var female_funFreq_outlier_index
EXPR******

len(index_to_remove)

self.should_create_white_edge True
    ###########
    create white edge
    white code: len(index_to_remove)

self.get_varible_results [('len', False), ('index_to_remove', False)]
    white varibles: ['index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x000001946BA2B910>
should_create_black_edge False
ASSIGN******

data_x = df[df.columns[0:20]].copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df[df.columns[0:20]].copy()

self.get_varible_results [('df', True), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = df[df.columns[0:20]].copy()

self.get_varible_results ['data_x', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

self.get_varible_results [('data_x', False)]
    white varibles: ['data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

EXPR******

data2.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: data2.head(3)

self.get_varible_results [('data2', False)]
    white varibles: ['data2']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x00000194561E0310>
should_create_black_edge False
ASSIGN******

data2 = data2.drop(index_to_remove, axis=0)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data2.drop(index_to_remove, axis=0)

self.get_varible_results [('data2', False), ('index_to_remove', False)]
    white varibles: ['data2', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data2.drop(index_to_remove, axis=0)

ASSIGN******

data_y = pd.Series(y).drop(index_to_remove, axis=0)

[1;33;44m{'pd'}[0m
    ###########
    create white edge
    white code: pd.Series(y).drop(index_to_remove, axis=0)

self.get_varible_results [('pd', False), ('y', False), ('index_to_remove', False)]
    white varibles: ['y', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data_y']
    assign code: 
data_y = pd.Series(y).drop(index_to_remove, axis=0)

ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

self.get_varible_results [('train_test_split', False), ('data2', False), ('data_y', False)]
    white varibles: ['data2', 'data_y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

ASSIGN******

clf1 = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
clf1 = RandomForestClassifier()

self.get_varible_results ['clf1', 'RandomForestClassifier']
    varibles ['clf1']
ASSIGN******

clf2 = DecisionTreeClassifier()

[1;33;44m{'DecisionTreeClassifier'}[0m
    ###########
    create black edge
    black code: 
clf2 = DecisionTreeClassifier()

self.get_varible_results ['clf2', 'DecisionTreeClassifier']
    varibles ['clf2']
ASSIGN******

clf3 = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
clf3 = GaussianNB()

self.get_varible_results ['clf3', 'GaussianNB']
    varibles ['clf3']
ASSIGN******

clf4 = LogisticRegression()

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
clf4 = LogisticRegression()

self.get_varible_results ['clf4', 'LogisticRegression']
    varibles ['clf4']
ASSIGN******

labels = ['female', 'male']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = ['female', 'male']

self.get_varible_results ['labels']
    varibles ['labels']
EXPR******

pl.title('Confusion matrix of the classifier')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.title('Confusion matrix of the classifier')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.title('Confusion matrix of the classifier')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.xlabel('Predicted')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.xlabel('Predicted')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.xlabel('Predicted')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.ylabel('True')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.ylabel('True')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.ylabel('True')

self.get_varible_results ['pl']
    varibles []
EXPR******

style.use('ggplot')

self.should_create_white_edge True
    ###########
    create white edge
    white code: style.use('ggplot')

self.get_varible_results [('style', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
style.use('ggplot')

self.get_varible_results ['style']
    varibles []
ASSIGN******

data_x = np.array(df[['meanfreq', 'meanfun']])

[1;33;44m{'np'}[0m
    ###########
    create white edge
    white code: np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results [('np', False), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results ['data_x', 'np', 'df']
    varibles ['data_x', 'df']
var data_x
var df
ASSIGN******

kmeans = KMeans(n_clusters=2)

[1;33;44m{'KMeans'}[0m
    ###########
    create black edge
    black code: 
kmeans = KMeans(n_clusters=2)

self.get_varible_results ['kmeans', 'KMeans']
    varibles ['kmeans']
EXPR******

kmeans.fit(data_x)

self.should_create_white_edge True
    ###########
    create white edge
    white code: kmeans.fit(data_x)

self.get_varible_results [('kmeans', False), ('data_x', False)]
    white varibles: ['kmeans', 'data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x00000194561E0460>
should_create_black_edge False
ASSIGN******

centroids = kmeans.cluster_centers_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
centroids = kmeans.cluster_centers_

self.get_varible_results ['centroids', 'kmeans']
    varibles ['kmeans']
var kmeans
ASSIGN******

labels = kmeans.labels_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = kmeans.labels_

self.get_varible_results ['labels', 'kmeans']
    varibles ['labels', 'kmeans']
var labels
var kmeans
ASSIGN******

colors = ['g.', 'b.']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['g.', 'b.']

self.get_varible_results ['colors']
    varibles ['colors']
ASSIGN******

model = LogisticRegression(solver='liblinear', random_state=0)

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
model = LogisticRegression(solver='liblinear', random_state=0)

self.get_varible_results ['model', 'LogisticRegression']
    varibles ['model']
EXPR******

model.fit(Xtrain, ytrain)

self.should_create_white_edge True
    ###########
    create white edge
    white code: model.fit(Xtrain, ytrain)

self.get_varible_results [('model', False), ('Xtrain', False), ('ytrain', False)]
    white varibles: ['model', 'Xtrain', 'ytrain']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x00000194561E0CA0>
should_create_black_edge False
ASSIGN******

y_pred = model.predict(Xtest)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: model.predict(Xtest)

self.get_varible_results [('model', False), ('Xtest', False)]
    white varibles: ['model', 'Xtest']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
    ###########
    create assign edge
    assign target: ['y_pred']
    assign code: 
y_pred = model.predict(Xtest)

ASSIGN******

score = accuracy_score(ytest, y_pred)

[1;33;44m{'accuracy_score'}[0m
    ###########
    create white edge
    white code: accuracy_score(ytest, y_pred)

self.get_varible_results [('accuracy_score', False), ('ytest', False), ('y_pred', False)]
    white varibles: ['ytest', 'y_pred']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred'])
    ###########
    create assign edge
    assign target: ['score']
    assign code: 
score = accuracy_score(ytest, y_pred)

EXPR******

np.save('HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.should_create_white_edge True
    ###########
    create white edge
    white code: np.save('HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.get_varible_results [('np', False), ('score', False)]
    white varibles: ['score']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred', '-RESULT-accuracy_score', 'score'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x00000194561DE040>
should_create_black_edge False
predict model.predict(Xtest)

predict model.predict(Xtest)

self.model_variable ['model', 'model']
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name correlation
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name X
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name male_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name female_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name index_to_remove
self.nodes[node_index].varible_name -RESULT-len
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
model
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name centroids
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
y_pred
self.nodes[node_index].varible_name y_pred
-RESULT-accuracy_score
self.nodes[node_index].varible_name -RESULT-accuracy_score
score
self.nodes[node_index].varible_name score
-RESULT-np.save
self.nodes[node_index].varible_name -RESULT-np.save
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line #print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import matplotlib.pyplot as plt
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import seaborn as sns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.columns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.shape
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.dtypes
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.isnull().values.any()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ['pink','Lightblue']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = df[df.columns[-1]]
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.axis('equal')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line print (df['label'].value_counts())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line correlation =df.corr()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.heatmap(correlation)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line X = df[df.columns[:-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y = df[df.columns[-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import warnings
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line warnings.filterwarnings("ignore")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.ensemble import RandomForestClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line rand_forest = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = rand_forest.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn import metrics, neighbors
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import confusion_matrix
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(confusion_matrix(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.naive_bayes import GaussianNB
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import cross_val_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line CVFirst = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line len(index_to_remove)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = df[df.columns[0:20]].copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
##############################
line_index 45
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy() edge data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

line_id [45]
[45]
[45]
xxxx
line data2.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf1 = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf1.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = clf1.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.tree import DecisionTreeClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf2 = DecisionTreeClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf2.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict = clf2.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf3 = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predd = clf3.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf4 = LogisticRegression()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf4.fit(Xtrain,ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict4 = clf4.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pylab as pl
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = ['female', 'male']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig = plt.figure()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax = fig.add_subplot(111)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cax =ax.matshow(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.title('Confusion matrix of the classifier')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig.colorbar(cax)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_xticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_yticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.xlabel('Predicted')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.ylabel('True')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #pl.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import classification_report
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(classification_report(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.cluster import KMeans
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from matplotlib import style
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line style.use("ggplot")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans = KMeans(n_clusters= 2)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans.fit(data_x)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line centroids = kmeans.cluster_centers_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = kmeans.labels_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ["g.","b."]  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #for i in range(len(data_x)):
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line     
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.ylabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.xlabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print("start running model training........")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y_pred = model.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line score = accuracy_score(ytest, y_pred)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line #print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File ".\example.py", line 4, in <module>
    generate_one_hai('datascientist25_gender-recognition-by-voice-using-machine-learning')
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 41, in generate_one_hai
    merge_one_code(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 2660, in merge_one_code
    merger.merging_one_notebook_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 790, in merging_one_notebook_rl
    res = self.enum_adding_rl(notebook_id)
  File "E:\firefly\HAIPipeGen\merge.py", line 356, in enum_adding_rl
    pprint.p#print(self.all_seq)
AttributeError: module 'pprint' has no attribute 'p'
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line #print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line #print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
male      1584
female    1584
Name: label, dtype: int64
ASSIGN******

df = pd.read_csv('../input/voicegender/voice.csv')

[1;33;44m{'pd'}[0m
    ###########
    create black edge
    black code: 
df = pd.read_csv('../input/voicegender/voice.csv')

self.get_varible_results ['df', 'pd']
    varibles ['df']
EXPR******

df.columns

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.columns

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.shape

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.shape

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.dtypes

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.dtypes

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.head(3)

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x000001C24B1D4670>
should_create_black_edge False
EXPR******

df.isnull().values.any()

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.isnull().values.any()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x000001C24B1D4850>
should_create_black_edge False
ASSIGN******

colors = ['pink', 'Lightblue']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['pink', 'Lightblue']

self.get_varible_results ['colors']
    varibles ['colors']
ASSIGN******

data_y = df[df.columns[(- 1)]]

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
data_y = df[df.columns[(- 1)]]

self.get_varible_results ['data_y', 'df', 'df']
    varibles ['df']
EXPR******

print(df['label'].value_counts())

self.should_create_white_edge True
    ###########
    create white edge
    white code: print(df['label'].value_counts())

self.get_varible_results [('print', False), ('df', True)]
should_create_black_edge True
    ###########
    create black edge
    black code: 
print(df['label'].value_counts())

self.get_varible_results ['print', 'df']
    varibles ['df']
ASSIGN******

correlation = df.corr()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df.corr()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df', 'colors', 'data_y'])
    ###########
    create assign edge
    assign target: ['correlation']
    assign code: 
correlation = df.corr()

ASSIGN******

X = df[df.columns[:(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
X = df[df.columns[:(- 1)]].values

self.get_varible_results ['X', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

y = df[df.columns[(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
y = df[df.columns[(- 1)]].values

self.get_varible_results ['y', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(X, y, test_size=0.3)

self.get_varible_results [('train_test_split', False), ('X', False), ('y', False)]
    white varibles: ['X', 'y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

EXPR******

warnings.filterwarnings('ignore')

self.should_create_white_edge True
    ###########
    create white edge
    white code: warnings.filterwarnings('ignore')

self.get_varible_results [('warnings', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
warnings.filterwarnings('ignore')

self.get_varible_results ['warnings']
    varibles []
ASSIGN******

rand_forest = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
rand_forest = RandomForestClassifier()

self.get_varible_results ['rand_forest', 'RandomForestClassifier']
    varibles ['rand_forest']
ASSIGN******

CVFirst = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
CVFirst = GaussianNB()

self.get_varible_results ['CVFirst', 'GaussianNB']
    varibles ['CVFirst']
ASSIGN******

male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

self.get_varible_results ['male_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

self.get_varible_results ['female_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

[1;33;44m{'list'}[0m
    ###########
    create black edge
    black code: 
index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

self.get_varible_results ['index_to_remove', 'list', 'male_funFreq_outlier_index', 'list', 'female_funFreq_outlier_index']
    varibles ['male_funFreq_outlier_index', 'female_funFreq_outlier_index']
var male_funFreq_outlier_index
var female_funFreq_outlier_index
EXPR******

len(index_to_remove)

self.should_create_white_edge True
    ###########
    create white edge
    white code: len(index_to_remove)

self.get_varible_results [('len', False), ('index_to_remove', False)]
    white varibles: ['index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x000001C24B1D48B0>
should_create_black_edge False
ASSIGN******

data_x = df[df.columns[0:20]].copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df[df.columns[0:20]].copy()

self.get_varible_results [('df', True), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = df[df.columns[0:20]].copy()

self.get_varible_results ['data_x', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

self.get_varible_results [('data_x', False)]
    white varibles: ['data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

EXPR******

data2.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: data2.head(3)

self.get_varible_results [('data2', False)]
    white varibles: ['data2']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x000001C239721310>
should_create_black_edge False
ASSIGN******

data2 = data2.drop(index_to_remove, axis=0)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data2.drop(index_to_remove, axis=0)

self.get_varible_results [('data2', False), ('index_to_remove', False)]
    white varibles: ['data2', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data2.drop(index_to_remove, axis=0)

ASSIGN******

data_y = pd.Series(y).drop(index_to_remove, axis=0)

[1;33;44m{'pd'}[0m
    ###########
    create white edge
    white code: pd.Series(y).drop(index_to_remove, axis=0)

self.get_varible_results [('pd', False), ('y', False), ('index_to_remove', False)]
    white varibles: ['y', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data_y']
    assign code: 
data_y = pd.Series(y).drop(index_to_remove, axis=0)

ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

self.get_varible_results [('train_test_split', False), ('data2', False), ('data_y', False)]
    white varibles: ['data2', 'data_y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

ASSIGN******

clf1 = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
clf1 = RandomForestClassifier()

self.get_varible_results ['clf1', 'RandomForestClassifier']
    varibles ['clf1']
ASSIGN******

clf2 = DecisionTreeClassifier()

[1;33;44m{'DecisionTreeClassifier'}[0m
    ###########
    create black edge
    black code: 
clf2 = DecisionTreeClassifier()

self.get_varible_results ['clf2', 'DecisionTreeClassifier']
    varibles ['clf2']
ASSIGN******

clf3 = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
clf3 = GaussianNB()

self.get_varible_results ['clf3', 'GaussianNB']
    varibles ['clf3']
ASSIGN******

clf4 = LogisticRegression()

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
clf4 = LogisticRegression()

self.get_varible_results ['clf4', 'LogisticRegression']
    varibles ['clf4']
ASSIGN******

labels = ['female', 'male']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = ['female', 'male']

self.get_varible_results ['labels']
    varibles ['labels']
EXPR******

pl.title('Confusion matrix of the classifier')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.title('Confusion matrix of the classifier')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.title('Confusion matrix of the classifier')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.xlabel('Predicted')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.xlabel('Predicted')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.xlabel('Predicted')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.ylabel('True')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.ylabel('True')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.ylabel('True')

self.get_varible_results ['pl']
    varibles []
EXPR******

style.use('ggplot')

self.should_create_white_edge True
    ###########
    create white edge
    white code: style.use('ggplot')

self.get_varible_results [('style', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
style.use('ggplot')

self.get_varible_results ['style']
    varibles []
ASSIGN******

data_x = np.array(df[['meanfreq', 'meanfun']])

[1;33;44m{'np'}[0m
    ###########
    create white edge
    white code: np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results [('np', False), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results ['data_x', 'np', 'df']
    varibles ['data_x', 'df']
var data_x
var df
ASSIGN******

kmeans = KMeans(n_clusters=2)

[1;33;44m{'KMeans'}[0m
    ###########
    create black edge
    black code: 
kmeans = KMeans(n_clusters=2)

self.get_varible_results ['kmeans', 'KMeans']
    varibles ['kmeans']
EXPR******

kmeans.fit(data_x)

self.should_create_white_edge True
    ###########
    create white edge
    white code: kmeans.fit(data_x)

self.get_varible_results [('kmeans', False), ('data_x', False)]
    white varibles: ['kmeans', 'data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x000001C239721460>
should_create_black_edge False
ASSIGN******

centroids = kmeans.cluster_centers_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
centroids = kmeans.cluster_centers_

self.get_varible_results ['centroids', 'kmeans']
    varibles ['kmeans']
var kmeans
ASSIGN******

labels = kmeans.labels_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = kmeans.labels_

self.get_varible_results ['labels', 'kmeans']
    varibles ['labels', 'kmeans']
var labels
var kmeans
ASSIGN******

colors = ['g.', 'b.']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['g.', 'b.']

self.get_varible_results ['colors']
    varibles ['colors']
ASSIGN******

model = LogisticRegression(solver='liblinear', random_state=0)

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
model = LogisticRegression(solver='liblinear', random_state=0)

self.get_varible_results ['model', 'LogisticRegression']
    varibles ['model']
EXPR******

model.fit(Xtrain, ytrain)

self.should_create_white_edge True
    ###########
    create white edge
    white code: model.fit(Xtrain, ytrain)

self.get_varible_results [('model', False), ('Xtrain', False), ('ytrain', False)]
    white varibles: ['model', 'Xtrain', 'ytrain']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x000001C239721CA0>
should_create_black_edge False
ASSIGN******

y_pred = model.predict(Xtest)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: model.predict(Xtest)

self.get_varible_results [('model', False), ('Xtest', False)]
    white varibles: ['model', 'Xtest']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
    ###########
    create assign edge
    assign target: ['y_pred']
    assign code: 
y_pred = model.predict(Xtest)

ASSIGN******

score = accuracy_score(ytest, y_pred)

[1;33;44m{'accuracy_score'}[0m
    ###########
    create white edge
    white code: accuracy_score(ytest, y_pred)

self.get_varible_results [('accuracy_score', False), ('ytest', False), ('y_pred', False)]
    white varibles: ['ytest', 'y_pred']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred'])
    ###########
    create assign edge
    assign target: ['score']
    assign code: 
score = accuracy_score(ytest, y_pred)

EXPR******

np.save('HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.should_create_white_edge True
    ###########
    create white edge
    white code: np.save('HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.get_varible_results [('np', False), ('score', False)]
    white varibles: ['score']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred', '-RESULT-accuracy_score', 'score'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x000001C239720040>
should_create_black_edge False
predict model.predict(Xtest)

predict model.predict(Xtest)

self.model_variable ['model', 'model']
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name correlation
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name X
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name male_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name female_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name index_to_remove
self.nodes[node_index].varible_name -RESULT-len
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
model
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name centroids
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
y_pred
self.nodes[node_index].varible_name y_pred
-RESULT-accuracy_score
self.nodes[node_index].varible_name -RESULT-accuracy_score
score
self.nodes[node_index].varible_name score
-RESULT-np.save
self.nodes[node_index].varible_name -RESULT-np.save
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line #print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import matplotlib.pyplot as plt
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import seaborn as sns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.columns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.shape
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.dtypes
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.isnull().values.any()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ['pink','Lightblue']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = df[df.columns[-1]]
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.axis('equal')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line print (df['label'].value_counts())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line correlation =df.corr()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.heatmap(correlation)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line X = df[df.columns[:-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y = df[df.columns[-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import warnings
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line warnings.filterwarnings("ignore")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.ensemble import RandomForestClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line rand_forest = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = rand_forest.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn import metrics, neighbors
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import confusion_matrix
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(confusion_matrix(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.naive_bayes import GaussianNB
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import cross_val_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line CVFirst = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line len(index_to_remove)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = df[df.columns[0:20]].copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
##############################
line_index 45
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy() edge data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

line_id [45]
[45]
[45]
xxxx
line data2.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf1 = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf1.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = clf1.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.tree import DecisionTreeClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf2 = DecisionTreeClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf2.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict = clf2.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf3 = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predd = clf3.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf4 = LogisticRegression()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf4.fit(Xtrain,ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict4 = clf4.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pylab as pl
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = ['female', 'male']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig = plt.figure()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax = fig.add_subplot(111)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cax =ax.matshow(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.title('Confusion matrix of the classifier')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig.colorbar(cax)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_xticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_yticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.xlabel('Predicted')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.ylabel('True')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #pl.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import classification_report
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(classification_report(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.cluster import KMeans
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from matplotlib import style
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line style.use("ggplot")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans = KMeans(n_clusters= 2)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans.fit(data_x)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line centroids = kmeans.cluster_centers_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = kmeans.labels_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ["g.","b."]  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #for i in range(len(data_x)):
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line     
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.ylabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.xlabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print("start running model training........")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y_pred = model.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line score = accuracy_score(ytest, y_pred)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line #print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line #print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line #print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
male      1584
female    1584
Name: label, dtype: int64
male      1584
female    1584
Name: label, dtype: int64
male      1584
female    1584
Name: label, dtype: int64
male      1584
female    1584
Name: label, dtype: int64
male      1584
female    1584
Name: label, dtype: int64
notebook_id datascientist25_gender-recognition-by-voice-using-machine-learning
hi_score: {'accuracy_score': 0.9186991869918699}
hai_score: {'accuracy_score': 0.9857723577235772}
female    1584
male      1584
Name: label, dtype: int64
ASSIGN******

df = pd.read_csv('../input/voicegender/voice.csv')

[1;33;44m{'pd'}[0m
    ###########
    create black edge
    black code: 
df = pd.read_csv('../input/voicegender/voice.csv')

self.get_varible_results ['df', 'pd']
    varibles ['df']
EXPR******

df.columns

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.columns

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.shape

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.shape

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.dtypes

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.dtypes

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.head(3)

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x0000020D589A4670>
should_create_black_edge False
EXPR******

df.isnull().values.any()

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.isnull().values.any()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x0000020D589A4850>
should_create_black_edge False
ASSIGN******

colors = ['pink', 'Lightblue']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['pink', 'Lightblue']

self.get_varible_results ['colors']
    varibles ['colors']
ASSIGN******

data_y = df[df.columns[(- 1)]]

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
data_y = df[df.columns[(- 1)]]

self.get_varible_results ['data_y', 'df', 'df']
    varibles ['df']
EXPR******

print(df['label'].value_counts())

self.should_create_white_edge True
    ###########
    create white edge
    white code: print(df['label'].value_counts())

self.get_varible_results [('print', False), ('df', True)]
should_create_black_edge True
    ###########
    create black edge
    black code: 
print(df['label'].value_counts())

self.get_varible_results ['print', 'df']
    varibles ['df']
ASSIGN******

correlation = df.corr()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df.corr()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df', 'colors', 'data_y'])
    ###########
    create assign edge
    assign target: ['correlation']
    assign code: 
correlation = df.corr()

ASSIGN******

X = df[df.columns[:(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
X = df[df.columns[:(- 1)]].values

self.get_varible_results ['X', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

y = df[df.columns[(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
y = df[df.columns[(- 1)]].values

self.get_varible_results ['y', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(X, y, test_size=0.3)

self.get_varible_results [('train_test_split', False), ('X', False), ('y', False)]
    white varibles: ['X', 'y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

EXPR******

warnings.filterwarnings('ignore')

self.should_create_white_edge True
    ###########
    create white edge
    white code: warnings.filterwarnings('ignore')

self.get_varible_results [('warnings', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
warnings.filterwarnings('ignore')

self.get_varible_results ['warnings']
    varibles []
ASSIGN******

rand_forest = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
rand_forest = RandomForestClassifier()

self.get_varible_results ['rand_forest', 'RandomForestClassifier']
    varibles ['rand_forest']
ASSIGN******

CVFirst = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
CVFirst = GaussianNB()

self.get_varible_results ['CVFirst', 'GaussianNB']
    varibles ['CVFirst']
ASSIGN******

male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

self.get_varible_results ['male_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

self.get_varible_results ['female_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

[1;33;44m{'list'}[0m
    ###########
    create black edge
    black code: 
index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

self.get_varible_results ['index_to_remove', 'list', 'male_funFreq_outlier_index', 'list', 'female_funFreq_outlier_index']
    varibles ['male_funFreq_outlier_index', 'female_funFreq_outlier_index']
var male_funFreq_outlier_index
var female_funFreq_outlier_index
EXPR******

len(index_to_remove)

self.should_create_white_edge True
    ###########
    create white edge
    white code: len(index_to_remove)

self.get_varible_results [('len', False), ('index_to_remove', False)]
    white varibles: ['index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x0000020D589A48B0>
should_create_black_edge False
ASSIGN******

data_x = df[df.columns[0:20]].copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df[df.columns[0:20]].copy()

self.get_varible_results [('df', True), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = df[df.columns[0:20]].copy()

self.get_varible_results ['data_x', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

self.get_varible_results [('data_x', False)]
    white varibles: ['data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

EXPR******

data2.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: data2.head(3)

self.get_varible_results [('data2', False)]
    white varibles: ['data2']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x0000020D46FEE310>
should_create_black_edge False
ASSIGN******

data2 = data2.drop(index_to_remove, axis=0)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data2.drop(index_to_remove, axis=0)

self.get_varible_results [('data2', False), ('index_to_remove', False)]
    white varibles: ['data2', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data2.drop(index_to_remove, axis=0)

ASSIGN******

data_y = pd.Series(y).drop(index_to_remove, axis=0)

[1;33;44m{'pd'}[0m
    ###########
    create white edge
    white code: pd.Series(y).drop(index_to_remove, axis=0)

self.get_varible_results [('pd', False), ('y', False), ('index_to_remove', False)]
    white varibles: ['y', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data_y']
    assign code: 
data_y = pd.Series(y).drop(index_to_remove, axis=0)

ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

self.get_varible_results [('train_test_split', False), ('data2', False), ('data_y', False)]
    white varibles: ['data2', 'data_y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

ASSIGN******

clf1 = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
clf1 = RandomForestClassifier()

self.get_varible_results ['clf1', 'RandomForestClassifier']
    varibles ['clf1']
ASSIGN******

clf2 = DecisionTreeClassifier()

[1;33;44m{'DecisionTreeClassifier'}[0m
    ###########
    create black edge
    black code: 
clf2 = DecisionTreeClassifier()

self.get_varible_results ['clf2', 'DecisionTreeClassifier']
    varibles ['clf2']
ASSIGN******

clf3 = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
clf3 = GaussianNB()

self.get_varible_results ['clf3', 'GaussianNB']
    varibles ['clf3']
ASSIGN******

clf4 = LogisticRegression()

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
clf4 = LogisticRegression()

self.get_varible_results ['clf4', 'LogisticRegression']
    varibles ['clf4']
ASSIGN******

labels = ['female', 'male']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = ['female', 'male']

self.get_varible_results ['labels']
    varibles ['labels']
EXPR******

pl.title('Confusion matrix of the classifier')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.title('Confusion matrix of the classifier')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.title('Confusion matrix of the classifier')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.xlabel('Predicted')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.xlabel('Predicted')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.xlabel('Predicted')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.ylabel('True')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.ylabel('True')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.ylabel('True')

self.get_varible_results ['pl']
    varibles []
EXPR******

style.use('ggplot')

self.should_create_white_edge True
    ###########
    create white edge
    white code: style.use('ggplot')

self.get_varible_results [('style', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
style.use('ggplot')

self.get_varible_results ['style']
    varibles []
ASSIGN******

data_x = np.array(df[['meanfreq', 'meanfun']])

[1;33;44m{'np'}[0m
    ###########
    create white edge
    white code: np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results [('np', False), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results ['data_x', 'np', 'df']
    varibles ['data_x', 'df']
var data_x
var df
ASSIGN******

kmeans = KMeans(n_clusters=2)

[1;33;44m{'KMeans'}[0m
    ###########
    create black edge
    black code: 
kmeans = KMeans(n_clusters=2)

self.get_varible_results ['kmeans', 'KMeans']
    varibles ['kmeans']
EXPR******

kmeans.fit(data_x)

self.should_create_white_edge True
    ###########
    create white edge
    white code: kmeans.fit(data_x)

self.get_varible_results [('kmeans', False), ('data_x', False)]
    white varibles: ['kmeans', 'data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x0000020D46FEE460>
should_create_black_edge False
ASSIGN******

centroids = kmeans.cluster_centers_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
centroids = kmeans.cluster_centers_

self.get_varible_results ['centroids', 'kmeans']
    varibles ['kmeans']
var kmeans
ASSIGN******

labels = kmeans.labels_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = kmeans.labels_

self.get_varible_results ['labels', 'kmeans']
    varibles ['labels', 'kmeans']
var labels
var kmeans
ASSIGN******

colors = ['g.', 'b.']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['g.', 'b.']

self.get_varible_results ['colors']
    varibles ['colors']
ASSIGN******

model = LogisticRegression(solver='liblinear', random_state=0)

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
model = LogisticRegression(solver='liblinear', random_state=0)

self.get_varible_results ['model', 'LogisticRegression']
    varibles ['model']
EXPR******

model.fit(Xtrain, ytrain)

self.should_create_white_edge True
    ###########
    create white edge
    white code: model.fit(Xtrain, ytrain)

self.get_varible_results [('model', False), ('Xtrain', False), ('ytrain', False)]
    white varibles: ['model', 'Xtrain', 'ytrain']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x0000020D46FEECA0>
should_create_black_edge False
ASSIGN******

y_pred = model.predict(Xtest)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: model.predict(Xtest)

self.get_varible_results [('model', False), ('Xtest', False)]
    white varibles: ['model', 'Xtest']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
    ###########
    create assign edge
    assign target: ['y_pred']
    assign code: 
y_pred = model.predict(Xtest)

ASSIGN******

score = accuracy_score(ytest, y_pred)

[1;33;44m{'accuracy_score'}[0m
    ###########
    create white edge
    white code: accuracy_score(ytest, y_pred)

self.get_varible_results [('accuracy_score', False), ('ytest', False), ('y_pred', False)]
    white varibles: ['ytest', 'y_pred']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred'])
    ###########
    create assign edge
    assign target: ['score']
    assign code: 
score = accuracy_score(ytest, y_pred)

EXPR******

np.save('HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.should_create_white_edge True
    ###########
    create white edge
    white code: np.save('HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.get_varible_results [('np', False), ('score', False)]
    white varibles: ['score']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred', '-RESULT-accuracy_score', 'score'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x0000020D46FE6040>
should_create_black_edge False
predict model.predict(Xtest)

predict model.predict(Xtest)

self.model_variable ['model', 'model']
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name correlation
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name X
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name male_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name female_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name index_to_remove
self.nodes[node_index].varible_name -RESULT-len
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
model
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name centroids
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
y_pred
self.nodes[node_index].varible_name y_pred
-RESULT-accuracy_score
self.nodes[node_index].varible_name -RESULT-accuracy_score
score
self.nodes[node_index].varible_name score
-RESULT-np.save
self.nodes[node_index].varible_name -RESULT-np.save
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line #print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import matplotlib.pyplot as plt
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import seaborn as sns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.columns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.shape
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.dtypes
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.isnull().values.any()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ['pink','Lightblue']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = df[df.columns[-1]]
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.axis('equal')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line print (df['label'].value_counts())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line correlation =df.corr()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.heatmap(correlation)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line X = df[df.columns[:-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y = df[df.columns[-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import warnings
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line warnings.filterwarnings("ignore")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.ensemble import RandomForestClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line rand_forest = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = rand_forest.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn import metrics, neighbors
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import confusion_matrix
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(confusion_matrix(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.naive_bayes import GaussianNB
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import cross_val_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line CVFirst = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line len(index_to_remove)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = df[df.columns[0:20]].copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
##############################
line_index 45
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy() edge data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

line_id [45]
[45]
[45]
xxxx
line data2.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf1 = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf1.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = clf1.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.tree import DecisionTreeClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf2 = DecisionTreeClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf2.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict = clf2.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf3 = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predd = clf3.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf4 = LogisticRegression()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf4.fit(Xtrain,ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict4 = clf4.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pylab as pl
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = ['female', 'male']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig = plt.figure()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax = fig.add_subplot(111)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cax =ax.matshow(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.title('Confusion matrix of the classifier')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig.colorbar(cax)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_xticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_yticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.xlabel('Predicted')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.ylabel('True')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #pl.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import classification_report
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(classification_report(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.cluster import KMeans
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from matplotlib import style
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line style.use("ggplot")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans = KMeans(n_clusters= 2)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans.fit(data_x)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line centroids = kmeans.cluster_centers_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = kmeans.labels_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ["g.","b."]  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #for i in range(len(data_x)):
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line     
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.ylabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.xlabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print("start running model training........")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y_pred = model.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line score = accuracy_score(ytest, y_pred)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line #print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line #print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line #print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
female    1584
male      1584
Name: label, dtype: int64
female    1584
male      1584
Name: label, dtype: int64
female    1584
male      1584
Name: label, dtype: int64
female    1584
male      1584
Name: label, dtype: int64
female    1584
male      1584
Name: label, dtype: int64
notebook_id datascientist25_gender-recognition-by-voice-using-machine-learning
hi_score: {'accuracy_score': 0.9186991869918699}
hai_score: {'accuracy_score': 0.9857723577235772}
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
female    1584
male      1584
Name: label, dtype: int64
[0;32;40msucceed[0m
ASSIGN******

df = pd.read_csv('../input/voicegender/voice.csv')

[1;33;44m{'pd'}[0m
    ###########
    create black edge
    black code: 
df = pd.read_csv('../input/voicegender/voice.csv')

self.get_varible_results ['df', 'pd']
    varibles ['df']
EXPR******

df.columns

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.columns

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.shape

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.shape

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.dtypes

self.should_create_white_edge False
should_create_black_edge True
    ###########
    create black edge
    black code: 
df.dtypes

self.get_varible_results ['df']
    varibles ['df']
EXPR******

df.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.head(3)

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x00000229B03E5AC0>
should_create_black_edge False
EXPR******

df.isnull().values.any()

self.should_create_white_edge True
    ###########
    create white edge
    white code: df.isnull().values.any()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x00000229B03E5C70>
should_create_black_edge False
ASSIGN******

colors = ['pink', 'Lightblue']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['pink', 'Lightblue']

self.get_varible_results ['colors']
    varibles ['colors']
ASSIGN******

data_y = df[df.columns[(- 1)]]

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
data_y = df[df.columns[(- 1)]]

self.get_varible_results ['data_y', 'df', 'df']
    varibles ['df']
EXPR******

print(df['label'].value_counts())

self.should_create_white_edge True
    ###########
    create white edge
    white code: print(df['label'].value_counts())

self.get_varible_results [('print', False), ('df', True)]
should_create_black_edge True
    ###########
    create black edge
    black code: 
print(df['label'].value_counts())

self.get_varible_results ['print', 'df']
    varibles ['df']
ASSIGN******

correlation = df.corr()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df.corr()

self.get_varible_results [('df', False)]
    white varibles: ['df']
dict_keys(['df', 'colors', 'data_y'])
    ###########
    create assign edge
    assign target: ['correlation']
    assign code: 
correlation = df.corr()

ASSIGN******

X = df[df.columns[:(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
X = df[df.columns[:(- 1)]].values

self.get_varible_results ['X', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

y = df[df.columns[(- 1)]].values

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
y = df[df.columns[(- 1)]].values

self.get_varible_results ['y', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(X, y, test_size=0.3)

self.get_varible_results [('train_test_split', False), ('X', False), ('y', False)]
    white varibles: ['X', 'y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.3)

EXPR******

warnings.filterwarnings('ignore')

self.should_create_white_edge True
    ###########
    create white edge
    white code: warnings.filterwarnings('ignore')

self.get_varible_results [('warnings', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
warnings.filterwarnings('ignore')

self.get_varible_results ['warnings']
    varibles []
ASSIGN******

rand_forest = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
rand_forest = RandomForestClassifier()

self.get_varible_results ['rand_forest', 'RandomForestClassifier']
    varibles ['rand_forest']
ASSIGN******

CVFirst = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
CVFirst = GaussianNB()

self.get_varible_results ['CVFirst', 'GaussianNB']
    varibles ['CVFirst']
ASSIGN******

male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
male_funFreq_outlier_index = df[(((df['meanfun'] < 0.085) | (df['meanfun'] > 0.18)) & (df['label'] == 'male'))].index

self.get_varible_results ['male_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
female_funFreq_outlier_index = df[(((df['meanfun'] < 0.165) | (df['meanfun'] > 0.255)) & (df['label'] == 'female'))].index

self.get_varible_results ['female_funFreq_outlier_index', 'df', 'df', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

[1;33;44m{'list'}[0m
    ###########
    create black edge
    black code: 
index_to_remove = (list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index))

self.get_varible_results ['index_to_remove', 'list', 'male_funFreq_outlier_index', 'list', 'female_funFreq_outlier_index']
    varibles ['male_funFreq_outlier_index', 'female_funFreq_outlier_index']
var male_funFreq_outlier_index
var female_funFreq_outlier_index
EXPR******

len(index_to_remove)

self.should_create_white_edge True
    ###########
    create white edge
    white code: len(index_to_remove)

self.get_varible_results [('len', False), ('index_to_remove', False)]
    white varibles: ['index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x00000229B03E5CD0>
should_create_black_edge False
ASSIGN******

data_x = df[df.columns[0:20]].copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: df[df.columns[0:20]].copy()

self.get_varible_results [('df', True), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = df[df.columns[0:20]].copy()

self.get_varible_results ['data_x', 'df', 'df']
    varibles ['df']
var df
ASSIGN******

data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

self.get_varible_results [('data_x', False)]
    white varibles: ['data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

EXPR******

data2.head(3)

self.should_create_white_edge True
    ###########
    create white edge
    white code: data2.head(3)

self.get_varible_results [('data2', False)]
    white varibles: ['data2']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x00000229B03F45E0>
should_create_black_edge False
ASSIGN******

data2 = data2.drop(index_to_remove, axis=0)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: data2.drop(index_to_remove, axis=0)

self.get_varible_results [('data2', False), ('index_to_remove', False)]
    white varibles: ['data2', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data2']
    assign code: 
data2 = data2.drop(index_to_remove, axis=0)

ASSIGN******

data_y = pd.Series(y).drop(index_to_remove, axis=0)

[1;33;44m{'pd'}[0m
    ###########
    create white edge
    white code: pd.Series(y).drop(index_to_remove, axis=0)

self.get_varible_results [('pd', False), ('y', False), ('index_to_remove', False)]
    white varibles: ['y', 'index_to_remove']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['data_y']
    assign code: 
data_y = pd.Series(y).drop(index_to_remove, axis=0)

ASSIGN******

(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

[1;33;44m{'train_test_split'}[0m
    ###########
    create white edge
    white code: train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

self.get_varible_results [('train_test_split', False), ('data2', False), ('data_y', False)]
    white varibles: ['data2', 'data_y']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2'])
    ###########
    create assign edge
    assign target: ['Xtrain', 'Xtest', 'ytrain', 'ytest']
    assign code: 
(Xtrain, Xtest, ytrain, ytest) = train_test_split(data2, data_y, train_size=0.8, test_size=(1 - 0.8), random_state=0)

ASSIGN******

clf1 = RandomForestClassifier()

[1;33;44m{'RandomForestClassifier'}[0m
    ###########
    create black edge
    black code: 
clf1 = RandomForestClassifier()

self.get_varible_results ['clf1', 'RandomForestClassifier']
    varibles ['clf1']
ASSIGN******

clf2 = DecisionTreeClassifier()

[1;33;44m{'DecisionTreeClassifier'}[0m
    ###########
    create black edge
    black code: 
clf2 = DecisionTreeClassifier()

self.get_varible_results ['clf2', 'DecisionTreeClassifier']
    varibles ['clf2']
ASSIGN******

clf3 = GaussianNB()

[1;33;44m{'GaussianNB'}[0m
    ###########
    create black edge
    black code: 
clf3 = GaussianNB()

self.get_varible_results ['clf3', 'GaussianNB']
    varibles ['clf3']
ASSIGN******

clf4 = LogisticRegression()

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
clf4 = LogisticRegression()

self.get_varible_results ['clf4', 'LogisticRegression']
    varibles ['clf4']
ASSIGN******

labels = ['female', 'male']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = ['female', 'male']

self.get_varible_results ['labels']
    varibles ['labels']
EXPR******

pl.title('Confusion matrix of the classifier')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.title('Confusion matrix of the classifier')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.title('Confusion matrix of the classifier')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.xlabel('Predicted')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.xlabel('Predicted')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.xlabel('Predicted')

self.get_varible_results ['pl']
    varibles []
EXPR******

pl.ylabel('True')

self.should_create_white_edge True
    ###########
    create white edge
    white code: pl.ylabel('True')

self.get_varible_results [('pl', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
pl.ylabel('True')

self.get_varible_results ['pl']
    varibles []
EXPR******

style.use('ggplot')

self.should_create_white_edge True
    ###########
    create white edge
    white code: style.use('ggplot')

self.get_varible_results [('style', False)]
    white varibles: []
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels'])
should_create_black_edge True
    ###########
    create black edge
    black code: 
style.use('ggplot')

self.get_varible_results ['style']
    varibles []
ASSIGN******

data_x = np.array(df[['meanfreq', 'meanfun']])

[1;33;44m{'np'}[0m
    ###########
    create white edge
    white code: np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results [('np', False), ('df', True)]
    ###########
    create black edge
    black code: 
data_x = np.array(df[['meanfreq', 'meanfun']])

self.get_varible_results ['data_x', 'np', 'df']
    varibles ['data_x', 'df']
var data_x
var df
ASSIGN******

kmeans = KMeans(n_clusters=2)

[1;33;44m{'KMeans'}[0m
    ###########
    create black edge
    black code: 
kmeans = KMeans(n_clusters=2)

self.get_varible_results ['kmeans', 'KMeans']
    varibles ['kmeans']
EXPR******

kmeans.fit(data_x)

self.should_create_white_edge True
    ###########
    create white edge
    white code: kmeans.fit(data_x)

self.get_varible_results [('kmeans', False), ('data_x', False)]
    white varibles: ['kmeans', 'data_x']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x00000229B03F4880>
should_create_black_edge False
ASSIGN******

centroids = kmeans.cluster_centers_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
centroids = kmeans.cluster_centers_

self.get_varible_results ['centroids', 'kmeans']
    varibles ['kmeans']
var kmeans
ASSIGN******

labels = kmeans.labels_

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
labels = kmeans.labels_

self.get_varible_results ['labels', 'kmeans']
    varibles ['labels', 'kmeans']
var labels
var kmeans
ASSIGN******

colors = ['g.', 'b.']

[1;33;44mset()[0m
    ###########
    create black edge
    black code: 
colors = ['g.', 'b.']

self.get_varible_results ['colors']
    varibles ['colors']
ASSIGN******

model = LogisticRegression(solver='liblinear', random_state=0)

[1;33;44m{'LogisticRegression'}[0m
    ###########
    create black edge
    black code: 
model = LogisticRegression(solver='liblinear', random_state=0)

self.get_varible_results ['model', 'LogisticRegression']
    varibles ['model']
EXPR******

model.fit(Xtrain, ytrain)

self.should_create_white_edge True
    ###########
    create white edge
    white code: model.fit(Xtrain, ytrain)

self.get_varible_results [('model', False), ('Xtrain', False), ('ytrain', False)]
    white varibles: ['model', 'Xtrain', 'ytrain']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x00000229B03FE130>
should_create_black_edge False
ASSIGN******

y_pred = model.predict(Xtest)

[1;33;44mset()[0m
    ###########
    create white edge
    white code: model.predict(Xtest)

self.get_varible_results [('model', False), ('Xtest', False)]
    white varibles: ['model', 'Xtest']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model'])
    ###########
    create assign edge
    assign target: ['y_pred']
    assign code: 
y_pred = model.predict(Xtest)

ASSIGN******

score = accuracy_score(ytest, y_pred)

[1;33;44m{'accuracy_score'}[0m
    ###########
    create white edge
    white code: accuracy_score(ytest, y_pred)

self.get_varible_results [('accuracy_score', False), ('ytest', False), ('y_pred', False)]
    white varibles: ['ytest', 'y_pred']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred'])
    ###########
    create assign edge
    assign target: ['score']
    assign code: 
score = accuracy_score(ytest, y_pred)

EXPR******

np.save('HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.should_create_white_edge True
    ###########
    create white edge
    white code: np.save('HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy', {'accuracy_score': score})

self.get_varible_results [('np', False), ('score', False)]
    white varibles: ['score']
dict_keys(['df', 'colors', 'data_y', 'correlation', 'X', 'y', '-RESULT-train_test_split', 'Xtrain', 'Xtest', 'ytrain', 'ytest', 'rand_forest', 'CVFirst', 'male_funFreq_outlier_index', 'female_funFreq_outlier_index', 'index_to_remove', '-RESULT-len', 'data_x', 'data2', 'clf1', 'clf2', 'clf3', 'clf4', 'labels', 'kmeans', 'centroids', 'model', 'y_pred', '-RESULT-accuracy_score', 'score'])
Expr white_edge_node <HAIPipeGen.NotebookGraph.Node object at 0x00000229B03FE490>
should_create_black_edge False
predict model.predict(Xtest)

predict model.predict(Xtest)

self.model_variable ['model', 'model']
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name df
self.nodes[node_index].varible_name correlation
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name X
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name rand_forest
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name CVFirst
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name male_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name female_funFreq_outlier_index
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name index_to_remove
self.nodes[node_index].varible_name -RESULT-len
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name data2
self.nodes[node_index].varible_name y
self.nodes[node_index].varible_name data_y
self.nodes[node_index].varible_name -RESULT-train_test_split
self.nodes[node_index].varible_name Xtrain
self.nodes[node_index].varible_name Xtest
model
self.nodes[node_index].varible_name ytrain
self.nodes[node_index].varible_name ytest
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf1
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf2
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf3
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name clf4
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name data_x
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name kmeans
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name centroids
self.nodes[node_index].varible_name -Blank-Result-
self.nodes[node_index].varible_name labels
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name colors
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
model
self.nodes[node_index].varible_name model
y_pred
self.nodes[node_index].varible_name y_pred
-RESULT-accuracy_score
self.nodes[node_index].varible_name -RESULT-accuracy_score
score
self.nodes[node_index].varible_name score
-RESULT-np.save
self.nodes[node_index].varible_name -RESULT-np.save
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line #print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.columns
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.shape
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.dtypes
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import warnings
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
pd.Series(y).drop(index_to_remove, axis=0)
##############################
line_index 48
line data_y = pd.Series(y).drop(index_to_remove,axis=0) edge pd.Series(y).drop(index_to_remove, axis=0)

line_id [48]
[48]
[48]
xxxx
line from sklearn.model_selection import train_test_split
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #pl.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line     
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #plt.show()
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line #print("start running model training........")
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line 
pd.Series(y).drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import matplotlib.pyplot as plt
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import seaborn as sns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.columns
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.shape
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.dtypes
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line df.isnull().values.any()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ['pink','Lightblue']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = df[df.columns[-1]]
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.axis('equal')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line print (df['label'].value_counts())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line correlation =df.corr()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.heatmap(correlation)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line X = df[df.columns[:-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y = df[df.columns[-1]].values
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import warnings
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line warnings.filterwarnings("ignore")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.ensemble import RandomForestClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line rand_forest = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = rand_forest.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn import metrics, neighbors
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import confusion_matrix
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(confusion_matrix(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.naive_bayes import GaussianNB
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import cross_val_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line CVFirst = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line len(index_to_remove)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = df[df.columns[0:20]].copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
##############################
line_index 45
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy() edge data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()

line_id [45]
[45]
[45]
xxxx
line data2.head(3)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.model_selection import train_test_split
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf1 = RandomForestClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf1.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_pred = clf1.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.tree import DecisionTreeClassifier
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf2 = DecisionTreeClassifier()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf2.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict = clf2.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf3 = GaussianNB()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predd = clf3.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line clf4 = LogisticRegression()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #clf4.fit(Xtrain,ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #y_predict4 = clf4.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pylab as pl
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = ['female', 'male']
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig = plt.figure()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax = fig.add_subplot(111)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #cax =ax.matshow(cm)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.title('Confusion matrix of the classifier')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #fig.colorbar(cax)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_xticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #ax.set_yticklabels([''] + labels)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.xlabel('Predicted')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line pl.ylabel('True')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #pl.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import classification_report
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print(classification_report(ytest, y_pred))
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.cluster import KMeans
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from matplotlib import style
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line style.use("ggplot")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans = KMeans(n_clusters= 2)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line kmeans.fit(data_x)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line centroids = kmeans.cluster_centers_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line labels = kmeans.labels_
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line colors = ["g.","b."]  
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #for i in range(len(data_x)):
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line     
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.ylabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.xlabel('meanfun')
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #plt.show()
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import pandas as pd
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.metrics import accuracy_score
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line #print("start running model training........")
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line model.fit(Xtrain, ytrain)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line y_pred = model.predict(Xtest)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line score = accuracy_score(ytest, y_pred)
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line 
data_x.drop(['kurt', 'centroid', 'dfrange'], axis=1).copy()
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line import matplotlib.pyplot as plt
data2.drop(index_to_remove, axis=0)
xxxx
line import seaborn as sns
data2.drop(index_to_remove, axis=0)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
data2.drop(index_to_remove, axis=0)
xxxx
line df.columns
data2.drop(index_to_remove, axis=0)
xxxx
line df.shape
data2.drop(index_to_remove, axis=0)
xxxx
line df.dtypes
data2.drop(index_to_remove, axis=0)
xxxx
line df.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line df.isnull().values.any()
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ['pink','Lightblue']
data2.drop(index_to_remove, axis=0)
xxxx
line data_y = df[df.columns[-1]]
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.axis('equal')
data2.drop(index_to_remove, axis=0)
xxxx
line print (df['label'].value_counts())
data2.drop(index_to_remove, axis=0)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
data2.drop(index_to_remove, axis=0)
xxxx
line correlation =df.corr()
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.heatmap(correlation)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line X = df[df.columns[:-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line y = df[df.columns[-1]].values
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
data2.drop(index_to_remove, axis=0)
xxxx
line import warnings
data2.drop(index_to_remove, axis=0)
xxxx
line warnings.filterwarnings("ignore")
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.ensemble import RandomForestClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line rand_forest = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = rand_forest.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn import metrics, neighbors
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import confusion_matrix
data2.drop(index_to_remove, axis=0)
xxxx
line #print(confusion_matrix(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.naive_bayes import GaussianNB
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import cross_val_score
data2.drop(index_to_remove, axis=0)
xxxx
line CVFirst = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
data2.drop(index_to_remove, axis=0)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
data2.drop(index_to_remove, axis=0)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
data2.drop(index_to_remove, axis=0)
xxxx
line len(index_to_remove)
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = df[df.columns[0:20]].copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
data2.drop(index_to_remove, axis=0)
xxxx
line data2.head(3)
data2.drop(index_to_remove, axis=0)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
##############################
line_index 47
line data2 = data2.drop(index_to_remove,axis=0) edge data2.drop(index_to_remove, axis=0)

line_id [47]
[47]
[47]
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.model_selection import train_test_split
data2.drop(index_to_remove, axis=0)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line clf1 = RandomForestClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf1.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_pred = clf1.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.tree import DecisionTreeClassifier
data2.drop(index_to_remove, axis=0)
xxxx
line clf2 = DecisionTreeClassifier()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf2.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict = clf2.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
data2.drop(index_to_remove, axis=0)
xxxx
line clf3 = GaussianNB()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predd = clf3.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line clf4 = LogisticRegression()
data2.drop(index_to_remove, axis=0)
xxxx
line #clf4.fit(Xtrain,ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line #y_predict4 = clf4.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
data2.drop(index_to_remove, axis=0)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
data2.drop(index_to_remove, axis=0)
xxxx
line import pylab as pl
data2.drop(index_to_remove, axis=0)
xxxx
line labels = ['female', 'male']
data2.drop(index_to_remove, axis=0)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
data2.drop(index_to_remove, axis=0)
xxxx
line #print(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line #fig = plt.figure()
data2.drop(index_to_remove, axis=0)
xxxx
line #ax = fig.add_subplot(111)
data2.drop(index_to_remove, axis=0)
xxxx
line #cax =ax.matshow(cm)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.title('Confusion matrix of the classifier')
data2.drop(index_to_remove, axis=0)
xxxx
line #fig.colorbar(cax)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_xticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line #ax.set_yticklabels([''] + labels)
data2.drop(index_to_remove, axis=0)
xxxx
line pl.xlabel('Predicted')
data2.drop(index_to_remove, axis=0)
xxxx
line pl.ylabel('True')
data2.drop(index_to_remove, axis=0)
xxxx
line #pl.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import classification_report
data2.drop(index_to_remove, axis=0)
xxxx
line #print(classification_report(ytest, y_pred))
data2.drop(index_to_remove, axis=0)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.cluster import KMeans
data2.drop(index_to_remove, axis=0)
xxxx
line from matplotlib import style
data2.drop(index_to_remove, axis=0)
xxxx
line style.use("ggplot")
data2.drop(index_to_remove, axis=0)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans = KMeans(n_clusters= 2)
data2.drop(index_to_remove, axis=0)
xxxx
line kmeans.fit(data_x)
data2.drop(index_to_remove, axis=0)
xxxx
line centroids = kmeans.cluster_centers_
data2.drop(index_to_remove, axis=0)
xxxx
line labels = kmeans.labels_
data2.drop(index_to_remove, axis=0)
xxxx
line colors = ["g.","b."]  
data2.drop(index_to_remove, axis=0)
xxxx
line #for i in range(len(data_x)):
data2.drop(index_to_remove, axis=0)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line     
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.ylabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.xlabel('meanfun')
data2.drop(index_to_remove, axis=0)
xxxx
line #plt.show()
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import pandas as pd
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.metrics import accuracy_score
data2.drop(index_to_remove, axis=0)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
data2.drop(index_to_remove, axis=0)
xxxx
line #print("start running model training........")
data2.drop(index_to_remove, axis=0)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
data2.drop(index_to_remove, axis=0)
xxxx
line model.fit(Xtrain, ytrain)
data2.drop(index_to_remove, axis=0)
xxxx
line y_pred = model.predict(Xtest)
data2.drop(index_to_remove, axis=0)
xxxx
line score = accuracy_score(ytest, y_pred)
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
data2.drop(index_to_remove, axis=0)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
data2.drop(index_to_remove, axis=0)
xxxx
line 
data2.drop(index_to_remove, axis=0)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line #print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line import matplotlib.pyplot as plt
kmeans.fit(data_x)
xxxx
line import seaborn as sns
kmeans.fit(data_x)
xxxx
line df = pd.read_csv('../input/voicegender/voice.csv')
kmeans.fit(data_x)
xxxx
line df.columns
kmeans.fit(data_x)
xxxx
line df.shape
kmeans.fit(data_x)
xxxx
line df.dtypes
kmeans.fit(data_x)
xxxx
line df.head(3)
kmeans.fit(data_x)
xxxx
line df.isnull().values.any()
kmeans.fit(data_x)
xxxx
line colors = ['pink','Lightblue']
kmeans.fit(data_x)
xxxx
line data_y = df[df.columns[-1]]
kmeans.fit(data_x)
xxxx
line #plt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])
kmeans.fit(data_x)
xxxx
line #plt.axis('equal')
kmeans.fit(data_x)
xxxx
line print (df['label'].value_counts())
kmeans.fit(data_x)
xxxx
line #df.boxplot(column = 'meanfreq',by='label',grid=False)
kmeans.fit(data_x)
xxxx
line correlation =df.corr()
kmeans.fit(data_x)
xxxx
line #sns.heatmap(correlation)
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line X = df[df.columns[:-1]].values
kmeans.fit(data_x)
xxxx
line y = df[df.columns[-1]].values
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)
kmeans.fit(data_x)
xxxx
line import warnings
kmeans.fit(data_x)
xxxx
line warnings.filterwarnings("ignore")
kmeans.fit(data_x)
xxxx
line from sklearn.ensemble import RandomForestClassifier
kmeans.fit(data_x)
xxxx
line rand_forest = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #rand_forest.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = rand_forest.predict(Xtest)
kmeans.fit(data_x)
xxxx
line from sklearn import metrics, neighbors
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import confusion_matrix
kmeans.fit(data_x)
xxxx
line #print(confusion_matrix(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.naive_bayes import GaussianNB
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import cross_val_score
kmeans.fit(data_x)
xxxx
line CVFirst = GaussianNB()
kmeans.fit(data_x)
xxxx
line #CVFirst = CVFirst.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &                               (df['label'] == 'male')].index
kmeans.fit(data_x)
xxxx
line female_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &                                 (df['label'] == 'female')].index
kmeans.fit(data_x)
xxxx
line index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)
kmeans.fit(data_x)
xxxx
line len(index_to_remove)
kmeans.fit(data_x)
xxxx
line data_x = df[df.columns[0:20]].copy()
kmeans.fit(data_x)
xxxx
line data2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()
kmeans.fit(data_x)
xxxx
line data2.head(3)
kmeans.fit(data_x)
xxxx
line data2 = data2.drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line data_y = pd.Series(y).drop(index_to_remove,axis=0)
kmeans.fit(data_x)
xxxx
line from sklearn.model_selection import train_test_split
kmeans.fit(data_x)
xxxx
line Xtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, train_size=0.8, test_size=1-0.8, random_state=0)
kmeans.fit(data_x)
xxxx
line clf1 = RandomForestClassifier()
kmeans.fit(data_x)
xxxx
line #clf1.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_pred = clf1.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line from sklearn.tree import DecisionTreeClassifier
kmeans.fit(data_x)
xxxx
line clf2 = DecisionTreeClassifier()
kmeans.fit(data_x)
xxxx
line #clf2.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict = clf2.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest, y_predict))
kmeans.fit(data_x)
xxxx
line clf3 = GaussianNB()
kmeans.fit(data_x)
xxxx
line #clf3 = clf3.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line #y_predd = clf3.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predd))
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model import LogisticRegression
kmeans.fit(data_x)
xxxx
line clf4 = LogisticRegression()
kmeans.fit(data_x)
xxxx
line #clf4.fit(Xtrain,ytrain)
kmeans.fit(data_x)
xxxx
line #y_predict4 = clf4.predict(Xtest)
kmeans.fit(data_x)
xxxx
line #print(metrics.accuracy_score(ytest,y_predict4))
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-flod cross validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line #test_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')
kmeans.fit(data_x)
xxxx
line #print('Accuracy obtained from 10-fold validation is:',test_result.mean())
kmeans.fit(data_x)
xxxx
line import pylab as pl
kmeans.fit(data_x)
xxxx
line labels = ['female', 'male']
kmeans.fit(data_x)
xxxx
line #cm = confusion_matrix(ytest,y_pred,labels)  
kmeans.fit(data_x)
xxxx
line #print(cm)
kmeans.fit(data_x)
xxxx
line #fig = plt.figure()
kmeans.fit(data_x)
xxxx
line #ax = fig.add_subplot(111)
kmeans.fit(data_x)
xxxx
line #cax =ax.matshow(cm)
kmeans.fit(data_x)
xxxx
line pl.title('Confusion matrix of the classifier')
kmeans.fit(data_x)
xxxx
line #fig.colorbar(cax)
kmeans.fit(data_x)
xxxx
line #ax.set_xticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line #ax.set_yticklabels([''] + labels)
kmeans.fit(data_x)
xxxx
line pl.xlabel('Predicted')
kmeans.fit(data_x)
xxxx
line pl.ylabel('True')
kmeans.fit(data_x)
xxxx
line #pl.show()
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import classification_report
kmeans.fit(data_x)
xxxx
line #print(classification_report(ytest, y_pred))
kmeans.fit(data_x)
xxxx
line #sns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,"meanfun").add_legend()
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line from sklearn.cluster import KMeans
kmeans.fit(data_x)
xxxx
line from matplotlib import style
kmeans.fit(data_x)
xxxx
line style.use("ggplot")
kmeans.fit(data_x)
xxxx
line data_x = np.array(df[['meanfreq','meanfun']])
kmeans.fit(data_x)
xxxx
line kmeans = KMeans(n_clusters= 2)
kmeans.fit(data_x)
xxxx
line kmeans.fit(data_x)
kmeans.fit(data_x)
##############################
line_index 96
line kmeans.fit(data_x) edge kmeans.fit(data_x)

line_id [96]
[96]
[96]
xxxx
line centroids = kmeans.cluster_centers_
kmeans.fit(data_x)
xxxx
line labels = kmeans.labels_
kmeans.fit(data_x)
xxxx
line colors = ["g.","b."]  
kmeans.fit(data_x)
xxxx
line #for i in range(len(data_x)):
kmeans.fit(data_x)
xxxx
line #    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)
kmeans.fit(data_x)
xxxx
line     
kmeans.fit(data_x)
xxxx
line #plt.scatter(centroids[:,0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)
kmeans.fit(data_x)
xxxx
line #plt.ylabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.xlabel('meanfun')
kmeans.fit(data_x)
xxxx
line #plt.show()
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
xxxx
line import pandas as pd
kmeans.fit(data_x)
xxxx
line from sklearn.metrics import accuracy_score
kmeans.fit(data_x)
xxxx
line from sklearn.linear_model.logistic import LogisticRegression
kmeans.fit(data_x)
xxxx
line #print("start running model training........")
kmeans.fit(data_x)
xxxx
line model = LogisticRegression(solver='liblinear', random_state=0)
kmeans.fit(data_x)
xxxx
line model.fit(Xtrain, ytrain)
kmeans.fit(data_x)
xxxx
line y_pred = model.predict(Xtest)
kmeans.fit(data_x)
xxxx
line score = accuracy_score(ytest, y_pred)
kmeans.fit(data_x)
xxxx
line import numpy as np
kmeans.fit(data_x)
xxxx
line np.save("HAIPipeGen/new_data/prenotebook_res/datascientist25_gender-recognition-by-voice-using-machine-learning.npy", { "accuracy_score": score })
kmeans.fit(data_x)
xxxx
line 
kmeans.fit(data_x)
Traceback (most recent call last):
  File ".\example.py", line 1, in <module>
    from HAIPipeGen.generate_hai import *
  File "E:\firefly\HAIPipeGen\generate_hai.py", line 5, in <module>
    from HAIPipeGen.NotebookGraph import build_one_graph
  File "E:\firefly\HAIPipeGen\NotebookGraph.py", line 97
    class NotebookGraph(ast.NodeVisitor):
    ^
IndentationError: expected an indented block
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\_pylab_helpers.py", line 86, in destroy_all
    manager.destroy()
  File "D:\Anaconda3\envs\firefly\lib\site-packages\matplotlib\backends\_backend_tk.py", line 495, in destroy
    self.window.after_idle(self.window.after, 0, delayed_destroy)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 830, in after_idle
    return self.after('idle', func, *args)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 821, in after
    name = self._register(callit)
  File "D:\Anaconda3\envs\firefly\lib\tkinter\__init__.py", line 1528, in _register
    self.tk.createcommand(name, f)
RuntimeError: main thread is not in main loop
male      1584
female    1584
Name: label, dtype: int64
[0;32;40msucceed[0m
male      1584
female    1584
Name: label, dtype: int64
[0;32;40msucceed[0m
male      1584
female    1584
Name: label, dtype: int64
[0;32;40msucceed[0m
male      1584
female    1584
Name: label, dtype: int64
[0;32;40msucceed[0m
Traceback (most recent call last):
  File "E:\firefly\HAIPipeGen\preprocessing.py", line 662, in run_one_code
    exec(cm,ns)
  File "<string>", line 5, in <module>
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 946, in __init__
    self._make_engine(self.engine)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 1178, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "D:\Anaconda3\envs\firefly\lib\site-packages\pandas\io\parsers.py", line 2008, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas\_libs\parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas\_libs\parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: '../input/voicegender/voice.csv'
male      1584
female    1584
Name: label, dtype: int64
[0;32;40msucceed[0m
notebook_id datascientist25_gender-recognition-by-voice-using-machine-learning
hi_score: {'accuracy_score': 0.9186991869918699}
hai_score: {'accuracy_score': 0.9857723577235772}
